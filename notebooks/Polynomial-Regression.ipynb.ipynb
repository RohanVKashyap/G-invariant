{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG2mYJPsOFmf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras.constraints import maxnorm, nonneg, unit_norm\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQWMYhhMAzwf",
        "outputId": "e6f681a4-5dcd-46ed-a6be-a486b298625b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxI3lCGtTmEu"
      },
      "source": [
        "## Permutation matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m5g8txKs0ZS"
      },
      "outputs": [],
      "source": [
        "def generate_per_matrix(n):\n",
        "    #P = np.zeros((n,n)).astype(np.int64)\n",
        "    atom = np.arange(n)\n",
        "    P = np.array([np.roll(atom, shift=i) for i in np.arange(n)])\n",
        "    return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFVnCeqvSXcU"
      },
      "outputs": [],
      "source": [
        "from itertools import permutations\n",
        "\n",
        "Z2 = [[0, 1], [1, 0]]\n",
        "Z4 = [[0, 1, 2, 3], [1, 2, 3, 0], [2, 3, 0, 1], [3, 0, 1, 2]]\n",
        "Z5 = [[0, 1, 2, 3, 4], [1, 2, 3, 4, 0], [2, 3, 4, 0, 1], [3, 4, 0, 1, 2], [4, 0, 1, 2, 3]]\n",
        "Z8 = [[0, 1, 2, 3, 4, 5, 6, 7], [7, 0, 1, 2, 3, 4, 5, 6], [6, 7, 0, 1, 2, 3, 4, 5], [5, 6, 7, 0, 1, 2, 3, 4],\n",
        "      [4, 5, 6, 7, 0, 1, 2, 3], [3, 4, 5, 6, 7, 0, 1, 2], [2, 3, 4, 5, 6, 7, 0, 1], [1, 2, 3, 4, 5, 6, 7, 0]]\n",
        "      \n",
        "\n",
        "Z4 = [[0, 1, 2, 3], [1, 2, 3, 0], [2, 3, 0, 1], [3, 0, 1, 2]]\n",
        "Z5 = [[0, 1, 2, 3, 4], [1, 2, 3, 4, 0], [2, 3, 4, 0, 1], [3, 4, 0, 1, 2], [4, 0, 1, 2, 3]]\n",
        "perm = [[0, 1, 2, 3], [1, 2, 3, 0], [2, 3, 0, 1], [3, 0, 1, 2],\n",
        "        [3, 2, 1, 0], [2, 1, 0, 3], [1, 0, 3, 2], [0, 3, 2, 1]]\n",
        "D8 = [list(x) + [4] for x in perm]\n",
        "perm = [[0, 1, 2, 3], [1, 2, 0, 3], [2, 0, 1, 3], [3, 0, 2, 1], [1, 3, 2, 0], [3, 1, 0, 2], [2, 1, 3, 0],\n",
        "        [0, 3, 1, 2], [0, 2, 3, 1], [1, 0, 3, 2], [2, 3, 0, 1], [3, 2, 1, 0]]\n",
        "A4 = [list(x) + [4] for x in perm]\n",
        "perm = list(permutations(range(4)))\n",
        "S4 = [list(x) + [4] for x in perm]\n",
        "perm = list(permutations(range(3)))\n",
        "S3 = [list(x) + [3, 4] for x in perm]\n",
        "\n",
        "Z5_Z10 = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 0, 1, 2, 3, 5, 6, 7, 8, 9],\n",
        "          [3, 4, 0, 1, 2, 5, 6, 7, 8, 9], [2, 3, 4, 0, 1, 5, 6, 7, 8, 9],\n",
        "          [1, 2, 3, 4, 0, 5, 6, 7, 8, 9]]\n",
        "\n",
        "Z16 = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
        "       [13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "       [10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8], [8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7],\n",
        "       [7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5], [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4],\n",
        "       [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1],\n",
        "       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0]]\n",
        "\n",
        "Z2_Z16 = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [1, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]\n",
        "\n",
        "Z4_Z16 = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [3, 0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "          [2, 3, 0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [1, 2, 3, 0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]\n",
        "\n",
        "Z8_Z16 = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [7, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "          [6, 7, 0, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15], [5, 6, 7, 0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "          [4, 5, 6, 7, 0, 1, 2, 3, 8, 9, 10, 11, 12, 13, 14, 15], [3, 4, 5, 6, 7, 0, 1, 2, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "          [2, 3, 4, 5, 6, 7, 0, 1, 8, 9, 10, 11, 12, 13, 14, 15], [1, 2, 3, 4, 5, 6, 7, 0, 8, 9, 10, 11, 12, 13, 14, 15]]\n",
        "\n",
        "Z10 = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [9, 0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
        "      [8, 9, 0, 1, 2, 3, 4, 5, 6, 7],  [7, 8, 9, 0, 1, 2, 3, 4, 5, 6],\n",
        "      [6, 7, 8, 9, 0, 1, 2, 3, 4, 5],  [5, 6, 7, 8, 9, 0, 1, 2, 3, 4],\n",
        "      [4, 5, 6, 7, 8, 9, 0, 1, 2, 3],  [3, 4, 5, 6, 7, 8, 9, 0, 1, 2],\n",
        "      [2, 3, 4, 5, 6, 7, 8, 9, 0, 1],  [1, 2, 3, 4, 5, 6, 7, 8, 9, 0,]]                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kHKpOvuTou1"
      },
      "source": [
        "## Polynomial functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxRk57yITqqy"
      },
      "outputs": [],
      "source": [
        "def poly_Z5(x):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = inv1(a, b) + inv1(b, c) + inv1(c, d) + inv1(d, e) + inv1(e, a)\n",
        "    return q1\n",
        "\n",
        "def poly_Z5_Z10(x):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    a, b, c, d, e, f , g , h, i, j  = tf.unstack(x, axis=1)\n",
        "    q1 = inv1(a, b) + inv1(b, c) + inv1(c, d) + inv1(d, e) + inv1(e, a)\n",
        "    return q1\n",
        "\n",
        "def poly_Zk_Zn(x, indices):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    unstacked_variables  = tf.unstack(x, axis=1)\n",
        "    #print(\"Before:\",unstacked_variables)\n",
        "    #unstacked_variables = unstacked_variables[indices]\n",
        "    unstacked_variables = tf.gather(unstacked_variables, indices)\n",
        "\n",
        "    #print(\"After:\",unstacked_variables)\n",
        "\n",
        "    q1 = 0\n",
        "    for i in np.arange(len(indices)-1):\n",
        "        q1 += inv1(unstacked_variables[i], unstacked_variables[i+1])\n",
        "    q1 += inv1(unstacked_variables[len(indices)-1], unstacked_variables[0])        \n",
        "    #q1 = inv1(a, b) + inv1(b, c) + inv1(c, d) + inv1(d, e) + inv1(e, a)\n",
        "    return q1\n",
        "\n",
        "\n",
        "def poly_D8(x):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = inv1(a, b) + inv1(b, c) + inv1(c, d) + inv1(d, a) + \\\n",
        "         inv1(b, a) + inv1(c, b) + inv1(d, c) + inv1(a, d)\n",
        "    return q1 + e\n",
        "\n",
        "\n",
        "def poly_A4(x):\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = a * b + c * d\n",
        "    q2 = a * c + b * d\n",
        "    q3 = a * d + b * c\n",
        "    q4 = a * b * c + a * b * d + a * c * d + b * c * d\n",
        "\n",
        "    return q1 + q2 + q3 + q4 + e\n",
        "\n",
        "\n",
        "def poly_S4(x):\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = a * b * c * d\n",
        "    return q1 + e\n",
        "\n",
        "\n",
        "def poly_S3xS2(x):\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = a*b*c + d + e\n",
        "    return q1\n",
        "\n",
        "def poly_S3(x):\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = a * b * c + 2 * d + e\n",
        "    return q1\n",
        "\n",
        "def poly_Z3(x):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    a, b, c, d, e = tf.unstack(x, axis=1)\n",
        "    q1 = inv1(a, b) + inv1(b, c) + inv1(c, a) + 2 * d + e\n",
        "    return q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Zz8bF7T31o"
      },
      "source": [
        "## Create data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH54izVk1l2K"
      },
      "source": [
        "        Ground truth: \n",
        "$\\mathbb{Z}_5$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDY5xDeHTsn9",
        "outputId": "1e8d30ef-dd1f-4acd-99ef-b130c105ec1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape info: [(16, 5), (16,), (480, 5), (480,)]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1024)\n",
        "ts = 1\n",
        "vs = 30\n",
        "batch_size = 16\n",
        "train_size = int(batch_size * ts)\n",
        "val_size = int(batch_size * vs)\n",
        "d = 5\n",
        "train_ds = np.random.rand(ts*batch_size, d)\n",
        "val_ds = np.random.rand(vs*batch_size, d)\n",
        "train_y = poly_Z5(train_ds).numpy()\n",
        "val_y = poly_Z5(val_ds).numpy()\n",
        "\n",
        "print(\"Shape info:\",[train_ds.shape, train_y.shape, val_ds.shape, val_y.shape])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X__6-vvNWXLH"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZgbbh3VRwcS"
      },
      "outputs": [],
      "source": [
        "def apply_layers(x, layers):\n",
        "    for l in layers:\n",
        "        x = l(x)\n",
        "    return x\n",
        "\n",
        "def sigmaPi(fin, m, n, p):\n",
        "    fin = tf.transpose(fin, (0, 2, 1, 3))\n",
        "    fin = fin[:, :, tf.newaxis]\n",
        "    fin = tf.tile(fin, (1, 1, m, 1, 1))\n",
        "    y = fin @ p\n",
        "    y = tf.linalg.diag_part(y)\n",
        "    y = tf.reduce_prod(y, axis=3)\n",
        "    y = tf.reduce_sum(y, axis=2)\n",
        "    return y\n",
        "\n",
        "\n",
        "def prepare_permutation_matices(perm, n, m):\n",
        "    p1 = np.eye(n, dtype=np.float32)\n",
        "    p = np.tile(p1[np.newaxis], (m, 1, 1))\n",
        "    for i, x in enumerate(perm):\n",
        "        p[i, x, :] = p1[np.arange(n)]\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MFKadp0ONjX"
      },
      "source": [
        "## Custom layer: SigmaPi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC1ElPX2OocX"
      },
      "outputs": [],
      "source": [
        "class SigmaPi(keras.layers.Layer):\n",
        "    def __init__(self, m, n, p):\n",
        "        super(SigmaPi, self).__init__()\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.p = p\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.transpose(inputs, (0, 2, 1, 3))\n",
        "        inputs = inputs[:, :, tf.newaxis]\n",
        "        inputs = tf.tile(inputs, (1, 1, self.m, 1, 1))\n",
        "        y = inputs @ self.p\n",
        "        y = tf.linalg.diag_part(y)\n",
        "        y = tf.reduce_prod(y, axis=3)\n",
        "        y = tf.reduce_sum(y, axis=2)        \n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbSP8l2J3hP9"
      },
      "source": [
        "## Ground Truth: Sum Of Products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KA32NDo3m8P"
      },
      "outputs": [],
      "source": [
        "class GroupInvariance(tf.keras.Model):\n",
        "    def __init__(self, perm, num_features):\n",
        "        super(GroupInvariance, self).__init__()\n",
        "        activation=tf.keras.activations.tanh\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.n = len(perm[0])\n",
        "        self.m = len(perm)\n",
        "        self.p = prepare_permutation_matices(perm, self.n, self.m)\n",
        "\n",
        "        self.features = [\n",
        "            tf.keras.layers.Dense(16, activation),\n",
        "            tf.keras.layers.Dense(64, activation),\n",
        "            tf.keras.layers.Dense(self.n * self.num_features, tf.keras.activations.sigmoid),\n",
        "            #tf.keras.layers.Dense(self.n * self.num_features, None),\n",
        "        ]\n",
        "\n",
        "        self.fc = [\n",
        "            #tf.keras.layers.Dense(32, tf.keras.activations.tanh),\n",
        "            tf.keras.layers.Dense(32, tf.keras.activations.relu, use_bias=False),\n",
        "            tf.keras.layers.Dense(1),\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[:, :, tf.newaxis]\n",
        "        x = apply_layers(x, self.features)\n",
        "        x = tf.reshape(x, (-1, self.n, self.num_features, self.n))\n",
        "        x = sigmaPi(x, self.m, self.n, self.p)\n",
        "        x = apply_layers(x, self.fc)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK4AQ9Q4VRti"
      },
      "source": [
        "## Run Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybhck7x55BFg"
      },
      "source": [
        "        Run GT\n",
        "$$\\mathbb{Z}_5$$ \n",
        "\n",
        "        Train Error = 0.0024\n",
        "        Val Error = 0.07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdCntd43SHWi"
      },
      "outputs": [],
      "source": [
        "np.random.seed(2048)\n",
        "Model = GroupInvariance(Z5, 64)\n",
        "adam = Adam(lr=1e-3)\n",
        "Model.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mc0m91GOTF6H",
        "outputId": "531a25dd-2dd0-44d3-ed13-80999b2bd213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2500\n",
            "16/16 [==============================] - 1s 17ms/step - loss: 0.4814 - val_loss: 0.4331\n",
            "Epoch 2/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4182 - val_loss: 0.3583\n",
            "Epoch 3/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3894 - val_loss: 0.3360\n",
            "Epoch 4/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3432 - val_loss: 0.2795\n",
            "Epoch 5/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2885 - val_loss: 0.2531\n",
            "Epoch 6/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2270 - val_loss: 0.1691\n",
            "Epoch 7/2500\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1801 - val_loss: 0.1434\n",
            "Epoch 8/2500\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1537 - val_loss: 0.1472\n",
            "Epoch 9/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1421 - val_loss: 0.1369\n",
            "Epoch 10/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1444 - val_loss: 0.1436\n",
            "Epoch 11/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1409 - val_loss: 0.1361\n",
            "Epoch 12/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1460 - val_loss: 0.1398\n",
            "Epoch 13/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1481 - val_loss: 0.1606\n",
            "Epoch 14/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1495 - val_loss: 0.1406\n",
            "Epoch 15/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1461 - val_loss: 0.1346\n",
            "Epoch 16/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1442 - val_loss: 0.1296\n",
            "Epoch 17/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1415 - val_loss: 0.1454\n",
            "Epoch 18/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1501 - val_loss: 0.1290\n",
            "Epoch 19/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1362 - val_loss: 0.1353\n",
            "Epoch 20/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1307 - val_loss: 0.1249\n",
            "Epoch 21/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1280 - val_loss: 0.1237\n",
            "Epoch 22/2500\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1313 - val_loss: 0.1211\n",
            "Epoch 23/2500\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1330 - val_loss: 0.1315\n",
            "Epoch 24/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1272 - val_loss: 0.1186\n",
            "Epoch 25/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1253 - val_loss: 0.1144\n",
            "Epoch 26/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1130 - val_loss: 0.1088\n",
            "Epoch 27/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1131 - val_loss: 0.1034\n",
            "Epoch 28/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1079 - val_loss: 0.0957\n",
            "Epoch 29/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1037 - val_loss: 0.0983\n",
            "Epoch 30/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1015 - val_loss: 0.0932\n",
            "Epoch 31/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0977 - val_loss: 0.0918\n",
            "Epoch 32/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0926 - val_loss: 0.0849\n",
            "Epoch 33/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0865 - val_loss: 0.0862\n",
            "Epoch 34/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0899 - val_loss: 0.0785\n",
            "Epoch 35/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0921 - val_loss: 0.0823\n",
            "Epoch 36/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0851 - val_loss: 0.0835\n",
            "Epoch 37/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0856 - val_loss: 0.0799\n",
            "Epoch 38/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0789 - val_loss: 0.0715\n",
            "Epoch 39/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0781 - val_loss: 0.0747\n",
            "Epoch 40/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0725 - val_loss: 0.0711\n",
            "Epoch 41/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0807 - val_loss: 0.0991\n",
            "Epoch 42/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0789 - val_loss: 0.0800\n",
            "Epoch 43/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0728 - val_loss: 0.0628\n",
            "Epoch 44/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0713 - val_loss: 0.0583\n",
            "Epoch 45/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0655 - val_loss: 0.0647\n",
            "Epoch 46/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0688 - val_loss: 0.0612\n",
            "Epoch 47/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0602 - val_loss: 0.0536\n",
            "Epoch 48/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0541 - val_loss: 0.0506\n",
            "Epoch 49/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0531 - val_loss: 0.0494\n",
            "Epoch 50/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0559 - val_loss: 0.0482\n",
            "Epoch 51/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0560 - val_loss: 0.0696\n",
            "Epoch 52/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0775 - val_loss: 0.0673\n",
            "Epoch 53/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0645 - val_loss: 0.0582\n",
            "Epoch 54/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0511 - val_loss: 0.0466\n",
            "Epoch 55/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0429 - val_loss: 0.0424\n",
            "Epoch 56/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0420 - val_loss: 0.0568\n",
            "Epoch 57/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0465 - val_loss: 0.0405\n",
            "Epoch 58/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0423 - val_loss: 0.0417\n",
            "Epoch 59/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0405 - val_loss: 0.0459\n",
            "Epoch 60/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0435 - val_loss: 0.0434\n",
            "Epoch 61/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0423 - val_loss: 0.0374\n",
            "Epoch 62/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0510 - val_loss: 0.0776\n",
            "Epoch 63/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0608 - val_loss: 0.0446\n",
            "Epoch 64/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0404 - val_loss: 0.0442\n",
            "Epoch 65/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0528 - val_loss: 0.0815\n",
            "Epoch 66/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0558 - val_loss: 0.0395\n",
            "Epoch 67/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0351 - val_loss: 0.0451\n",
            "Epoch 68/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0491 - val_loss: 0.0368\n",
            "Epoch 69/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0385 - val_loss: 0.0379\n",
            "Epoch 70/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0370 - val_loss: 0.0351\n",
            "Epoch 71/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0351 - val_loss: 0.0363\n",
            "Epoch 72/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0373 - val_loss: 0.0363\n",
            "Epoch 73/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0345 - val_loss: 0.0345\n",
            "Epoch 74/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0353 - val_loss: 0.0410\n",
            "Epoch 75/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0438 - val_loss: 0.0343\n",
            "Epoch 76/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0446 - val_loss: 0.0369\n",
            "Epoch 77/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0391 - val_loss: 0.0398\n",
            "Epoch 78/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0412 - val_loss: 0.0408\n",
            "Epoch 79/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0361 - val_loss: 0.0338\n",
            "Epoch 80/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0368 - val_loss: 0.0391\n",
            "Epoch 81/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0414 - val_loss: 0.0492\n",
            "Epoch 82/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0351 - val_loss: 0.0367\n",
            "Epoch 83/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 0.0338\n",
            "Epoch 84/2500\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0344 - val_loss: 0.0334\n",
            "Epoch 85/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0335 - val_loss: 0.0347\n",
            "Epoch 86/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0347 - val_loss: 0.0326\n",
            "Epoch 87/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0327 - val_loss: 0.0385\n",
            "Epoch 88/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0340 - val_loss: 0.0313\n",
            "Epoch 89/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0314 - val_loss: 0.0343\n",
            "Epoch 90/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0327 - val_loss: 0.0367\n",
            "Epoch 91/2500\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0369 - val_loss: 0.0389\n",
            "Epoch 92/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0387 - val_loss: 0.0450\n",
            "Epoch 93/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0358 - val_loss: 0.0459\n",
            "Epoch 94/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0329 - val_loss: 0.0341\n",
            "Epoch 95/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0329 - val_loss: 0.0329\n",
            "Epoch 96/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0340 - val_loss: 0.0320\n",
            "Epoch 97/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0311 - val_loss: 0.0319\n",
            "Epoch 98/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0344 - val_loss: 0.0403\n",
            "Epoch 99/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0327 - val_loss: 0.0313\n",
            "Epoch 100/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0283 - val_loss: 0.0313\n",
            "Epoch 101/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0313 - val_loss: 0.0382\n",
            "Epoch 102/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0325 - val_loss: 0.0316\n",
            "Epoch 103/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0370 - val_loss: 0.0357\n",
            "Epoch 104/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0336 - val_loss: 0.0326\n",
            "Epoch 105/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0315 - val_loss: 0.0300\n",
            "Epoch 106/2500\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0291 - val_loss: 0.0299\n",
            "Epoch 107/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0298 - val_loss: 0.0308\n",
            "Epoch 108/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0345 - val_loss: 0.0304\n",
            "Epoch 109/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0342 - val_loss: 0.0299\n",
            "Epoch 110/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0283 - val_loss: 0.0290\n",
            "Epoch 111/2500\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0320 - val_loss: 0.0393\n",
            "Epoch 112/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0288 - val_loss: 0.0288\n",
            "Epoch 113/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0296 - val_loss: 0.0327\n",
            "Epoch 114/2500\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0289 - val_loss: 0.0297\n",
            "Epoch 115/2500\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0341 - val_loss: 0.0279\n",
            "Epoch 116/2500\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.0331\n",
            "Epoch 117/2500\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0326 - val_loss: 0.0549\n",
            "Epoch 118/2500\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0442"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5d31cc3b7470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m Model.fit(train_ds, train_y, \n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           shuffle=True, validation_data=(val_ds, val_y))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1454\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1457\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m       \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m       can_run_full_execution = (\n\u001b[1;32m   1250\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \"\"\"\n\u001b[1;32m    711\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0;32m--> 682\u001b[0;31m           self.handle, self._dtype)\n\u001b[0m\u001b[1;32m    683\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 480\u001b[0;31m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0m\u001b[1;32m    481\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "max_epochs = 2500\n",
        "Model.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=16,\n",
        "          shuffle=True, validation_data=(val_ds, val_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzc0XDP9UDp7"
      },
      "source": [
        "## Create Data for $\\mathbb{Z}_5 : \\mathbb{Z}_{10}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI5doD9_1-r1",
        "outputId": "1b5662f1-00e7-44aa-c78f-bb5aba68e39c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape info: [(32, 10), (32,), (480, 10), (480,)]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1024)\n",
        "ts = 32\n",
        "vs = 480\n",
        "batch_size = 1\n",
        "train_size = int(batch_size * ts)\n",
        "val_size = int(batch_size * vs)\n",
        "d = 10\n",
        "train_ds = np.random.rand(ts*batch_size, d)\n",
        "val_ds = np.random.rand(vs*batch_size, d)\n",
        "\n",
        "indices = np.array([0, 2, 4, 6, 8]).astype(np.int64)#np.arange(5).astype(np.int64)\n",
        "train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
        "val_y = poly_Zk_Zn(val_ds, indices).numpy()\n",
        "print(\"Shape info:\",[train_ds.shape, train_y.shape, val_ds.shape, val_y.shape])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkHYgvZnk8hs"
      },
      "source": [
        "## Create Data for $\\mathbb{Z}_4 : \\mathbb{Z}_{16}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuaYMZ26k3fg"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1024)\n",
        "ts = 32\n",
        "vs = 30\n",
        "batch_size = 1\n",
        "d = 16\n",
        "train_ds = np.random.rand(ts*batch_size, d)\n",
        "val_ds = np.random.rand(vs*batch_size, d)\n",
        "\n",
        "indices = np.array([0, 1, 2, 3]).astype(np.int64)#np.arange(5).astype(np.int64)\n",
        "train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
        "val_y = poly_Zk_Zn(val_ds, indices).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ewogqNphvJ"
      },
      "source": [
        "## Create Data for $\\mathbb{Z}_8 : \\mathbb{Z}_{16}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEPWSrGLphvM"
      },
      "source": [
        "        Proposed: \n",
        "$\\mathbb{Z}_8 : \\mathbb{Z}_{16}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpbelFT3phvN"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1024)\n",
        "ts = 64\n",
        "vs = 480\n",
        "batch_size = 1\n",
        "d = 16\n",
        "train_ds = np.random.rand(ts*batch_size, d)\n",
        "val_ds = np.random.rand(vs*batch_size, d)\n",
        "\n",
        "\n",
        "indices = np.array([5, 6, 7, 8, 9, 10, 12, 13]).astype(np.int64)#np.arange(5).astype(np.int64)\n",
        "train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
        "val_y = poly_Zk_Zn(val_ds, indices).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1R7cjVltgYW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-jH5DIKt2pA"
      },
      "source": [
        "## Create Data for $\\mathbb{Z}_8 : \\mathbb{Z}_{512}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W77g_X4Zty0u"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1024)\n",
        "ts = 1024\n",
        "vs = 4096\n",
        "batch_size = 1\n",
        "d = 512\n",
        "train_ds = np.random.rand(ts*batch_size, d)\n",
        "val_ds = np.random.rand(vs*batch_size, d)\n",
        "\n",
        "indices = np.array([0, 1, 3, 6, 7, 10, 12, 13]).astype(np.int64)#np.arange(5).astype(np.int64)\n",
        "train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
        "val_y = poly_Zk_Zn(val_ds, indices).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht6ZpzZj3vXW"
      },
      "source": [
        "## Proposed $M$ + $G$-Invariant Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU-nGwf_AoeY"
      },
      "outputs": [],
      "source": [
        "class Simple_FC(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Simple_FC, self).__init__()\n",
        "        activation = tf.keras.activations.tanh\n",
        "        self.features = [\n",
        "            tf.keras.layers.Dense(10, activation),\n",
        "            tf.keras.layers.Dense(89, activation),\n",
        "            tf.keras.layers.Dense(6 * 32, activation),\n",
        "            tf.keras.layers.Dense(32, activation),\n",
        "            tf.keras.layers.Dense(1),\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = apply_layers(inputs, self.features)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12u5eNhu3upX"
      },
      "outputs": [],
      "source": [
        "def my_regularizer(x):\n",
        "    #x = tf.abs(x) + 1e-8\n",
        "    x = x/(tf.reduce_sum(x, axis=0))\n",
        "    entropy = tf.reduce_mean(tf.reduce_sum(-x*tf.math.log(x), axis=0))\n",
        "    return 1e-5 * entropy\n",
        "\n",
        "lambda_val = 1e-02\n",
        "l2_reg = tf.keras.regularizers.l2(1e-5)\n",
        "class GroupInvarianceProposed(tf.keras.Model):\n",
        "    def __init__(self, perm, num_features):\n",
        "        super(GroupInvarianceProposed, self).__init__()\n",
        "        activation=tf.keras.activations.tanh\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.n = len(perm[0])\n",
        "        self.m = len(perm)\n",
        "        self.p = prepare_permutation_matices(perm, self.n, self.m)\n",
        "        self.d = self.n\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(self.d, activation=None) \n",
        "                                         #kernel_regularizer=tf.keras.regularizers.l1(lambda_val),\n",
        "                                         #kernel_constraint = nonneg())\n",
        "\n",
        "        self.features = [\n",
        "            tf.keras.layers.Dense(16, activation, kernel_regularizer=l2_reg),\n",
        "            #tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(64, activation, kernel_regularizer=l2_reg),\n",
        "            #tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(self.n * self.num_features, tf.keras.activations.sigmoid, kernel_regularizer=l2_reg),\n",
        "            #tf.keras.layers.BatchNormalization(),\n",
        "            #tf.keras.layers.Dense(self.n * self.num_features, None),\n",
        "        ]\n",
        "\n",
        "        self.fc = [\n",
        "            #tf.keras.layers.Dense(32, tf.keras.activations.tanh),\n",
        "            tf.keras.layers.Dense(32, tf.keras.activations.relu, use_bias=False, kernel_regularizer=l2_reg),\n",
        "            #tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.fc1(inputs)\n",
        "        x = x[:, :, tf.newaxis]\n",
        "        x = apply_layers(x, self.features)\n",
        "        x = tf.reshape(x, (-1, self.n, self.num_features, self.n))\n",
        "        x = sigmaPi(x, self.m, self.n, self.p)\n",
        "        x = apply_layers(x, self.fc)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf6wUTuBneCa"
      },
      "source": [
        "## Multiple trails: Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25yeLNmznhH8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "num_trails = 5\n",
        "np.random.seed(1024)\n",
        "#ts = 32\n",
        "#vs = 480\n",
        "total_train_samples = 32\n",
        "total_val_samples = 480\n",
        "#batch_size = 1\n",
        "#train_size = int(batch_size * ts)\n",
        "#val_size = int(batch_size * vs)\n",
        "d = 10\n",
        "k = 5\n",
        "GT_indices = []\n",
        "all_diff = []\n",
        "all_estimates = []\n",
        "patience = 200\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "import pandas as pd\n",
        "\n",
        "def poly_Zk_Zn(x, indices):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    unstacked_variables  = tf.unstack(x, axis=1)\n",
        "    #print(\"Before:\",unstacked_variables)\n",
        "    #unstacked_variables = unstacked_variables[indices]\n",
        "    unstacked_variables = tf.gather(unstacked_variables, indices)\n",
        "\n",
        "    #print(\"After:\",unstacked_variables)\n",
        "\n",
        "    q1 = 0\n",
        "    for i in np.arange(len(indices)-1):\n",
        "        q1 += inv1(unstacked_variables[i], unstacked_variables[i+1])\n",
        "    q1 += inv1(unstacked_variables[len(indices)-1], unstacked_variables[0])        \n",
        "    #q1 = inv1(a, b) + inv1(b, c) + inv1(c, d) + inv1(d, e) + inv1(e, a)\n",
        "    return q1\n",
        "\n",
        "def save_pd(history, fileName):\n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    hist_json_file = fileName + '_history.json' \n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "\n",
        "    # save to csv: \n",
        "    hist_csv_file = fileName + '_history.csv'\n",
        "    with open(hist_csv_file, mode='w') as f:\n",
        "        hist_df.to_csv(f) \n",
        "\n",
        "def extract_M(model, filepath, imagePath):\n",
        "  model.load_weights(filepath)\n",
        "  M = Model_discover.layers[0].weights[0].numpy().T\n",
        "  width = 10\n",
        "  height = 10\n",
        "  plt.figure(figsize=(width, height))  \n",
        "  plt.imshow(np.round(M,1), cmap='BuGn', interpolation='nearest')\n",
        "  plt.savefig(imagePath, dpi=400, transparent=False)\n",
        "  plt.show()\n",
        "  L1_norms = np.round(np.sum(M,0),3)\n",
        "  mean = np.mean(L1_norms)\n",
        "  std = np.std(L1_norms)\n",
        "  return L1_norms, mean, std\n",
        "\n",
        "for trail in np.arange(num_trails):\n",
        "\n",
        "  #%% Create DataSet \n",
        "  train_ds = np.random.rand(total_train_samples, d)\n",
        "  val_ds = np.random.rand(total_val_samples, d)\n",
        "  print(\"Before Shape info:\",[train_ds.shape,  val_ds.shape])\n",
        "  indices = np.sort(np.array(random.sample(range(d),k)).astype(np.int64))#np.arange(5).astype(np.int64)\n",
        "  GT_indices.append(indices)\n",
        "  print(\"*****GT******:\",indices)\n",
        "  train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
        "  val_y = poly_Zk_Zn(val_ds, indices).numpy()\n",
        "  print(\"Shape info:\",[train_ds.shape, train_y.shape, val_ds.shape, val_y.shape])\n",
        "\n",
        "  filepath = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'Model_Zk5_Zn10_32pts_' + str(trail) + '.h5'\n",
        "  imagePath = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'Matrix_M_Zk5_Zn10_32pts_' + str(trail) + '.png'\n",
        " \n",
        "\n",
        "  #%% Create Model\n",
        "  try:\n",
        "      del Model_discover\n",
        "      #gc.collect()\n",
        "  except:\n",
        "      print(\"Do nothing\")\n",
        "\n",
        "  #np.random.seed(2048)\n",
        "  Model_discover = GroupInvarianceProposed(Z10, 64)\n",
        "  adam = Adam(lr=1e-3)\n",
        "  Model_discover.compile(optimizer=adam, loss='mae')\n",
        "\n",
        "  #%% Run Model\n",
        "  callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath,\n",
        "      save_best_only = True,\n",
        "      save_weights_only = True,)\n",
        "\n",
        "  train_history = Model_discover.fit(train_ds, train_y, \n",
        "            epochs=max_epochs, batch_size=batch_size,\n",
        "            shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback]) \n",
        "\n",
        "  history_fileName = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'History_Zk5_Zn10_32pts_' + str(trail)\n",
        "  save_pd(train_history, history_fileName)\n",
        "  L1_norms, mean, std = extract_M(Model_discover, filepath, imagePath)\n",
        "  print('L1_norms: ',L1_norms)\n",
        "  print('mean and std: ',[mean, std])\n",
        "\n",
        "  estimates = np.sort(np.where(L1_norms >= mean)[0])\n",
        "  all_estimates.append(estimates)\n",
        "  print(\"Estimates:\",estimates)\n",
        "  print(\"GT:\",indices)\n",
        "  diff = set(estimates) - set(indices)\n",
        "  print(\"diff:\",diff)\n",
        "  print(len(diff))\n",
        "\n",
        "\n",
        "  GT_Indices_Path = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'GT_IDXs_Zk5_Zn10_32pts_' + str(trail) + '.npy'\n",
        "  All_Estimates_Path = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'Est_IDxs_Zk5_Zn10_32pts_' + str(trail) + '.npy'\n",
        "\n",
        "  np.save(GT_Indices_Path, np.array(GT_indices))\n",
        "  np.save(All_Estimates_Path, np.array(all_estimates))\n",
        "\n",
        "  try:\n",
        "      print(\"Deleting data:\")\n",
        "      del train_ds, train_y, val_ds, val_y\n",
        "      #gc.collect()\n",
        "  except:\n",
        "      print(\"Do nothing\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J1DkSJ5jxukt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "num_trails = 10\n",
        "np.random.seed(1024)\n",
        "#ts = 32\n",
        "#vs = 480\n",
        "total_train_samples = 64\n",
        "total_val_samples = 480\n",
        "#batch_size = 1\n",
        "#train_size = int(batch_size * ts)\n",
        "#val_size = int(batch_size * vs)\n",
        "d = 16\n",
        "k = 8\n",
        "GT_indices = []\n",
        "all_diff = []\n",
        "all_estimates = []\n",
        "patience = 200\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "import pandas as pd\n",
        "\n",
        "def poly_Zk_Zn(x, indices):\n",
        "    def inv1(a, b):\n",
        "        return a * b ** 2\n",
        "\n",
        "    unstacked_variables  = tf.unstack(x, axis=1)\n",
        "    #print(\"Before:\",unstacked_variables)\n",
        "    #unstacked_variables = unstacked_variables[indices]\n",
        "    unstacked_variables = tf.gather(unstacked_variables, indices)\n",
        "\n",
        "    #print(\"After:\",unstacked_variables)\n",
        "\n",
        "    q1 = 0\n",
        "    for i in np.arange(len(indices)-1):\n",
        "        q1 += inv1(unstacked_variables[i], unstacked_variables[i+1])\n",
        "    q1 += inv1(unstacked_variables[len(indices)-1], unstacked_variables[0])        \n",
        "    #q1 = inv1(a, b) + inv1(b, c) + inv1(c, d) + inv1(d, e) + inv1(e, a)\n",
        "    return q1\n",
        "\n",
        "def save_pd(history, fileName):\n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    hist_json_file = fileName + '_history.json' \n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "\n",
        "    # save to csv: \n",
        "    hist_csv_file = fileName + '_history.csv'\n",
        "    with open(hist_csv_file, mode='w') as f:\n",
        "        hist_df.to_csv(f) \n",
        "\n",
        "def extract_M(model, filepath, imagePath):\n",
        "  model.load_weights(filepath)\n",
        "  M = Model_discover.layers[0].weights[0].numpy().T\n",
        "  width = 10\n",
        "  height = 10\n",
        "  plt.figure(figsize=(width, height))  \n",
        "  plt.imshow(np.round(M,1), cmap='BuGn', interpolation='nearest')\n",
        "  plt.savefig(imagePath, dpi=400, transparent=False)\n",
        "  plt.show()\n",
        "  L1_norms = np.round(np.sum(M,0),3)\n",
        "  mean = np.mean(L1_norms)\n",
        "  std = np.std(L1_norms)\n",
        "  return L1_norms, mean, std\n",
        "\n",
        "for trail in np.arange(0, num_trails):\n",
        "\n",
        "  #%% Create DataSet \n",
        "  train_ds = np.random.rand(total_train_samples, d)\n",
        "  val_ds = np.random.rand(total_val_samples, d)\n",
        "  print(\"Before Shape info:\",[train_ds.shape,  val_ds.shape])\n",
        "  indices = np.sort(np.array(random.sample(range(d),k)).astype(np.int64))#np.arange(5).astype(np.int64)\n",
        "  GT_indices.append(indices)\n",
        "  print(\"*****GT******:\",indices)\n",
        "  train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
        "  val_y = poly_Zk_Zn(val_ds, indices).numpy()\n",
        "  print(\"Shape info:\",[train_ds.shape, train_y.shape, val_ds.shape, val_y.shape])\n",
        "\n",
        "  filepath = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'Model_Zk8_Zn16_64pts_' + str(trail) + '.h5'\n",
        "  imagePath = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'Matrix_M_Zk8_Zn16_64pts_' + str(trail) + '.png'\n",
        " \n",
        "\n",
        "  #%% Create Model\n",
        "  try:\n",
        "      del Model_discover\n",
        "      #gc.collect()\n",
        "  except:\n",
        "      print(\"Do nothing\")\n",
        "\n",
        "  #np.random.seed(2048)\n",
        "  Model_discover = GroupInvarianceProposed(Z16, 64)\n",
        "  adam = Adam(lr=1e-3)\n",
        "  Model_discover.compile(optimizer=adam, loss='mae')\n",
        "\n",
        "  #%% Run Model\n",
        "  callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath,\n",
        "      save_best_only = True,\n",
        "      save_weights_only = True,)\n",
        "\n",
        "  train_history = Model_discover.fit(train_ds, train_y, \n",
        "            epochs=max_epochs, batch_size=batch_size,\n",
        "            shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback]) \n",
        "\n",
        "  history_fileName = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'History_Zk8_Zn16_64pts_' + str(trail)\n",
        "  save_pd(train_history, history_fileName)\n",
        "  L1_norms, mean, std = extract_M(Model_discover, filepath, imagePath)\n",
        "  print('L1_norms: ',L1_norms)\n",
        "  print('mean and std: ',[mean, std])\n",
        "\n",
        "  estimates = np.sort(np.where(L1_norms >= mean)[0])\n",
        "  all_estimates.append(estimates)\n",
        "  print(\"Estimates:\",estimates)\n",
        "  print(\"GT:\",indices)\n",
        "  diff = set(estimates) - set(indices)\n",
        "  print(\"diff:\",diff)\n",
        "  print(len(diff))\n",
        "\n",
        "\n",
        "  GT_Indices_Path = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'GT_IDXs_Zk8_Zn16_64pts_' + str(trail) + '.npy'\n",
        "  All_Estimates_Path = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/' + 'Est_IDxs_Zk8_Zn16_64pts_' + str(trail) + '.npy'\n",
        "\n",
        "  np.save(GT_Indices_Path, np.array(GT_indices))\n",
        "  np.save(All_Estimates_Path, np.array(all_estimates))\n",
        "\n",
        "  try:\n",
        "      print(\"Deleting data:\")\n",
        "      del train_ds, train_y, val_ds, val_y\n",
        "      #gc.collect()\n",
        "  except:\n",
        "      print(\"Do nothing\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRHchnEw6Xz5"
      },
      "source": [
        "## Run Proposed\n",
        "$$\\mathbb{Z}_5 : \\mathbb{Z}_{10} $$\n",
        "\n",
        "        Train Error (4096 pts): 0.092\n",
        "        Val Error: 0.087\n",
        "\n",
        "        Train Error (64 pts): 0.09\n",
        "        Val Error: 0.1329"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9RhpR7J6Xz6",
        "outputId": "d958e5aa-8dd8-437e-f3bd-0f564b87ea05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do nothing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = GroupInvarianceProposed(Z10, 64)\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbxzOxir6Xz8",
        "outputId": "a3d20414-327e-4074-f0a4-bc20252d716f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((32, 10), (32,), (480, 10), (480,))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.shape, train_y.shape, val_ds.shape, val_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-yhdOUd6Xz8"
      },
      "outputs": [],
      "source": [
        "filepath='/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/Z5_Z10_32.h5'\n",
        "callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGdHXHoj6Xz8"
      },
      "outputs": [],
      "source": [
        "patience = 200\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "ene2I77y7RX9",
        "outputId": "10c1f794-b26e-4725-b877-b33d56e6165a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L1_norms:  [0.6   0.    0.689 0.    0.693 0.    0.63  0.077 0.466 0.   ]\n",
            "mean and std:  [0.31550002, 0.3065212]\n"
          ]
        }
      ],
      "source": [
        "def extract_M(model, filepath):\n",
        "  model.load_weights(filepath)\n",
        "  M = Model_discover.layers[0].weights[0].numpy().T\n",
        "  plt.imshow(np.round(M,1), cmap='BuGn', interpolation='nearest')\n",
        "  plt.show()\n",
        "  L1_norms = np.round(np.sum(M,0),3)\n",
        "  mean = np.mean(L1_norms)\n",
        "  std = np.std(L1_norms)\n",
        "  return L1_norms, mean, std\n",
        "filepath='/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/Z5_Z10_32.h5'\n",
        "L1_norms, mean, std = extract_M(Model_discover, filepath)\n",
        "print('L1_norms: ',L1_norms)\n",
        "print('mean and std: ',[mean, std])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI9J_fygub4X"
      },
      "source": [
        "## Run Proposed\n",
        "$$\\mathbb{Z}_8 : \\mathbb{Z}_{512} $$\n",
        "\n",
        "        Train Error (4096 pts): 0.092\n",
        "        Val Error: 0.087\n",
        "\n",
        "        Train Error (64 pts): 0.09\n",
        "        Val Error: 0.1329"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aD1K2_nAs20"
      },
      "outputs": [],
      "source": [
        "np.random.seed(2048)\n",
        "Model_discover = Simple_FC()\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPph-b2fub4Z",
        "outputId": "d666d275-be25-4821-939b-5d649af7a611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do nothing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = GroupInvarianceProposed(Z512, 64)\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae54fSqLub4a",
        "outputId": "29082ee1-bcd6-4a5d-d015-74f2dcf97684"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((16, 16), (16,), (480, 16), (480,))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.shape, train_y.shape, val_ds.shape, val_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNeY8R15BczC"
      },
      "outputs": [],
      "source": [
        "Model_discover.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdJOQ8Tnub4b"
      },
      "outputs": [],
      "source": [
        "patience = 250\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y)) #callbacks=[reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf3SBUUFub4b"
      },
      "outputs": [],
      "source": [
        "M_discover_8_16_pts_128 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_8_16_pts_128,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_8_16_pts_128,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "#indices = np.array([0, 1, 3, 6, 7, 10, 12, 13]).astype(np.int64)\n",
        "plt.imshow(M_discover_8_16_pts_128, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U6ze_f5ub4c",
        "outputId": "98e2412f-12b4-4831-e424-23d683205c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.186, 0.196, 0.   , 0.2  , 0.004, 0.011, 0.177, 0.247, 0.   ,\n",
              "        0.049, 0.259, 0.012, 0.209, 0.147, 0.014, 0.028], dtype=float32),\n",
              " 0.108687505,\n",
              " 0.09758824)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#indices = np.array([0, 3, 6, 13]).astype(np.int64)#np.arange(5).astype(np.int64)\\\n",
        "#indices = np.array([0, 1, 3, 6, 7, 10, 12, 13])\n",
        "np.round(np.sum(M_discover_8_16_pts_128,0), 3), np.mean(np.round(np.sum(M_discover_8_16_pts_128,0), 3)), np.std(np.round(np.sum(M_discover_8_16_pts_128,0), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WezhM4JqDZp"
      },
      "source": [
        "## Run Proposed\n",
        "$$\\mathbb{Z}_8 : \\mathbb{Z}_{16} $$\n",
        "\n",
        "        Train Error (4096 pts): 0.092\n",
        "        Val Error: 0.087\n",
        "\n",
        "        Train Error (64 pts): 0.09\n",
        "        Val Error: 0.1329"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9lbcVP4qDZr"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = GroupInvarianceProposed(Z16, 64)\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZNQLGEr9J4u"
      },
      "outputs": [],
      "source": [
        "filepath='/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/Z8_Z16_64_v0.h5'\n",
        "callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bb4bq5iqDZt"
      },
      "outputs": [],
      "source": [
        "patience = 200\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ZLgVffXvxX4Q",
        "outputId": "30bdb378-3d04-4ba1-9f21-9c40edf87c5d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L1_norms:  [0.    0.126 0.051 0.001 0.063 0.31  0.383 0.432 0.409 0.399 0.403 0.\n",
            " 0.392 0.549 0.064 0.   ]\n",
            "mean and std:  [0.223875, 0.19332482]\n"
          ]
        }
      ],
      "source": [
        "def extract_M(model, filepath, imagePath):\n",
        "  model.load_weights(filepath)\n",
        "  M = Model_discover.layers[0].weights[0].numpy().T\n",
        "  width = 10\n",
        "  height = 10\n",
        "  plt.figure(figsize=(width, height))  \n",
        "  plt.imshow(np.round(M,1), cmap='BuGn', interpolation='nearest')\n",
        "  plt.savefig(imagePath, dpi=400, transparent=False)\n",
        "  plt.show()\n",
        "  L1_norms = np.round(np.sum(M,0),3)\n",
        "  mean = np.mean(L1_norms)\n",
        "  std = np.std(L1_norms)\n",
        "  return L1_norms, mean, std\n",
        "filepath='/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/Z8_Z16_64_v0.h5'\n",
        "imagePath = '/content/gdrive/MyDrive/DeepSetsData/Sym_Poly/M_Z8_Z16_64_v0.png'\n",
        "L1_norms, mean, std = extract_M(Model_discover, filepath, imagePath)\n",
        "print('L1_norms: ',L1_norms)\n",
        "print('mean and std: ',[mean, std])\n",
        "#[5, 6, 7, 8, 9, 10, 12, 13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoX8VcbqxdfH",
        "outputId": "98e2412f-12b4-4831-e424-23d683205c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.186, 0.196, 0.   , 0.2  , 0.004, 0.011, 0.177, 0.247, 0.   ,\n",
              "        0.049, 0.259, 0.012, 0.209, 0.147, 0.014, 0.028], dtype=float32),\n",
              " 0.108687505,\n",
              " 0.09758824)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#indices = np.array([0, 3, 6, 13]).astype(np.int64)#np.arange(5).astype(np.int64)\\\n",
        "#indices = np.array([0, 1, 3, 6, 7, 10, 12, 13])\n",
        "np.round(np.sum(M_discover_8_16_pts_128,0), 3), np.mean(np.round(np.sum(M_discover_8_16_pts_128,0), 3)), np.std(np.round(np.sum(M_discover_8_16_pts_128,0), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqTV8-n9wROh"
      },
      "outputs": [],
      "source": [
        "def get_matrix(matrix):\n",
        "  #indices = np.array([0, 3, 6, 13]).astype(np.int64)#np.arange(5).astype(np.int64)\\\n",
        "  #indices = np.array([0, 1, 3, 6, 7, 10, 12, 13])\n",
        "  L1_norm = np.sum(M_discover_8_16_pts_16,0)\n",
        "  column_mean = np.mean(np.sum(M_discover_8_16_pts_16,0))\n",
        "  estimated_indices = np.where(L1_norm>=column_mean)\n",
        "  k = len(estimated_indices[0])\n",
        "  n = len(L1_norm)\n",
        "  P = np.zeros((n,n))\n",
        "  for i,j in zip(np.arange(k),estimated_indices[0]):\n",
        "    P[i,j]=1\n",
        "  M_matrix=np.tile(P[0:k,:],(n//k,1))\n",
        "  return M_matrix   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhFZ6OnhqDZt"
      },
      "outputs": [],
      "source": [
        "M_discover_8_16_pts_16 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_8_16_pts_16,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_8_16_pts_16,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "#indices = np.array([0, 1, 3, 6, 7, 10, 12, 13]).astype(np.int64)\n",
        "plt.imshow(M_discover_8_16_pts_16, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69uZWXTimNev"
      },
      "source": [
        "## Run Proposed\n",
        "$$\\mathbb{Z}_4 : \\mathbb{Z}_{16} $$\n",
        "\n",
        "        Train Error (4096 pts): 0.092\n",
        "        Val Error: 0.087\n",
        "\n",
        "        Train Error (64 pts): 0.09\n",
        "        Val Error: 0.1329"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ULj6zeVlzpm",
        "outputId": "359fc82e-ccaf-4761-e0ed-17bdbc5f51fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do nothing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = GroupInvarianceProposed(Z16, 64)\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYR7IbhsBoEp"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = Simple_FC()\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_dhj-uh3p0j"
      },
      "outputs": [],
      "source": [
        "filepath='./Z4_Z16_32.h5'\n",
        "callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvMTytTHDcPY"
      },
      "outputs": [],
      "source": [
        "patience = 200\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0vnP-32lzpo"
      },
      "outputs": [],
      "source": [
        "patience = 250\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "-s_AvDyA9P97",
        "outputId": "6c85eca0-883b-4ad7-cd53-a19491b0f043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 149ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filepath='/content/gdrive/MyDrive/DeepSetsData/AllFinal_v1/Z4_Z16_32.h5'\n",
        "dummy = Model_discover.predict(train_ds)\n",
        "Model_discover.load_weights(filepath)\n",
        "M_discover_4_16_pts_32 = Model_discover.layers[0].weights[0].numpy().T\n",
        "width = 10\n",
        "height = 5\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.subplot(121)\n",
        "plt.imshow(M_discover_4_16_pts_32, cmap='BuGn', interpolation='nearest')\n",
        "M_proper = get_matrix(M_discover_4_16_pts_32)\n",
        "plt.subplot(122)\n",
        "plt.imshow(M_proper, cmap='BuGn', interpolation='nearest')\n",
        "plt.savefig(\"/content/gdrive/MyDrive/DeepSetsData/AllFinal_v1/small_size_both_matrices.png\", dpi=400, transparent=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "CCWKofBX5xl-",
        "outputId": "12813398-c654-438f-c38c-e5a5b2a94b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [1 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Matrix M: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. -0. -0.  0.  0.  0. -0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANaUlEQVR4nO3dfaxkdX3H8fenCxRFKtDdIgK6YISEGlo2G+pTqCktXSlhbeIfa2qLD8nGtLTa2BisSTX9S2trH42GKi1tiZiiVmJgZetDTElZxC0sD4vLQhGhCyy1AVtNkPrtH3PWzF7u3b07c87h3v7er2QyZ875zfy+e2Y/c86ce878UlVIas+PPdcFSHpuGH6pUYZfapThlxpl+KVGHTVmZ2vXrq2Xrl8/ZpdSU7714IM88cQTWU7bUcP/0vXruXnHrWN2KTXlNT93/rLbutsvNcrwS42aK/xJNiX5ZpK9Sa7oqyhJw5s5/EnWAB8FXg+cA7wpyTl9FSZpWPNs+c8H9lbVA1X1NHAtsLmfsiQNbZ7wnwp8e+rxw928gyTZmuS2JLft379/ju4k9WnwA35VdWVVbayqjevWrRu6O0nLNE/4HwFOn3p8WjdP0iowT/i/Drw8yRlJjgG2ANf3U5akoc18hl9VPZPkcuCLwBrgqqq6u7fKJA1qrtN7q+oG4IaeapE0Is/wkxo16oU9O/fs4nmbXnLEz/v+toeO+Dmz9DNrX9Jq5JZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUaNe2LPhrHO5eds4I/Z4gY50aG75pUYZfqlRhl9q1Dwj9pye5CtJ7klyd5J39lmYpGHNc8DvGeDdVbUzyfHAN5Jsr6p7eqpN0oBm3vJX1b6q2tlNfxfYzSIj9khamXr5zp9kPXAesGORZQ7XJa1Ac4c/yQuAzwDvqqqnFi53uC5pZZor/EmOZhL8a6rqs/2UJGkM8xztD/BJYHdVfaS/kiSNYZ4t/2uAXwd+Icnt3e3inuqSNLB5xur7FyA91iJpRJ7hJzVq1Kv6ZjXWEF9j9yU9l9zyS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNWrUC3t27tm14i+c8SIdtcItv9Qowy81yvBLjerjp7vXJPm3JF/ooyBJ4+hjy/9OJqP1SFpF5v3d/tOAXwE+0U85ksYy75b/z4D3AD/soRZJI5pn0I5LgMer6huHafejsfr4gZ8R0kox76AdlyZ5ELiWyeAd/7Cw0fRYfRztHxeklWKeIbrfW1WnVdV6YAvw5ap6c2+VSRqUm2KpUb2c219VXwW+2sdrSRqHW36pUaNe1bfhrHO5edutY3YpaQlu+aVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGjXpV36xW+vh+0mrkll9qlOGXGmX4pUbNO2LPCUmuS3Jvkt1JXtVXYZKGNe8Bvz8HtlXVG5McAzy/h5okjWDm8Cd5IXAB8BaAqnoaeLqfsiQNbZ7d/jOA/cDfdEN0fyLJcQsbTQ/XtX///jm6k9SnecJ/FLAB+FhVnQf8D3DFwkbTw3WtW7duju4k9Wme8D8MPFxVO7rH1zH5MJC0CswzVt+jwLeTnN3NuhC4p5eqJA1u3qP9vw1c0x3pfwB46/wlSRrDXOGvqtuBjT3VImlEq+LCHi/Skfrn6b1Sowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSo+Ydrut3k9yd5K4kn0pybF+FSRrWzOFPcirwO8DGqnoFsAbY0ldhkoY1727/UcDzkhzFZJy+/5i/JEljmOd3+x8B/hh4CNgHPFlVNy1s53Bd0so0z27/icBmJmP2vRg4LsmbF7ZzuC5pZZpnt/8XgX+vqv1V9QPgs8Cr+ylL0tDmCf9DwCuTPD9JmAzXtbufsiQNbZ7v/DuYDM65E7ize60re6pL0sDmHa7r/cD7e6pF0og8w09qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGjXXJb1HaueeXTxv00uO+Hnf3/bQANVIbXPLLzXK8EuNMvxSow4b/iRXJXk8yV1T805Ksj3Jfd39icOWKalvy9ny/y2wacG8K4AvVdXLgS91jyWtIocNf1V9DfjOgtmbgau76auBN/Rcl6SBzfqnvpOral83/Shw8lINk2wFtgJw7JoZu5PUt7kP+FVVAXWI5T8aroujPb4orRSzpvGxJKcAdPeP91eSpDHMGv7rgcu66cuAz/dTjqSxLOdPfZ8C/hU4O8nDSd4OfBD4pST3MRmw84PDlimpb4c94FdVb1pi0YU91yJpRB6Bkxo16lV9G846l5u33Tpml5KW4JZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUaNe2DMrh/iS+ueWX2qU4ZcaZfilRs06XNeHk9ybZFeSzyU5YdgyJfVt1uG6tgOvqKpzgT3Ae3uuS9LAZhquq6puqqpnuoe3AKcNUJukAfXxnf9twI1LLUyyNcltSW7bv39/D91J6sNc4U/yPuAZ4Jql2kwP17Vu3bp5upPUo5lP8knyFuAS4MJuvD5Jq8hM4U+yCXgP8PNV9b1+S5I0hlmH6/or4Hhge5Lbk3x84Dol9WzW4bo+OUAtkkbkGX5So1bFVX1eoSf1zy2/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1KhRr+rbuWfXaOPuzdLPrH1Jq5FbfqlRhl9q1EzDdU0te3eSSrJ2mPIkDWXW4bpIcjpwEeCXZGkVmmm4rs6fMvn5bn+zX1qFZv3d/s3AI1V1R5LDtd0KbAXg2DWzdCdpAEcc/iTPB36fyS7/YVXVlcCVAPmJY9xLkFaIWY72vww4A7gjyYNMRujdmeRFfRYmaVhHvOWvqjuBnzrwuPsA2FhVT/RYl6SBzTpcl6RVbtbhuqaXr++tGkmj8Qw/qVGjXtiz4axzuXnbraP05QU60qG55ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcalarxflYvyX7gW0ssXgushF8Dso6DWcfBVnodL62qdct5gVHDfyhJbquqjdZhHdYxTh3u9kuNMvxSo1ZS+K98rgvoWMfBrONg/2/qWDHf+SWNayVt+SWNyPBLjRo1/Ek2Jflmkr1Jrlhk+Y8n+XS3fEeS9QPUcHqSryS5J8ndSd65SJvXJXkyye3d7Q/6rmOqrweT3Nn1c9siy5PkL7p1sivJhp77P3vq33l7kqeSvGtBm8HWR5Krkjye5K6peScl2Z7kvu7+xCWee1nX5r4klw1Qx4eT3Nut988lOWGJ5x7yPeyhjg8keWRq/V+8xHMPma9nqapRbsAa4H7gTOAY4A7gnAVtfhP4eDe9Bfj0AHWcAmzopo8H9ixSx+uAL4y0Xh4E1h5i+cXAjUCAVwI7Bn6PHmVyosgo6wO4ANgA3DU174+AK7rpK4APLfK8k4AHuvsTu+kTe67jIuCobvpDi9WxnPewhzo+APzeMt67Q+Zr4W3MLf/5wN6qeqCqngauBTYvaLMZuLqbvg64MIcbA/wIVdW+qtrZTX8X2A2c2mcfPdsM/F1N3AKckOSUgfq6ELi/qpY6C7N3VfU14DsLZk//P7gaeMMiT/1lYHtVfaeq/gvYDmzqs46quqmqnuke3sJkUNpBLbE+lmM5+TrImOE/Ffj21OOHeXboftSmW+lPAj85VEHd14rzgB2LLH5VkjuS3Jjkp4eqASjgpiTfSLJ1keXLWW992QJ8aollY60PgJOral83/Shw8iJtxlwvAG9jsge2mMO9h324vPv6cdUSX4OOeH00e8AvyQuAzwDvqqqnFizeyWTX92eAvwT+acBSXltVG4DXA7+V5IIB+1pSkmOAS4F/XGTxmOvjIDXZp31O/x6d5H3AM8A1SzQZ+j38GPAy4GeBfcCf9PGiY4b/EeD0qcendfMWbZPkKOCFwH/2XUiSo5kE/5qq+uzC5VX1VFX9dzd9A3B0krV919G9/iPd/ePA55jsvk1bznrrw+uBnVX12CI1jrY+Oo8d+GrT3T++SJtR1kuStwCXAL/WfRA9yzLew7lU1WNV9b9V9UPgr5d4/SNeH2OG/+vAy5Oc0W1ltgDXL2hzPXDgqO0bgS8vtcJn1R1D+CSwu6o+skSbFx041pDkfCbraYgPoeOSHH9gmskBprsWNLse+I3uqP8rgSendon79CaW2OUfa31Mmf5/cBnw+UXafBG4KMmJ3W7wRd283iTZBLwHuLSqvrdEm+W8h/PWMX2M51eXeP3l5OtgfRyhPIIjmRczObp+P/C+bt4fMlm5AMcy2e3cC9wKnDlADa9lshu5C7i9u10MvAN4R9fmcuBuJkdMbwFePdD6OLPr446uvwPrZLqWAB/t1tmdwMYB6jiOSZhfODVvlPXB5ANnH/ADJt9T387kOM+XgPuAfwZO6tpuBD4x9dy3df9X9gJvHaCOvUy+Rx/4f3LgL1EvBm441HvYcx1/3733u5gE+pSFdSyVr0PdPL1XalSzB/yk1hl+qVGGX2qU4ZcaZfilRhl+qVGGX2rU/wGiEug+Cu5bHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filepath='Z4_Z16_512_m5_l1.h5'\n",
        "Model_discover.load_weights(filepath)\n",
        "M_discover_4_16_pts_512_original = Model_discover.layers[0].weights[0].numpy().T\n",
        "M_discover_4_16_pts_512 = 0*M_discover_4_16_pts_512_original\n",
        "M_discover_4_16_pts_512[np.where(M_discover_4_16_pts_512_original>=0.7)] = 1\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_4_16_pts_512,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_4_16_pts_512,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(np.round(M_discover_4_16_pts_512,1), cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBjpDX-L6Y8g",
        "outputId": "40ca18cc-d0e9-4336-a3aa-6e6120775247"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([6.801, 6.582, 6.891, 6.608, 0.374, 0.038, 0.023, 0.696, 0.024,\n",
              "        1.163, 0.99 , 0.279, 0.19 , 0.306, 0.038, 0.039], dtype=float32),\n",
              " 1.9401368,\n",
              " 2.7801244)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.round(np.sum(M_discover_4_16_pts_512,0),3), np.mean(np.sum(M_discover_4_16_pts_512,0)), np.std(np.sum(M_discover_4_16_pts_512,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "uuANQZEa1ci7",
        "outputId": "8f90d9a4-93ba-41d7-edf9-d54793cf72ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [11  3 11  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Matrix M: [[ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0. ]\n",
            " [ 0.   0.1  0.   0.1  0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.1  0.   0.1  0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.1  0.   0.1  0.  -0.   0.   0.   0.   0.   0.  -0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.1  0.   0.1  0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.  -0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0. ]\n",
            " [ 0.1  0.   0.1  0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  -0.   0. ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filepath='Z4_Z16_64.h5'\n",
        "Model_discover.load_weights(filepath)\n",
        "M_discover_4_16_pts_64 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_4_16_pts_64,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_4_16_pts_64,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_4_16_pts_64, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pftlu8fJ2F9h",
        "outputId": "70576423-cd6c-44bd-8dfc-70e9f756b6fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.281, 0.332, 0.308, 0.367, 0.001, 0.   , 0.004, 0.021, 0.002,\n",
              "        0.035, 0.008, 0.002, 0.005, 0.001, 0.   , 0.029], dtype=float32),\n",
              " 0.08732847,\n",
              " 0.1367902)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.round(np.sum(M_discover_4_16_pts_64,0),3), np.mean(np.sum(M_discover_4_16_pts_64,0)), np.std(np.sum(M_discover_4_16_pts_64,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "0rIvhpxrrKb3",
        "outputId": "58d1b615-a163-4f6f-d367-04318f4b3769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [ 6 10  6 10  0  0  0  0  0  0  2  0  0  0  0  0]\n",
            "Matrix M: [[ 0.  -0.   0.   0.   0.  -0.   0.   0.  -0.   0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.  -0.   0.  -0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.  -0.   0.  -0.  -0.  -0.   0.  -0.  -0.   0.1  0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.  -0.   0.   0.   0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.1 -0.   0.1  0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.1 -0.   0.1  0.  -0.   0.   0.  -0.  -0.   0.  -0.   0.   0.\n",
            "  -0.  -0. ]\n",
            " [ 0.1 -0.   0.1  0.   0.  -0.   0.   0.  -0.   0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.1 -0.   0.1  0.   0.  -0.   0.   0.  -0.   0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.  -0.   0.   0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.  -0.   0.  -0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.2 -0.   0.2  0.  -0.   0.   0.  -0.   0.   0.  -0.   0.   0.\n",
            "  -0.   0. ]\n",
            " [ 0.   0.  -0.   0.   0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.1 -0.   0.1  0.  -0.  -0.   0.   0.  -0.   0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.   0.1 -0.   0.1  0.  -0.   0.   0.  -0.  -0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.1 -0.   0.1  0.   0.  -0.   0.   0.  -0.   0.   0.   0.  -0.  -0.\n",
            "  -0.  -0. ]\n",
            " [ 0.1 -0.   0.1  0.  -0.  -0.   0.   0.  -0.   0.   0.  -0.  -0.  -0.\n",
            "  -0.  -0. ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Model_discover.load_weights(filepath)\n",
        "M_discover_4_16_pts_32 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_4_16_pts_32,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_4_16_pts_32,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_4_16_pts_32, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFlrUAVg6X9v"
      },
      "outputs": [],
      "source": [
        "def get_matrix(matrix):\n",
        "  L1_norm=np.sum(matrix,0)\n",
        "  mean=np.mean(L1_norm)\n",
        "  est_idx=np.where(L1_norm>=mean)\n",
        "  k=len(est_idx[0])\n",
        "  n=len(matrix)\n",
        "  P=np.zeros((n,n))\n",
        "  for i,j in zip(np.arange(k),est_idx[0]):\n",
        "    P[i,j]=1\n",
        "  M=np.tile(P[0:k],(n//k,1))  \n",
        "  return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GumSB2Zw7-hZ"
      },
      "outputs": [],
      "source": [
        "M_proper=get_matrix(M_discover_4_16_pts_16)\n",
        "filepath='Z4_Z16_32_retrained.h5'\n",
        "callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,)\n",
        "\n",
        "Model_discover = GroupInvarianceProposed(Z16, 64)\n",
        "pred=Model_discover(train_ds)\n",
        "bias = np.zeros((d,))\n",
        "Model_discover.layers[0].set_weights([M_proper.T, bias])\n",
        "Model_discover.layers[0].trainable=False\n",
        "#Model_discover.layers[0].bias=False\n",
        "patience = 250\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "Ov7qWZD4mxr6",
        "outputId": "201f416f-1d34-4eb1-e399-c26c2d894bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [0 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Matrix M: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Model_discover.load_weights(filepath)\n",
        "M_discover_4_16_pts_32_R = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_4_16_pts_32_R,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_4_16_pts_32_R,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_4_16_pts_32_R, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGLSGuLPnLWq",
        "outputId": "aa45bc0f-685c-475c-9175-7fb1e482f171"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1.487331  , 0.15100947, 0.09213524, 1.1928607 , 0.        ,\n",
              "        0.        , 1.6716325 , 0.        , 0.01060625, 0.        ,\n",
              "        0.0036066 , 0.        , 0.04723004, 1.0608729 , 0.25785497,\n",
              "        0.0314549 ], dtype=float32), 0.37541217, 0.58100426)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#indices = np.array([0, 3, 6, 13]).astype(np.int64)#np.arange(5).astype(np.int64)\n",
        "np.sum(M_discover_4_16_pts_32_R,0), np.mean(np.sum(M_discover_4_16_pts_32_R,0)), np.std(np.sum(M_discover_4_16_pts_32_R,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TddxLHIv7G60"
      },
      "outputs": [],
      "source": [
        "M_proper=get_matrix(M_discover_4_16_pts_16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF9DGRt45IOf"
      },
      "source": [
        "## Run Proposed\n",
        "$$\\mathbb{Z}_5 : \\mathbb{Z}_{10} $$\n",
        "\n",
        "        Train Error (4096 pts): 0.092\n",
        "        Val Error: 0.087\n",
        "\n",
        "        Train Error (64 pts): 0.09\n",
        "        Val Error: 0.1329"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3js595wo5XRo"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = GroupInvarianceProposed(Z10, 64)\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9QHCm7P5XRs"
      },
      "outputs": [],
      "source": [
        "patience = 250\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y),callbacks=[reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgLBYpqJ5W5Y"
      },
      "outputs": [],
      "source": [
        "Model_discover.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an5vEH3j8uxD",
        "outputId": "41f468ae-d55c-4a3e-e2f2-090f69c644b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00000000e+00, -0.00000000e+00,  1.64344564e-01,\n",
              "        -0.00000000e+00,  3.99862111e-01,  3.75244133e-02,\n",
              "        -0.00000000e+00, -0.00000000e+00,  5.49242757e-02,\n",
              "         4.32574779e-01],\n",
              "       [ 2.16745570e-01,  4.20134701e-02,  9.82799977e-02,\n",
              "         1.38207421e-01,  8.45086202e-02,  2.11348087e-01,\n",
              "         8.15661922e-02,  3.81208695e-02,  5.56362309e-02,\n",
              "        -0.00000000e+00],\n",
              "       [ 2.10300043e-01,  6.45136982e-02,  1.11615397e-01,\n",
              "         3.74813020e-01,  2.96314247e-03,  8.01648647e-02,\n",
              "         9.45182517e-02, -0.00000000e+00,  3.88585329e-02,\n",
              "         1.76070362e-01],\n",
              "       [ 2.83901572e-01,  1.72558808e-04,  1.65148955e-02,\n",
              "        -0.00000000e+00,  2.68397063e-01,  1.63296256e-02,\n",
              "         1.58937961e-01,  2.57873863e-01,  1.76687747e-01,\n",
              "         2.41622716e-01],\n",
              "       [-0.00000000e+00,  3.82629156e-01,  5.48419077e-03,\n",
              "         6.49949312e-02, -0.00000000e+00,  2.99993515e-01,\n",
              "        -0.00000000e+00,  1.07380763e-01,  2.69144326e-01,\n",
              "        -0.00000000e+00],\n",
              "       [-0.00000000e+00,  3.92701514e-02,  3.08788810e-02,\n",
              "         7.98072759e-03, -0.00000000e+00,  9.26386286e-03,\n",
              "         3.17079877e-03,  1.06172776e-02,  1.67034157e-02,\n",
              "        -0.00000000e+00],\n",
              "       [-0.00000000e+00, -0.00000000e+00,  1.03414350e-03,\n",
              "        -0.00000000e+00,  3.24413963e-02, -0.00000000e+00,\n",
              "         2.56210915e-04,  1.29964901e-04, -0.00000000e+00,\n",
              "         1.03706315e-01],\n",
              "       [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
              "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
              "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
              "        -0.00000000e+00],\n",
              "       [-0.00000000e+00, -0.00000000e+00,  1.48707284e-02,\n",
              "        -0.00000000e+00,  1.56000108e-01, -0.00000000e+00,\n",
              "        -0.00000000e+00, -0.00000000e+00,  1.08504787e-01,\n",
              "        -0.00000000e+00],\n",
              "       [-0.00000000e+00, -0.00000000e+00,  8.29585940e-02,\n",
              "         1.66304484e-01, -0.00000000e+00, -0.00000000e+00,\n",
              "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
              "        -0.00000000e+00]], dtype=float32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Model_discover.layers[0].weights[0].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "_4CC0YjAfhOh",
        "outputId": "8b3b3163-966c-47a7-c00b-6e4b6c2cd7a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [4 0 3 0 1 0 9 0 4 3]\n",
            "Matrix M: [[-0.   0.2  0.2  0.3 -0.  -0.  -0.  -0.  -0.  -0. ]\n",
            " [-0.   0.   0.1  0.   0.4  0.  -0.  -0.  -0.  -0. ]\n",
            " [ 0.2  0.1  0.1  0.   0.   0.   0.  -0.   0.   0.1]\n",
            " [-0.   0.1  0.4 -0.   0.1  0.  -0.  -0.  -0.   0.2]\n",
            " [ 0.4  0.1  0.   0.3 -0.  -0.   0.  -0.   0.2 -0. ]\n",
            " [ 0.   0.2  0.1  0.   0.3  0.  -0.  -0.  -0.  -0. ]\n",
            " [-0.   0.1  0.1  0.2 -0.   0.   0.  -0.  -0.  -0. ]\n",
            " [-0.   0.  -0.   0.3  0.1  0.   0.  -0.  -0.  -0. ]\n",
            " [ 0.1  0.1  0.   0.2  0.3  0.  -0.  -0.   0.1 -0. ]\n",
            " [ 0.4 -0.   0.2  0.2 -0.  -0.   0.1 -0.  -0.  -0. ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M_discover_5_10 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_5_10,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_5_10,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_5_10, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMPVe_JgfmK3",
        "outputId": "0a4b8568-4435-4920-b957-a53e3e579a5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1.0892301 , 0.9664265 , 1.1538173 , 1.4204379 , 1.1296269 ,\n",
              "        0.11788511, 0.13756803, 0.        , 0.2793756 , 0.24926308],\n",
              "       dtype=float32), 0.65436304, 0.5134678)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(M_discover_5_10,0), np.mean(np.sum(M_discover_5_10,0)), np.std(np.sum(M_discover_5_10,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "mPU4lM887nLy",
        "outputId": "2be80c2a-b606-495e-b224-ff1331fcf23c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [5 2 3 5 0 0 5 0 4 0]\n",
            "Matrix M: [[-0.   0.   0.2 -0.   0.4  0.1 -0.  -0.   0.   0.1]\n",
            " [ 0.2  0.   0.1  0.   0.1  0.   0.  -0.   0.   0. ]\n",
            " [-0.   0.2  0.   0.2  0.  -0.  -0.  -0.  -0.  -0. ]\n",
            " [-0.   0.2  0.3 -0.   0.4 -0.  -0.  -0.  -0.  -0. ]\n",
            " [ 0.1  0.   0.2  0.   0.1  0.  -0.  -0.   0.1  0. ]\n",
            " [ 0.4  0.1  0.   0.4 -0.  -0.   0.1 -0.   0.  -0. ]\n",
            " [-0.   0.   0.   0.4  0.1 -0.  -0.  -0.   0.1 -0. ]\n",
            " [ 0.   0.   0.1  0.1  0.1  0.  -0.  -0.   0.   0. ]\n",
            " [ 0.3  0.2  0.1  0.   0.  -0.  -0.  -0.   0.   0.1]\n",
            " [-0.   0.2  0.   0.2  0.  -0.  -0.  -0.  -0.   0. ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M_discover_5_10 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_5_10,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_5_10,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_5_10, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "hJWA4hQklitS",
        "outputId": "e15945df-ae6d-435a-9062-0ab41a2666f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [8 4 0 8 0 0 0 0 0 0]\n",
            "Matrix M: [[ 0.   0.1  0.1  0.   0.2  0.   0.   0.   0.   0. ]\n",
            " [-0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0. ]\n",
            " [-0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.   0.  -0. ]\n",
            " [-0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0. ]\n",
            " [ 0.   0.2  0.   0.1  0.1  0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
            " [-0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0. ]\n",
            " [-0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0.  -0. ]\n",
            " [ 0.2  0.   0.1  0.2  0.   0.   0.   0.   0.   0. ]\n",
            " [ 0.1  0.   0.1  0.   0.1  0.   0.   0.   0.   0. ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M_discover_5_10 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_5_10,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_5_10,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_5_10, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "b1V_5FVFOYhj",
        "outputId": "77cebe1e-48a2-4fc0-8b8d-f11fdbdc686d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [7 2 0 3 2 0 0 0 0 0]\n",
            "Matrix M: [[ 0.   0.   0.1  0.   0.1  0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.1  0.   0.   0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.1  0.   0.   0.2  0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.   0.1  0.   0.   0.   0.   0.  -0. ]\n",
            " [ 0.   0.1  0.   0.1  0.   0.  -0.   0.   0.  -0. ]\n",
            " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.   0.1 -0.   0.   0.   0.   0.  -0. ]\n",
            " [ 0.1  0.   0.1  0.1 -0.   0.   0.   0.   0.  -0. ]\n",
            " [ 0.   0.   0.1  0.   0.   0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.  -0. ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M_discover_5_10 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_5_10,1)),0))\n",
        "print(\"Matrix M:\",np.round(M_discover_5_10,1))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_5_10, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "AiBQN8MlGk84",
        "outputId": "8e8268b3-653a-4db1-a7c0-851733c305e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "locs: [3 0 7 5 2 1 0 4 1 4]\n",
            "Matrix M: [[ 0.    0.    0.32  0.44  0.01  0.    0.    0.    0.   -0.  ]\n",
            " [ 0.21  0.    0.15  0.    0.01  0.    0.04  0.09 -0.    0.  ]\n",
            " [ 0.02 -0.   -0.   -0.    0.   -0.   -0.    0.26 -0.    0.08]\n",
            " [ 0.    0.    0.    0.    0.    0.25  0.13  0.12  0.14  0.01]\n",
            " [ 0.    0.    0.26  0.06  0.16  0.    0.   -0.    0.    0.11]\n",
            " [ 0.26  0.29  0.04  0.    0.    0.    0.02 -0.    0.    0.  ]\n",
            " [ 0.32  0.02  0.08  0.03  0.05  0.    0.    0.    0.14  0.  ]\n",
            " [ 0.23  0.08  0.17  0.05  0.32 -0.   -0.   -0.    0.02 -0.  ]\n",
            " [-0.    0.47  0.03  0.08 -0.   -0.    0.    0.21  0.01 -0.  ]\n",
            " [-0.    0.    0.01  0.08  0.28  0.08  0.   -0.    0.   -0.  ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "M_discover_5_10 = Model_discover.layers[0].weights[0].numpy().T\n",
        "print(\"locs:\",np.argmax(np.abs(np.round(M_discover_5_10,2)),1))\n",
        "print(\"Matrix M:\",np.round(M_discover_5_10,2))\n",
        "#Find arg max along each column\n",
        "#8,1,0,9,5,3,6,4,7\n",
        "plt.imshow(M_discover_5_10, cmap='BuGn', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52zm5lpRRBi9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXlmmkgLawwv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ya_uzcXQUE0"
      },
      "source": [
        "## Simple FC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBpu8v6uQWOU"
      },
      "outputs": [],
      "source": [
        "class Simple_FC(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Simple_FC, self).__init__()\n",
        "        activation = tf.keras.activations.tanh\n",
        "        self.features = [\n",
        "            #tf.keras.layers.Dense(10, activation),\n",
        "            tf.keras.layers.Dense(89, activation),\n",
        "            tf.keras.layers.Dense(6 * 64, activation),\n",
        "            tf.keras.layers.Dense(32, activation),\n",
        "            tf.keras.layers.Dense(1),\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = apply_layers(inputs, self.features)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7hHcwi1RCDy"
      },
      "source": [
        "## Run Simple FC\n",
        "$$\\mathbb{Z}_5 : \\mathbb{Z}_{10} $$\n",
        "\n",
        "        Train Error (4096 pts): --\n",
        "        Val Error: --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Kq2x2rjRCD1"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Simple_FC_Model\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "    \n",
        "np.random.seed(2048)\n",
        "Simple_FC_Model = Simple_FC()\n",
        "adam = Adam(lr=1e-3)\n",
        "Simple_FC_Model.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNaAoCsBRCD3",
        "outputId": "da1e9dbc-4404-48d4-89f9-d89eeed1a4ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((128, 10), (128,), (480, 10), (480,))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.shape, train_y.shape, val_ds.shape, val_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySz4AkWcRCD3",
        "outputId": "0080a738-5b2b-4a8f-92e7-606899c1ac72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.5685 - val_loss: 0.3384\n",
            "Epoch 2/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3776 - val_loss: 0.3744\n",
            "Epoch 3/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3700 - val_loss: 0.3180\n",
            "Epoch 4/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2971 - val_loss: 0.3617\n",
            "Epoch 5/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2763 - val_loss: 0.2947\n",
            "Epoch 6/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2512 - val_loss: 0.2865\n",
            "Epoch 7/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2434 - val_loss: 0.2595\n",
            "Epoch 8/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2513 - val_loss: 0.2651\n",
            "Epoch 9/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2655 - val_loss: 0.2521\n",
            "Epoch 10/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2533 - val_loss: 0.3057\n",
            "Epoch 11/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2937 - val_loss: 0.2541\n",
            "Epoch 12/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2246 - val_loss: 0.2247\n",
            "Epoch 13/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2173 - val_loss: 0.2313\n",
            "Epoch 14/2500\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.2069 - val_loss: 0.2161\n",
            "Epoch 15/2500\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.2187 - val_loss: 0.2135\n",
            "Epoch 16/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2134 - val_loss: 0.2697\n",
            "Epoch 17/2500\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.2278 - val_loss: 0.2036\n",
            "Epoch 18/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2065 - val_loss: 0.2193\n",
            "Epoch 19/2500\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2064 - val_loss: 0.2024\n",
            "Epoch 20/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.1963 - val_loss: 0.2179\n",
            "Epoch 21/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2086 - val_loss: 0.1833\n",
            "Epoch 22/2500\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1985 - val_loss: 0.1879\n",
            "Epoch 23/2500\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.2216 - val_loss: 0.2185\n",
            "Epoch 24/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2006 - val_loss: 0.2065\n",
            "Epoch 25/2500\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1851 - val_loss: 0.1697\n",
            "Epoch 26/2500\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.1954 - val_loss: 0.1999\n",
            "Epoch 27/2500\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.1845 - val_loss: 0.1792\n",
            "Epoch 28/2500\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.1797 - val_loss: 0.2218\n",
            "Epoch 29/2500\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.1871 - val_loss: 0.1766\n",
            "Epoch 30/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.1643 - val_loss: 0.1687\n",
            "Epoch 31/2500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.1756 - val_loss: 0.1991\n",
            "Epoch 32/2500\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.1950 - val_loss: 0.1978\n",
            "Epoch 33/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.1924 - val_loss: 0.1884\n",
            "Epoch 34/2500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.1842 - val_loss: 0.2195\n",
            "Epoch 35/2500\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1892 - val_loss: 0.1731\n",
            "Epoch 36/2500\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.1550 - val_loss: 0.2008\n",
            "Epoch 37/2500\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1564 - val_loss: 0.1607\n",
            "Epoch 38/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1388 - val_loss: 0.1629\n",
            "Epoch 39/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1389 - val_loss: 0.1655\n",
            "Epoch 40/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1314 - val_loss: 0.1820\n",
            "Epoch 41/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1570 - val_loss: 0.1740\n",
            "Epoch 42/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1742 - val_loss: 0.1975\n",
            "Epoch 43/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1607 - val_loss: 0.1693\n",
            "Epoch 44/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1589 - val_loss: 0.1657\n",
            "Epoch 45/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1401 - val_loss: 0.1657\n",
            "Epoch 46/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1384 - val_loss: 0.1677\n",
            "Epoch 47/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1471 - val_loss: 0.1748\n",
            "Epoch 48/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1337 - val_loss: 0.1558\n",
            "Epoch 49/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1245 - val_loss: 0.1582\n",
            "Epoch 50/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1248 - val_loss: 0.1567\n",
            "Epoch 51/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1165 - val_loss: 0.1574\n",
            "Epoch 52/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1291 - val_loss: 0.1640\n",
            "Epoch 53/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1221 - val_loss: 0.1706\n",
            "Epoch 54/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1158 - val_loss: 0.1534\n",
            "Epoch 55/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1108 - val_loss: 0.1719\n",
            "Epoch 56/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1204 - val_loss: 0.1700\n",
            "Epoch 57/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1327 - val_loss: 0.1671\n",
            "Epoch 58/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1206 - val_loss: 0.1687\n",
            "Epoch 59/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1202 - val_loss: 0.1721\n",
            "Epoch 60/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1350 - val_loss: 0.1581\n",
            "Epoch 61/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1088 - val_loss: 0.1655\n",
            "Epoch 62/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1182 - val_loss: 0.1545\n",
            "Epoch 63/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1333 - val_loss: 0.1702\n",
            "Epoch 64/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1361 - val_loss: 0.2010\n",
            "Epoch 65/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1215 - val_loss: 0.1790\n",
            "Epoch 66/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1252 - val_loss: 0.1835\n",
            "Epoch 67/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1207 - val_loss: 0.1509\n",
            "Epoch 68/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1043 - val_loss: 0.1531\n",
            "Epoch 69/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1003 - val_loss: 0.1495\n",
            "Epoch 70/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0995 - val_loss: 0.1557\n",
            "Epoch 71/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1038 - val_loss: 0.1767\n",
            "Epoch 72/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1085 - val_loss: 0.1755\n",
            "Epoch 73/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1040 - val_loss: 0.1680\n",
            "Epoch 74/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1286 - val_loss: 0.1696\n",
            "Epoch 75/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1125 - val_loss: 0.1592\n",
            "Epoch 76/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1013 - val_loss: 0.1591\n",
            "Epoch 77/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0920 - val_loss: 0.1669\n",
            "Epoch 78/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1157 - val_loss: 0.1754\n",
            "Epoch 79/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1060 - val_loss: 0.1623\n",
            "Epoch 80/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0923 - val_loss: 0.1512\n",
            "Epoch 81/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0857 - val_loss: 0.1603\n",
            "Epoch 82/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0864 - val_loss: 0.1588\n",
            "Epoch 83/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1032 - val_loss: 0.1577\n",
            "Epoch 84/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1018 - val_loss: 0.1575\n",
            "Epoch 85/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0838 - val_loss: 0.1619\n",
            "Epoch 86/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0833 - val_loss: 0.1600\n",
            "Epoch 87/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0838 - val_loss: 0.1544\n",
            "Epoch 88/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0802 - val_loss: 0.1598\n",
            "Epoch 89/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0810 - val_loss: 0.1529\n",
            "Epoch 90/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0849 - val_loss: 0.1653\n",
            "Epoch 91/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0909 - val_loss: 0.1580\n",
            "Epoch 92/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0948 - val_loss: 0.1514\n",
            "Epoch 93/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0871 - val_loss: 0.1698\n",
            "Epoch 94/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0931 - val_loss: 0.1570\n",
            "Epoch 95/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0913 - val_loss: 0.1897\n",
            "Epoch 96/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1002 - val_loss: 0.1570\n",
            "Epoch 97/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0928 - val_loss: 0.1566\n",
            "Epoch 98/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0828 - val_loss: 0.1610\n",
            "Epoch 99/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0863 - val_loss: 0.1627\n",
            "Epoch 100/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0816 - val_loss: 0.1556\n",
            "Epoch 101/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0716 - val_loss: 0.1543\n",
            "Epoch 102/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0725 - val_loss: 0.1655\n",
            "Epoch 103/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0753 - val_loss: 0.1656\n",
            "Epoch 104/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0925 - val_loss: 0.1537\n",
            "Epoch 105/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1089 - val_loss: 0.1918\n",
            "Epoch 106/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0778 - val_loss: 0.1655\n",
            "Epoch 107/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0919 - val_loss: 0.1629\n",
            "Epoch 108/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0767 - val_loss: 0.1598\n",
            "Epoch 109/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0693 - val_loss: 0.1692\n",
            "Epoch 110/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0801 - val_loss: 0.1586\n",
            "Epoch 111/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0723 - val_loss: 0.1700\n",
            "Epoch 112/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0756 - val_loss: 0.1536\n",
            "Epoch 113/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0663 - val_loss: 0.1682\n",
            "Epoch 114/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0844 - val_loss: 0.1633\n",
            "Epoch 115/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0732 - val_loss: 0.1598\n",
            "Epoch 116/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0762 - val_loss: 0.1568\n",
            "Epoch 117/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0699 - val_loss: 0.1622\n",
            "Epoch 118/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0658 - val_loss: 0.1604\n",
            "Epoch 119/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0601 - val_loss: 0.1643\n",
            "Epoch 120/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0586 - val_loss: 0.1567\n",
            "Epoch 121/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0590 - val_loss: 0.1726\n",
            "Epoch 122/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0847 - val_loss: 0.1821\n",
            "Epoch 123/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0936 - val_loss: 0.1569\n",
            "Epoch 124/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0910 - val_loss: 0.1679\n",
            "Epoch 125/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0635 - val_loss: 0.1694\n",
            "Epoch 126/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0739 - val_loss: 0.1610\n",
            "Epoch 127/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0745 - val_loss: 0.1661\n",
            "Epoch 128/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0669 - val_loss: 0.1576\n",
            "Epoch 129/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0642 - val_loss: 0.1631\n",
            "Epoch 130/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0857 - val_loss: 0.1647\n",
            "Epoch 131/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0931 - val_loss: 0.1800\n",
            "Epoch 132/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0675 - val_loss: 0.1602\n",
            "Epoch 133/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0707 - val_loss: 0.1685\n",
            "Epoch 134/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0545 - val_loss: 0.1574\n",
            "Epoch 135/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0686 - val_loss: 0.1686\n",
            "Epoch 136/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0659 - val_loss: 0.1701\n",
            "Epoch 137/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0697 - val_loss: 0.1756\n",
            "Epoch 138/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0720 - val_loss: 0.1664\n",
            "Epoch 139/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0622 - val_loss: 0.1751\n",
            "Epoch 140/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0673 - val_loss: 0.1628\n",
            "Epoch 141/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0613 - val_loss: 0.1597\n",
            "Epoch 142/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0711 - val_loss: 0.1707\n",
            "Epoch 143/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0840 - val_loss: 0.1640\n",
            "Epoch 144/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0942 - val_loss: 0.1615\n",
            "Epoch 145/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0832 - val_loss: 0.1927\n",
            "Epoch 146/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0854 - val_loss: 0.1695\n",
            "Epoch 147/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0830 - val_loss: 0.1838\n",
            "Epoch 148/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0692 - val_loss: 0.1659\n",
            "Epoch 149/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0852 - val_loss: 0.1781\n",
            "Epoch 150/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0666 - val_loss: 0.1602\n",
            "Epoch 151/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0559 - val_loss: 0.1559\n",
            "Epoch 152/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0560 - val_loss: 0.1722\n",
            "Epoch 153/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0637 - val_loss: 0.1625\n",
            "Epoch 154/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0641 - val_loss: 0.1778\n",
            "Epoch 155/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0602 - val_loss: 0.1679\n",
            "Epoch 156/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0628 - val_loss: 0.1772\n",
            "Epoch 157/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0649 - val_loss: 0.1604\n",
            "Epoch 158/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0546 - val_loss: 0.1800\n",
            "Epoch 159/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0557 - val_loss: 0.1562\n",
            "Epoch 160/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0493 - val_loss: 0.1736\n",
            "Epoch 161/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0596 - val_loss: 0.1607\n",
            "Epoch 162/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0716 - val_loss: 0.1672\n",
            "Epoch 163/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0921 - val_loss: 0.1933\n",
            "Epoch 164/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0745 - val_loss: 0.1629\n",
            "Epoch 165/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0582 - val_loss: 0.1736\n",
            "Epoch 166/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0709 - val_loss: 0.1687\n",
            "Epoch 167/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0808 - val_loss: 0.1694\n",
            "Epoch 168/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0588 - val_loss: 0.1669\n",
            "Epoch 169/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0522 - val_loss: 0.1624\n",
            "Epoch 170/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0557 - val_loss: 0.1658\n",
            "Epoch 171/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0565 - val_loss: 0.1637\n",
            "Epoch 172/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0472 - val_loss: 0.1683\n",
            "Epoch 173/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0611 - val_loss: 0.1775\n",
            "Epoch 174/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0662 - val_loss: 0.1597\n",
            "Epoch 175/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0627 - val_loss: 0.1724\n",
            "Epoch 176/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0541 - val_loss: 0.1634\n",
            "Epoch 177/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0652 - val_loss: 0.1732\n",
            "Epoch 178/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0832 - val_loss: 0.1893\n",
            "Epoch 179/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0971 - val_loss: 0.1682\n",
            "Epoch 180/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0777 - val_loss: 0.1832\n",
            "Epoch 181/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0687 - val_loss: 0.1656\n",
            "Epoch 182/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0646 - val_loss: 0.1714\n",
            "Epoch 183/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0593 - val_loss: 0.1731\n",
            "Epoch 184/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0572 - val_loss: 0.1586\n",
            "Epoch 185/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0467 - val_loss: 0.1756\n",
            "Epoch 186/2500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0558 - val_loss: 0.1677\n",
            "Epoch 187/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0500 - val_loss: 0.1713\n",
            "Epoch 188/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0496 - val_loss: 0.1713\n",
            "Epoch 189/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0457 - val_loss: 0.1637\n",
            "Epoch 190/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0379 - val_loss: 0.1698\n",
            "Epoch 191/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0448 - val_loss: 0.1676\n",
            "Epoch 192/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0454 - val_loss: 0.1623\n",
            "Epoch 193/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0381 - val_loss: 0.1654\n",
            "Epoch 194/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.1667\n",
            "Epoch 195/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0453 - val_loss: 0.1607\n",
            "Epoch 196/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0492 - val_loss: 0.1739\n",
            "Epoch 197/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0508 - val_loss: 0.1643\n",
            "Epoch 198/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0476 - val_loss: 0.1826\n",
            "Epoch 199/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0480 - val_loss: 0.1576\n",
            "Epoch 200/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0626 - val_loss: 0.1701\n",
            "Epoch 201/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0528 - val_loss: 0.1713\n",
            "Epoch 202/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0623 - val_loss: 0.1649\n",
            "Epoch 203/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0457 - val_loss: 0.1718\n",
            "Epoch 204/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0472 - val_loss: 0.1695\n",
            "Epoch 205/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0610 - val_loss: 0.1901\n",
            "Epoch 206/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0752 - val_loss: 0.1852\n",
            "Epoch 207/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0817 - val_loss: 0.1681\n",
            "Epoch 208/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0613 - val_loss: 0.1723\n",
            "Epoch 209/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0439 - val_loss: 0.1623\n",
            "Epoch 210/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0409 - val_loss: 0.1700\n",
            "Epoch 211/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0333 - val_loss: 0.1608\n",
            "Epoch 212/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0412 - val_loss: 0.1741\n",
            "Epoch 213/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0442 - val_loss: 0.1693\n",
            "Epoch 214/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0492 - val_loss: 0.1694\n",
            "Epoch 215/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0510 - val_loss: 0.1676\n",
            "Epoch 216/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0483 - val_loss: 0.1760\n",
            "Epoch 217/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0501 - val_loss: 0.1637\n",
            "Epoch 218/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0534 - val_loss: 0.1816\n",
            "Epoch 219/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0523 - val_loss: 0.1617\n",
            "Epoch 220/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0454 - val_loss: 0.1733\n",
            "Epoch 221/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0381 - val_loss: 0.1647\n",
            "Epoch 222/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0501 - val_loss: 0.1706\n",
            "Epoch 223/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0455 - val_loss: 0.1797\n",
            "Epoch 224/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0624 - val_loss: 0.1872\n",
            "Epoch 225/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0579 - val_loss: 0.1729\n",
            "Epoch 226/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0460 - val_loss: 0.1688\n",
            "Epoch 227/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0396 - val_loss: 0.1678\n",
            "Epoch 228/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0572 - val_loss: 0.1753\n",
            "Epoch 229/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0844 - val_loss: 0.1785\n",
            "Epoch 230/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0680 - val_loss: 0.1850\n",
            "Epoch 231/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0974 - val_loss: 0.1834\n",
            "Epoch 232/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0635 - val_loss: 0.1790\n",
            "Epoch 233/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0619 - val_loss: 0.1723\n",
            "Epoch 234/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0480 - val_loss: 0.1752\n",
            "Epoch 235/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0516 - val_loss: 0.1724\n",
            "Epoch 236/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0395 - val_loss: 0.1779\n",
            "Epoch 237/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 0.1713\n",
            "Epoch 238/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0554 - val_loss: 0.1732\n",
            "Epoch 239/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 0.1873\n",
            "Epoch 240/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0520 - val_loss: 0.1723\n",
            "Epoch 241/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0535 - val_loss: 0.1804\n",
            "Epoch 242/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0399 - val_loss: 0.1730\n",
            "Epoch 243/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0427 - val_loss: 0.1779\n",
            "Epoch 244/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0521 - val_loss: 0.1803\n",
            "Epoch 245/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0430 - val_loss: 0.1839\n",
            "Epoch 246/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0417 - val_loss: 0.1675\n",
            "Epoch 247/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0479 - val_loss: 0.1816\n",
            "Epoch 248/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0545 - val_loss: 0.1764\n",
            "Epoch 249/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0568 - val_loss: 0.1681\n",
            "Epoch 250/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0614 - val_loss: 0.1683\n",
            "Epoch 251/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0514 - val_loss: 0.1746\n",
            "Epoch 252/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0491 - val_loss: 0.1729\n",
            "Epoch 253/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0652 - val_loss: 0.1731\n",
            "Epoch 254/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0485 - val_loss: 0.1696\n",
            "Epoch 255/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0420 - val_loss: 0.1810\n",
            "Epoch 256/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0526 - val_loss: 0.1659\n",
            "Epoch 257/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0619 - val_loss: 0.1813\n",
            "Epoch 258/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0504 - val_loss: 0.1724\n",
            "Epoch 259/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0518 - val_loss: 0.1861\n",
            "Epoch 260/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0579 - val_loss: 0.1705\n",
            "Epoch 261/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0451 - val_loss: 0.1733\n",
            "Epoch 262/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0464 - val_loss: 0.1680\n",
            "Epoch 263/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0527 - val_loss: 0.1786\n",
            "Epoch 264/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0537 - val_loss: 0.1662\n",
            "Epoch 265/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0435 - val_loss: 0.1706\n",
            "Epoch 266/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0390 - val_loss: 0.1665\n",
            "Epoch 267/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0389 - val_loss: 0.1696\n",
            "Epoch 268/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0366 - val_loss: 0.1705\n",
            "Epoch 269/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0386 - val_loss: 0.1732\n",
            "Epoch 270/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0415 - val_loss: 0.1708\n",
            "Epoch 271/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0410 - val_loss: 0.1747\n",
            "Epoch 272/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0474 - val_loss: 0.1752\n",
            "Epoch 273/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0434 - val_loss: 0.1781\n",
            "Epoch 274/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0381 - val_loss: 0.1709\n",
            "Epoch 275/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0334 - val_loss: 0.1790\n",
            "Epoch 276/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0605 - val_loss: 0.1791\n",
            "Epoch 277/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0579 - val_loss: 0.1730\n",
            "Epoch 278/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0383 - val_loss: 0.1674\n",
            "Epoch 279/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 0.1732\n",
            "Epoch 280/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0315 - val_loss: 0.1679\n",
            "Epoch 281/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0292 - val_loss: 0.1667\n",
            "Epoch 282/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0308 - val_loss: 0.1757\n",
            "Epoch 283/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0316 - val_loss: 0.1729\n",
            "Epoch 284/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.1776\n",
            "Epoch 285/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0416 - val_loss: 0.1709\n",
            "Epoch 286/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0578 - val_loss: 0.1874\n",
            "Epoch 287/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0564 - val_loss: 0.1772\n",
            "Epoch 288/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0545 - val_loss: 0.1781\n",
            "Epoch 289/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0586 - val_loss: 0.1686\n",
            "Epoch 290/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0558 - val_loss: 0.1897\n",
            "Epoch 291/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0467 - val_loss: 0.1683\n",
            "Epoch 292/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0501 - val_loss: 0.1810\n",
            "Epoch 293/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.1645\n",
            "Epoch 294/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0431 - val_loss: 0.1817\n",
            "Epoch 295/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0580 - val_loss: 0.1750\n",
            "Epoch 296/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0614 - val_loss: 0.1717\n",
            "Epoch 297/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0573 - val_loss: 0.1733\n",
            "Epoch 298/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0576 - val_loss: 0.1657\n",
            "Epoch 299/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0578 - val_loss: 0.1906\n",
            "Epoch 300/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0648 - val_loss: 0.1788\n",
            "Epoch 301/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0560 - val_loss: 0.1801\n",
            "Epoch 302/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0641 - val_loss: 0.1853\n",
            "Epoch 303/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0778 - val_loss: 0.1678\n",
            "Epoch 304/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0498 - val_loss: 0.1857\n",
            "Epoch 305/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0601 - val_loss: 0.1741\n",
            "Epoch 306/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0604 - val_loss: 0.1796\n",
            "Epoch 307/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0802 - val_loss: 0.1903\n",
            "Epoch 308/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0627 - val_loss: 0.1718\n",
            "Epoch 309/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0468 - val_loss: 0.1738\n",
            "Epoch 310/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0473 - val_loss: 0.1795\n",
            "Epoch 311/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0434 - val_loss: 0.1720\n",
            "Epoch 312/2500\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0448 - val_loss: 0.1678\n",
            "Epoch 313/2500\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0528 - val_loss: 0.1884\n",
            "Epoch 314/2500\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0641 - val_loss: 0.1662\n",
            "Epoch 315/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0432 - val_loss: 0.1738\n",
            "Epoch 316/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.1749\n",
            "Epoch 317/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.1770\n",
            "Epoch 318/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0400 - val_loss: 0.1728\n",
            "Epoch 319/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0316 - val_loss: 0.1664\n",
            "Epoch 320/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0338 - val_loss: 0.1719\n",
            "Epoch 321/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0389 - val_loss: 0.1822\n",
            "Epoch 322/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0490 - val_loss: 0.1727\n",
            "Epoch 323/2500\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.0335 - val_loss: 0.1668\n",
            "Epoch 324/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0451 - val_loss: 0.1879\n",
            "Epoch 325/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0469 - val_loss: 0.1681\n",
            "Epoch 326/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0595 - val_loss: 0.2011\n",
            "Epoch 327/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0556 - val_loss: 0.1802\n",
            "Epoch 328/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0660 - val_loss: 0.1789\n",
            "Epoch 329/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0477 - val_loss: 0.1758\n",
            "Epoch 330/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0454 - val_loss: 0.1790\n",
            "Epoch 331/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0354 - val_loss: 0.1655\n",
            "Epoch 332/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0318 - val_loss: 0.1794\n",
            "Epoch 333/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0379 - val_loss: 0.1677\n",
            "Epoch 334/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0307 - val_loss: 0.1764\n",
            "Epoch 335/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0359 - val_loss: 0.1719\n",
            "Epoch 336/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0450 - val_loss: 0.1699\n",
            "Epoch 337/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0351 - val_loss: 0.1692\n",
            "Epoch 338/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0349 - val_loss: 0.1726\n",
            "Epoch 339/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0380 - val_loss: 0.1794\n",
            "Epoch 340/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0415 - val_loss: 0.1728\n",
            "Epoch 341/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0503 - val_loss: 0.1846\n",
            "Epoch 342/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0474 - val_loss: 0.1731\n",
            "Epoch 343/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0343 - val_loss: 0.1761\n",
            "Epoch 344/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0343 - val_loss: 0.1729\n",
            "Epoch 345/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.1709\n",
            "Epoch 346/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0401 - val_loss: 0.1649\n",
            "Epoch 347/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0340 - val_loss: 0.1763\n",
            "Epoch 348/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 0.1651\n",
            "Epoch 349/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0345 - val_loss: 0.1724\n",
            "Epoch 350/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0303 - val_loss: 0.1722\n",
            "Epoch 351/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0403 - val_loss: 0.1742\n",
            "Epoch 352/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0520 - val_loss: 0.1840\n",
            "Epoch 353/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0654 - val_loss: 0.1667\n",
            "Epoch 354/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0677 - val_loss: 0.1807\n",
            "Epoch 355/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0562 - val_loss: 0.1703\n",
            "Epoch 356/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0509 - val_loss: 0.1786\n",
            "Epoch 357/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0383 - val_loss: 0.1657\n",
            "Epoch 358/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0423 - val_loss: 0.1813\n",
            "Epoch 359/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0422 - val_loss: 0.1684\n",
            "Epoch 360/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0397 - val_loss: 0.1706\n",
            "Epoch 361/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0355 - val_loss: 0.1686\n",
            "Epoch 362/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0375 - val_loss: 0.1700\n",
            "Epoch 363/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0347 - val_loss: 0.1714\n",
            "Epoch 364/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0385 - val_loss: 0.1762\n",
            "Epoch 365/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0459 - val_loss: 0.1725\n",
            "Epoch 366/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0477 - val_loss: 0.1737\n",
            "Epoch 367/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0353 - val_loss: 0.1738\n",
            "Epoch 368/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0326 - val_loss: 0.1800\n",
            "Epoch 369/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0393 - val_loss: 0.1716\n",
            "Epoch 370/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0371 - val_loss: 0.1716\n",
            "Epoch 371/2500\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0310 - val_loss: 0.1695\n",
            "Epoch 372/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0335 - val_loss: 0.1727\n",
            "Epoch 373/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0401 - val_loss: 0.1727\n",
            "Epoch 374/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0386 - val_loss: 0.1688\n",
            "Epoch 375/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0392 - val_loss: 0.1797\n",
            "Epoch 376/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0409 - val_loss: 0.1671\n",
            "Epoch 377/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0418 - val_loss: 0.1804\n",
            "Epoch 378/2500\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0448 - val_loss: 0.1634\n",
            "Epoch 379/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0393 - val_loss: 0.1784\n",
            "Epoch 380/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0332 - val_loss: 0.1650\n",
            "Epoch 381/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0343 - val_loss: 0.1723\n",
            "Epoch 382/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0384 - val_loss: 0.1693\n",
            "Epoch 383/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0508 - val_loss: 0.1711\n",
            "Epoch 384/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0367 - val_loss: 0.1706\n",
            "Epoch 385/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0343 - val_loss: 0.1683\n",
            "Epoch 386/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0395 - val_loss: 0.1694\n",
            "Epoch 387/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.1697\n",
            "Epoch 388/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.1724\n",
            "Epoch 389/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0329 - val_loss: 0.1738\n",
            "Epoch 390/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0338 - val_loss: 0.1716\n",
            "Epoch 391/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0346 - val_loss: 0.1725\n",
            "Epoch 392/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0383 - val_loss: 0.1714\n",
            "Epoch 393/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0356 - val_loss: 0.1754\n",
            "Epoch 394/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0448 - val_loss: 0.1763\n",
            "Epoch 395/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0537 - val_loss: 0.1876\n",
            "Epoch 396/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0533 - val_loss: 0.1749\n",
            "Epoch 397/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0502 - val_loss: 0.1719\n",
            "Epoch 398/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0516 - val_loss: 0.1708\n",
            "Epoch 399/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0405 - val_loss: 0.1695\n",
            "Epoch 400/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0340 - val_loss: 0.1673\n",
            "Epoch 401/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0310 - val_loss: 0.1708\n",
            "Epoch 402/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0345 - val_loss: 0.1731\n",
            "Epoch 403/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0309 - val_loss: 0.1668\n",
            "Epoch 404/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0291 - val_loss: 0.1770\n",
            "Epoch 405/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0328 - val_loss: 0.1663\n",
            "Epoch 406/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0261 - val_loss: 0.1708\n",
            "Epoch 407/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0309 - val_loss: 0.1781\n",
            "Epoch 408/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0359 - val_loss: 0.1667\n",
            "Epoch 409/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0377 - val_loss: 0.1796\n",
            "Epoch 410/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0426 - val_loss: 0.1689\n",
            "Epoch 411/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0386 - val_loss: 0.1760\n",
            "Epoch 412/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0418 - val_loss: 0.1692\n",
            "Epoch 413/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0433 - val_loss: 0.1816\n",
            "Epoch 414/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0460 - val_loss: 0.1643\n",
            "Epoch 415/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0450 - val_loss: 0.1781\n",
            "Epoch 416/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0404 - val_loss: 0.1610\n",
            "Epoch 417/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0420 - val_loss: 0.1856\n",
            "Epoch 418/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0515 - val_loss: 0.1727\n",
            "Epoch 419/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0354 - val_loss: 0.1641\n",
            "Epoch 420/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0348 - val_loss: 0.1751\n",
            "Epoch 421/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0374 - val_loss: 0.1738\n",
            "Epoch 422/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0349 - val_loss: 0.1733\n",
            "Epoch 423/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0342 - val_loss: 0.1745\n",
            "Epoch 424/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0292 - val_loss: 0.1657\n",
            "Epoch 425/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0402 - val_loss: 0.1722\n",
            "Epoch 426/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0342 - val_loss: 0.1704\n",
            "Epoch 427/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0381 - val_loss: 0.1704\n",
            "Epoch 428/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0385 - val_loss: 0.1764\n",
            "Epoch 429/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0394 - val_loss: 0.1712\n",
            "Epoch 430/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 0.1695\n",
            "Epoch 431/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.1713\n",
            "Epoch 432/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0405 - val_loss: 0.1704\n",
            "Epoch 433/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0404 - val_loss: 0.1712\n",
            "Epoch 434/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0396 - val_loss: 0.1696\n",
            "Epoch 435/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0442 - val_loss: 0.1738\n",
            "Epoch 436/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0464 - val_loss: 0.1721\n",
            "Epoch 437/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0349 - val_loss: 0.1695\n",
            "Epoch 438/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0347 - val_loss: 0.1721\n",
            "Epoch 439/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0303 - val_loss: 0.1640\n",
            "Epoch 440/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0279 - val_loss: 0.1709\n",
            "Epoch 441/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0338 - val_loss: 0.1701\n",
            "Epoch 442/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0393 - val_loss: 0.1669\n",
            "Epoch 443/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0423 - val_loss: 0.1685\n",
            "Epoch 444/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0406 - val_loss: 0.1706\n",
            "Epoch 445/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0266 - val_loss: 0.1673\n",
            "Epoch 446/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.1697\n",
            "Epoch 447/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0290 - val_loss: 0.1638\n",
            "Epoch 448/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0238 - val_loss: 0.1717\n",
            "Epoch 449/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0266 - val_loss: 0.1632\n",
            "Epoch 450/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0358 - val_loss: 0.1772\n",
            "Epoch 451/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0368 - val_loss: 0.1730\n",
            "Epoch 452/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0323 - val_loss: 0.1699\n",
            "Epoch 453/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 0.1711\n",
            "Epoch 454/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0423 - val_loss: 0.1714\n",
            "Epoch 455/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0374 - val_loss: 0.1712\n",
            "Epoch 456/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.1704\n",
            "Epoch 457/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0435 - val_loss: 0.1880\n",
            "Epoch 458/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0481 - val_loss: 0.1707\n",
            "Epoch 459/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0353 - val_loss: 0.1676\n",
            "Epoch 460/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0394 - val_loss: 0.1710\n",
            "Epoch 461/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.1603\n",
            "Epoch 462/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0316 - val_loss: 0.1685\n",
            "Epoch 463/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0289 - val_loss: 0.1670\n",
            "Epoch 464/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0363 - val_loss: 0.1751\n",
            "Epoch 465/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0389 - val_loss: 0.1661\n",
            "Epoch 466/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0355 - val_loss: 0.1713\n",
            "Epoch 467/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0405 - val_loss: 0.1627\n",
            "Epoch 468/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0293 - val_loss: 0.1723\n",
            "Epoch 469/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0342 - val_loss: 0.1602\n",
            "Epoch 470/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0236 - val_loss: 0.1664\n",
            "Epoch 471/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0278 - val_loss: 0.1691\n",
            "Epoch 472/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0391 - val_loss: 0.1666\n",
            "Epoch 473/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0384 - val_loss: 0.1785\n",
            "Epoch 474/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0404 - val_loss: 0.1593\n",
            "Epoch 475/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0300 - val_loss: 0.1702\n",
            "Epoch 476/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.1528\n",
            "Epoch 477/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.1758\n",
            "Epoch 478/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0397 - val_loss: 0.1577\n",
            "Epoch 479/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0341 - val_loss: 0.1724\n",
            "Epoch 480/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0498 - val_loss: 0.1641\n",
            "Epoch 481/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0527 - val_loss: 0.1708\n",
            "Epoch 482/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.1669\n",
            "Epoch 483/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0393 - val_loss: 0.1677\n",
            "Epoch 484/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0462 - val_loss: 0.1639\n",
            "Epoch 485/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0485 - val_loss: 0.1659\n",
            "Epoch 486/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0390 - val_loss: 0.1717\n",
            "Epoch 487/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0419 - val_loss: 0.1574\n",
            "Epoch 488/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0298 - val_loss: 0.1686\n",
            "Epoch 489/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0255 - val_loss: 0.1592\n",
            "Epoch 490/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0305 - val_loss: 0.1656\n",
            "Epoch 491/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0320 - val_loss: 0.1640\n",
            "Epoch 492/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0463 - val_loss: 0.1718\n",
            "Epoch 493/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0478 - val_loss: 0.1764\n",
            "Epoch 494/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0335 - val_loss: 0.1614\n",
            "Epoch 495/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0362 - val_loss: 0.1680\n",
            "Epoch 496/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0338 - val_loss: 0.1659\n",
            "Epoch 497/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0364 - val_loss: 0.1697\n",
            "Epoch 498/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0374 - val_loss: 0.1695\n",
            "Epoch 499/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0367 - val_loss: 0.1707\n",
            "Epoch 500/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0422 - val_loss: 0.1679\n",
            "Epoch 501/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0441 - val_loss: 0.1714\n",
            "Epoch 502/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0393 - val_loss: 0.1661\n",
            "Epoch 503/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0369 - val_loss: 0.1671\n",
            "Epoch 504/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0415 - val_loss: 0.1743\n",
            "Epoch 505/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0363 - val_loss: 0.1670\n",
            "Epoch 506/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0329 - val_loss: 0.1657\n",
            "Epoch 507/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0262 - val_loss: 0.1673\n",
            "Epoch 508/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0347 - val_loss: 0.1721\n",
            "Epoch 509/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0370 - val_loss: 0.1677\n",
            "Epoch 510/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0452 - val_loss: 0.1682\n",
            "Epoch 511/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0334 - val_loss: 0.1753\n",
            "Epoch 512/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0394 - val_loss: 0.1605\n",
            "Epoch 513/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0342 - val_loss: 0.1687\n",
            "Epoch 514/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0369 - val_loss: 0.1652\n",
            "Epoch 515/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0397 - val_loss: 0.1648\n",
            "Epoch 516/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0401 - val_loss: 0.1753\n",
            "Epoch 517/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0501 - val_loss: 0.1733\n",
            "Epoch 518/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0454 - val_loss: 0.1704\n",
            "Epoch 519/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0466 - val_loss: 0.1729\n",
            "Epoch 520/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 0.1625\n",
            "Epoch 521/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0378 - val_loss: 0.1656\n",
            "Epoch 522/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0410 - val_loss: 0.1761\n",
            "Epoch 523/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0641 - val_loss: 0.1640\n",
            "Epoch 524/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0550 - val_loss: 0.1716\n",
            "Epoch 525/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0440 - val_loss: 0.1681\n",
            "Epoch 526/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0413 - val_loss: 0.1722\n",
            "Epoch 527/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0445 - val_loss: 0.1808\n",
            "Epoch 528/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0586 - val_loss: 0.1706\n",
            "Epoch 529/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0362 - val_loss: 0.1628\n",
            "Epoch 530/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0358 - val_loss: 0.1654\n",
            "Epoch 531/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0388 - val_loss: 0.1608\n",
            "Epoch 532/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0279 - val_loss: 0.1719\n",
            "Epoch 533/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0412 - val_loss: 0.1623\n",
            "Epoch 534/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0390 - val_loss: 0.1643\n",
            "Epoch 535/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0306 - val_loss: 0.1576\n",
            "Epoch 536/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.1651\n",
            "Epoch 537/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0264 - val_loss: 0.1632\n",
            "Epoch 538/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0260 - val_loss: 0.1692\n",
            "Epoch 539/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0262 - val_loss: 0.1636\n",
            "Epoch 540/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 0.1742\n",
            "Epoch 541/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0477 - val_loss: 0.1652\n",
            "Epoch 542/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0411 - val_loss: 0.1672\n",
            "Epoch 543/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0375 - val_loss: 0.1640\n",
            "Epoch 544/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.1715\n",
            "Epoch 545/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0414 - val_loss: 0.1700\n",
            "Epoch 546/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0596 - val_loss: 0.1623\n",
            "Epoch 547/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0367 - val_loss: 0.1664\n",
            "Epoch 548/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0366 - val_loss: 0.1646\n",
            "Epoch 549/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0351 - val_loss: 0.1623\n",
            "Epoch 550/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0357 - val_loss: 0.1660\n",
            "Epoch 551/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0309 - val_loss: 0.1644\n",
            "Epoch 552/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0290 - val_loss: 0.1678\n",
            "Epoch 553/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0322 - val_loss: 0.1672\n",
            "Epoch 554/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0477 - val_loss: 0.1773\n",
            "Epoch 555/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0411 - val_loss: 0.1659\n",
            "Epoch 556/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0359 - val_loss: 0.1719\n",
            "Epoch 557/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0299 - val_loss: 0.1626\n",
            "Epoch 558/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.1751\n",
            "Epoch 559/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0357 - val_loss: 0.1614\n",
            "Epoch 560/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0353 - val_loss: 0.1681\n",
            "Epoch 561/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.1654\n",
            "Epoch 562/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0267 - val_loss: 0.1700\n",
            "Epoch 563/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0305 - val_loss: 0.1645\n",
            "Epoch 564/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0409 - val_loss: 0.1642\n",
            "Epoch 565/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0357 - val_loss: 0.1663\n",
            "Epoch 566/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.1648\n",
            "Epoch 567/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0426 - val_loss: 0.1684\n",
            "Epoch 568/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0314 - val_loss: 0.1660\n",
            "Epoch 569/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0332 - val_loss: 0.1647\n",
            "Epoch 570/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1676\n",
            "Epoch 571/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0273 - val_loss: 0.1727\n",
            "Epoch 572/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 0.1700\n",
            "Epoch 573/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0452 - val_loss: 0.1675\n",
            "Epoch 574/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.1663\n",
            "Epoch 575/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0287 - val_loss: 0.1683\n",
            "Epoch 576/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0320 - val_loss: 0.1670\n",
            "Epoch 577/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.1691\n",
            "Epoch 578/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0314 - val_loss: 0.1653\n",
            "Epoch 579/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0276 - val_loss: 0.1728\n",
            "Epoch 580/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0331 - val_loss: 0.1653\n",
            "Epoch 581/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.1691\n",
            "Epoch 582/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0300 - val_loss: 0.1709\n",
            "Epoch 583/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.1672\n",
            "Epoch 584/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0230 - val_loss: 0.1721\n",
            "Epoch 585/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0250 - val_loss: 0.1663\n",
            "Epoch 586/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0391 - val_loss: 0.1750\n",
            "Epoch 587/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0421 - val_loss: 0.1707\n",
            "Epoch 588/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0428 - val_loss: 0.1721\n",
            "Epoch 589/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0410 - val_loss: 0.1721\n",
            "Epoch 590/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0410 - val_loss: 0.1708\n",
            "Epoch 591/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0459 - val_loss: 0.1662\n",
            "Epoch 592/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0442 - val_loss: 0.1712\n",
            "Epoch 593/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0299 - val_loss: 0.1737\n",
            "Epoch 594/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0341 - val_loss: 0.1741\n",
            "Epoch 595/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0372 - val_loss: 0.1722\n",
            "Epoch 596/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0299 - val_loss: 0.1678\n",
            "Epoch 597/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0304 - val_loss: 0.1698\n",
            "Epoch 598/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.1664\n",
            "Epoch 599/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0349 - val_loss: 0.1763\n",
            "Epoch 600/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0498 - val_loss: 0.1772\n",
            "Epoch 601/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0351 - val_loss: 0.1669\n",
            "Epoch 602/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0471 - val_loss: 0.1695\n",
            "Epoch 603/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0327 - val_loss: 0.1648\n",
            "Epoch 604/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0279 - val_loss: 0.1677\n",
            "Epoch 605/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0281 - val_loss: 0.1712\n",
            "Epoch 606/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0330 - val_loss: 0.1712\n",
            "Epoch 607/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - val_loss: 0.1623\n",
            "Epoch 608/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1654\n",
            "Epoch 609/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.1667\n",
            "Epoch 610/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.1661\n",
            "Epoch 611/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0357 - val_loss: 0.1656\n",
            "Epoch 612/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.1683\n",
            "Epoch 613/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0362 - val_loss: 0.1661\n",
            "Epoch 614/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0387 - val_loss: 0.1675\n",
            "Epoch 615/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0384 - val_loss: 0.1760\n",
            "Epoch 616/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0371 - val_loss: 0.1628\n",
            "Epoch 617/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0346 - val_loss: 0.1710\n",
            "Epoch 618/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0301 - val_loss: 0.1628\n",
            "Epoch 619/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0358 - val_loss: 0.1718\n",
            "Epoch 620/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0418 - val_loss: 0.1635\n",
            "Epoch 621/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0385 - val_loss: 0.1748\n",
            "Epoch 622/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0366 - val_loss: 0.1614\n",
            "Epoch 623/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0334 - val_loss: 0.1658\n",
            "Epoch 624/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0332 - val_loss: 0.1657\n",
            "Epoch 625/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0351 - val_loss: 0.1681\n",
            "Epoch 626/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0371 - val_loss: 0.1642\n",
            "Epoch 627/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.1713\n",
            "Epoch 628/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0375 - val_loss: 0.1608\n",
            "Epoch 629/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0339 - val_loss: 0.1656\n",
            "Epoch 630/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0391 - val_loss: 0.1647\n",
            "Epoch 631/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0412 - val_loss: 0.1631\n",
            "Epoch 632/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0480 - val_loss: 0.1625\n",
            "Epoch 633/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0360 - val_loss: 0.1660\n",
            "Epoch 634/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0318 - val_loss: 0.1633\n",
            "Epoch 635/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0278 - val_loss: 0.1677\n",
            "Epoch 636/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0333 - val_loss: 0.1668\n",
            "Epoch 637/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0243 - val_loss: 0.1703\n",
            "Epoch 638/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0267 - val_loss: 0.1625\n",
            "Epoch 639/2500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0324 - val_loss: 0.1761\n",
            "Epoch 640/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0456 - val_loss: 0.1662\n",
            "Epoch 641/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0458 - val_loss: 0.1635\n",
            "Epoch 642/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0365 - val_loss: 0.1740\n",
            "Epoch 643/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0399 - val_loss: 0.1593\n",
            "Epoch 644/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0349 - val_loss: 0.1668\n",
            "Epoch 645/2500\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0329 - val_loss: 0.1661\n",
            "Epoch 646/2500\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0301 - val_loss: 0.1734\n",
            "Epoch 647/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0430 - val_loss: 0.1683\n",
            "Epoch 648/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0320 - val_loss: 0.1620\n",
            "Epoch 649/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0306 - val_loss: 0.1726\n",
            "Epoch 650/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0340 - val_loss: 0.1625\n",
            "Epoch 651/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0422 - val_loss: 0.1617\n",
            "Epoch 652/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0406 - val_loss: 0.1838\n",
            "Epoch 653/2500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0526 - val_loss: 0.1586\n",
            "Epoch 654/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0408 - val_loss: 0.1711\n",
            "Epoch 655/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0386 - val_loss: 0.1656\n",
            "Epoch 656/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.1627\n",
            "Epoch 657/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0399 - val_loss: 0.1696\n",
            "Epoch 658/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0309 - val_loss: 0.1625\n",
            "Epoch 659/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 0.1615\n",
            "Epoch 660/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.1663\n",
            "Epoch 661/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0379 - val_loss: 0.1621\n",
            "Epoch 662/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0407 - val_loss: 0.1619\n",
            "Epoch 663/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.1808\n",
            "Epoch 664/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0400 - val_loss: 0.1625\n",
            "Epoch 665/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.1646\n",
            "Epoch 666/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.1692\n",
            "Epoch 667/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0300 - val_loss: 0.1605\n",
            "Epoch 668/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0309 - val_loss: 0.1718\n",
            "Epoch 669/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0323 - val_loss: 0.1604\n",
            "Epoch 670/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0322 - val_loss: 0.1706\n",
            "Epoch 671/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0395 - val_loss: 0.1603\n",
            "Epoch 672/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - val_loss: 0.1646\n",
            "Epoch 673/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0406 - val_loss: 0.1654\n",
            "Epoch 674/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0392 - val_loss: 0.1610\n",
            "Epoch 675/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 0.1706\n",
            "Epoch 676/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0381 - val_loss: 0.1613\n",
            "Epoch 677/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0306 - val_loss: 0.1623\n",
            "Epoch 678/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0361 - val_loss: 0.1645\n",
            "Epoch 679/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0346 - val_loss: 0.1734\n",
            "Epoch 680/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0417 - val_loss: 0.1674\n",
            "Epoch 681/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0511 - val_loss: 0.1570\n",
            "Epoch 682/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0350 - val_loss: 0.1612\n",
            "Epoch 683/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0341 - val_loss: 0.1589\n",
            "Epoch 684/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1619\n",
            "Epoch 685/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0374 - val_loss: 0.1603\n",
            "Epoch 686/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0275 - val_loss: 0.1668\n",
            "Epoch 687/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 0.1642\n",
            "Epoch 688/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0307 - val_loss: 0.1607\n",
            "Epoch 689/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.1640\n",
            "Epoch 690/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0319 - val_loss: 0.1593\n",
            "Epoch 691/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.1668\n",
            "Epoch 692/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0323 - val_loss: 0.1600\n",
            "Epoch 693/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0333 - val_loss: 0.1653\n",
            "Epoch 694/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0285 - val_loss: 0.1620\n",
            "Epoch 695/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0260 - val_loss: 0.1606\n",
            "Epoch 696/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0222 - val_loss: 0.1627\n",
            "Epoch 697/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0236 - val_loss: 0.1621\n",
            "Epoch 698/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0252 - val_loss: 0.1614\n",
            "Epoch 699/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0258 - val_loss: 0.1613\n",
            "Epoch 700/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 0.1641\n",
            "Epoch 701/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0214 - val_loss: 0.1584\n",
            "Epoch 702/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0237 - val_loss: 0.1653\n",
            "Epoch 703/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0275 - val_loss: 0.1623\n",
            "Epoch 704/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0285 - val_loss: 0.1668\n",
            "Epoch 705/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.1568\n",
            "Epoch 706/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.1644\n",
            "Epoch 707/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0438 - val_loss: 0.1653\n",
            "Epoch 708/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.1652\n",
            "Epoch 709/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.1637\n",
            "Epoch 710/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0247 - val_loss: 0.1613\n",
            "Epoch 711/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.1614\n",
            "Epoch 712/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0315 - val_loss: 0.1620\n",
            "Epoch 713/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.1622\n",
            "Epoch 714/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0304 - val_loss: 0.1589\n",
            "Epoch 715/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0261 - val_loss: 0.1641\n",
            "Epoch 716/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0310 - val_loss: 0.1633\n",
            "Epoch 717/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0354 - val_loss: 0.1600\n",
            "Epoch 718/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0470 - val_loss: 0.1608\n",
            "Epoch 719/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0442 - val_loss: 0.1697\n",
            "Epoch 720/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0457 - val_loss: 0.1696\n",
            "Epoch 721/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0435 - val_loss: 0.1638\n",
            "Epoch 722/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0314 - val_loss: 0.1663\n",
            "Epoch 723/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0261 - val_loss: 0.1634\n",
            "Epoch 724/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0268 - val_loss: 0.1629\n",
            "Epoch 725/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0280 - val_loss: 0.1646\n",
            "Epoch 726/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0281 - val_loss: 0.1594\n",
            "Epoch 727/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0267 - val_loss: 0.1560\n",
            "Epoch 728/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0301 - val_loss: 0.1646\n",
            "Epoch 729/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0389 - val_loss: 0.1649\n",
            "Epoch 730/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0403 - val_loss: 0.1669\n",
            "Epoch 731/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0316 - val_loss: 0.1549\n",
            "Epoch 732/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0352 - val_loss: 0.1608\n",
            "Epoch 733/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0345 - val_loss: 0.1649\n",
            "Epoch 734/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0385 - val_loss: 0.1612\n",
            "Epoch 735/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0306 - val_loss: 0.1587\n",
            "Epoch 736/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0214 - val_loss: 0.1614\n",
            "Epoch 737/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0367 - val_loss: 0.1614\n",
            "Epoch 738/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0351 - val_loss: 0.1650\n",
            "Epoch 739/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.1569\n",
            "Epoch 740/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0261 - val_loss: 0.1702\n",
            "Epoch 741/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0283 - val_loss: 0.1588\n",
            "Epoch 742/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0233 - val_loss: 0.1640\n",
            "Epoch 743/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0224 - val_loss: 0.1611\n",
            "Epoch 744/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0208 - val_loss: 0.1596\n",
            "Epoch 745/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.1569\n",
            "Epoch 746/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.1603\n",
            "Epoch 747/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0321 - val_loss: 0.1634\n",
            "Epoch 748/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0406 - val_loss: 0.1540\n",
            "Epoch 749/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0250 - val_loss: 0.1646\n",
            "Epoch 750/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0269 - val_loss: 0.1561\n",
            "Epoch 751/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0289 - val_loss: 0.1653\n",
            "Epoch 752/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0278 - val_loss: 0.1553\n",
            "Epoch 753/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0250 - val_loss: 0.1642\n",
            "Epoch 754/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0266 - val_loss: 0.1560\n",
            "Epoch 755/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0273 - val_loss: 0.1636\n",
            "Epoch 756/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0291 - val_loss: 0.1548\n",
            "Epoch 757/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0290 - val_loss: 0.1632\n",
            "Epoch 758/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0396 - val_loss: 0.1542\n",
            "Epoch 759/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0421 - val_loss: 0.1625\n",
            "Epoch 760/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0323 - val_loss: 0.1555\n",
            "Epoch 761/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0319 - val_loss: 0.1619\n",
            "Epoch 762/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0269 - val_loss: 0.1644\n",
            "Epoch 763/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0214 - val_loss: 0.1566\n",
            "Epoch 764/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0310 - val_loss: 0.1648\n",
            "Epoch 765/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0303 - val_loss: 0.1591\n",
            "Epoch 766/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0396 - val_loss: 0.1681\n",
            "Epoch 767/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0426 - val_loss: 0.1597\n",
            "Epoch 768/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.1575\n",
            "Epoch 769/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0251 - val_loss: 0.1602\n",
            "Epoch 770/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.1571\n",
            "Epoch 771/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.1610\n",
            "Epoch 772/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0187 - val_loss: 0.1590\n",
            "Epoch 773/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 0.1605\n",
            "Epoch 774/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0251 - val_loss: 0.1607\n",
            "Epoch 775/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0263 - val_loss: 0.1600\n",
            "Epoch 776/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1600\n",
            "Epoch 777/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.1575\n",
            "Epoch 778/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0200 - val_loss: 0.1621\n",
            "Epoch 779/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0218 - val_loss: 0.1539\n",
            "Epoch 780/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0279 - val_loss: 0.1647\n",
            "Epoch 781/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0288 - val_loss: 0.1569\n",
            "Epoch 782/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.1673\n",
            "Epoch 783/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0357 - val_loss: 0.1570\n",
            "Epoch 784/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0296 - val_loss: 0.1588\n",
            "Epoch 785/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0346 - val_loss: 0.1645\n",
            "Epoch 786/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0333 - val_loss: 0.1528\n",
            "Epoch 787/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.1683\n",
            "Epoch 788/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0426 - val_loss: 0.1538\n",
            "Epoch 789/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0365 - val_loss: 0.1539\n",
            "Epoch 790/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0291 - val_loss: 0.1565\n",
            "Epoch 791/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0292 - val_loss: 0.1557\n",
            "Epoch 792/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0248 - val_loss: 0.1570\n",
            "Epoch 793/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1627\n",
            "Epoch 794/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0240 - val_loss: 0.1553\n",
            "Epoch 795/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0259 - val_loss: 0.1574\n",
            "Epoch 796/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0268 - val_loss: 0.1564\n",
            "Epoch 797/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0424 - val_loss: 0.1615\n",
            "Epoch 798/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0344 - val_loss: 0.1558\n",
            "Epoch 799/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0331 - val_loss: 0.1603\n",
            "Epoch 800/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.1580\n",
            "Epoch 801/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0284 - val_loss: 0.1622\n",
            "Epoch 802/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.1561\n",
            "Epoch 803/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0376 - val_loss: 0.1654\n",
            "Epoch 804/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0273 - val_loss: 0.1544\n",
            "Epoch 805/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0310 - val_loss: 0.1682\n",
            "Epoch 806/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0288 - val_loss: 0.1555\n",
            "Epoch 807/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0259 - val_loss: 0.1634\n",
            "Epoch 808/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1544\n",
            "Epoch 809/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.1560\n",
            "Epoch 810/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0229 - val_loss: 0.1647\n",
            "Epoch 811/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.1500\n",
            "Epoch 812/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0254 - val_loss: 0.1604\n",
            "Epoch 813/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.1529\n",
            "Epoch 814/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0225 - val_loss: 0.1627\n",
            "Epoch 815/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0258 - val_loss: 0.1527\n",
            "Epoch 816/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0226 - val_loss: 0.1598\n",
            "Epoch 817/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0269 - val_loss: 0.1535\n",
            "Epoch 818/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 0.1584\n",
            "Epoch 819/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.1544\n",
            "Epoch 820/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0246 - val_loss: 0.1609\n",
            "Epoch 821/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0266 - val_loss: 0.1570\n",
            "Epoch 822/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0376 - val_loss: 0.1580\n",
            "Epoch 823/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0277 - val_loss: 0.1582\n",
            "Epoch 824/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0266 - val_loss: 0.1591\n",
            "Epoch 825/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0300 - val_loss: 0.1639\n",
            "Epoch 826/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0237 - val_loss: 0.1579\n",
            "Epoch 827/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0231 - val_loss: 0.1539\n",
            "Epoch 828/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.1619\n",
            "Epoch 829/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0341 - val_loss: 0.1570\n",
            "Epoch 830/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0369 - val_loss: 0.1573\n",
            "Epoch 831/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0346 - val_loss: 0.1596\n",
            "Epoch 832/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0337 - val_loss: 0.1633\n",
            "Epoch 833/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.1645\n",
            "Epoch 834/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0522 - val_loss: 0.1598\n",
            "Epoch 835/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0388 - val_loss: 0.1646\n",
            "Epoch 836/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0327 - val_loss: 0.1575\n",
            "Epoch 837/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0345 - val_loss: 0.1630\n",
            "Epoch 838/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0399 - val_loss: 0.1698\n",
            "Epoch 839/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0499 - val_loss: 0.1666\n",
            "Epoch 840/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0480 - val_loss: 0.1653\n",
            "Epoch 841/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0348 - val_loss: 0.1608\n",
            "Epoch 842/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0352 - val_loss: 0.1641\n",
            "Epoch 843/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.1579\n",
            "Epoch 844/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0300 - val_loss: 0.1621\n",
            "Epoch 845/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0391 - val_loss: 0.1598\n",
            "Epoch 846/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0276 - val_loss: 0.1628\n",
            "Epoch 847/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1588\n",
            "Epoch 848/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.1607\n",
            "Epoch 849/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0234 - val_loss: 0.1582\n",
            "Epoch 850/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0306 - val_loss: 0.1633\n",
            "Epoch 851/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.1627\n",
            "Epoch 852/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0265 - val_loss: 0.1602\n",
            "Epoch 853/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.1630\n",
            "Epoch 854/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0376 - val_loss: 0.1587\n",
            "Epoch 855/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0304 - val_loss: 0.1646\n",
            "Epoch 856/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 0.1558\n",
            "Epoch 857/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0287 - val_loss: 0.1579\n",
            "Epoch 858/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.1564\n",
            "Epoch 859/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0296 - val_loss: 0.1605\n",
            "Epoch 860/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0305 - val_loss: 0.1610\n",
            "Epoch 861/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0316 - val_loss: 0.1583\n",
            "Epoch 862/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1628\n",
            "Epoch 863/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0352 - val_loss: 0.1581\n",
            "Epoch 864/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0302 - val_loss: 0.1600\n",
            "Epoch 865/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0336 - val_loss: 0.1591\n",
            "Epoch 866/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0284 - val_loss: 0.1613\n",
            "Epoch 867/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0325 - val_loss: 0.1668\n",
            "Epoch 868/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0238 - val_loss: 0.1523\n",
            "Epoch 869/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 0.1602\n",
            "Epoch 870/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0274 - val_loss: 0.1565\n",
            "Epoch 871/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0322 - val_loss: 0.1652\n",
            "Epoch 872/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0282 - val_loss: 0.1581\n",
            "Epoch 873/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0247 - val_loss: 0.1590\n",
            "Epoch 874/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0243 - val_loss: 0.1587\n",
            "Epoch 875/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0269 - val_loss: 0.1585\n",
            "Epoch 876/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1615\n",
            "Epoch 877/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0190 - val_loss: 0.1569\n",
            "Epoch 878/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0230 - val_loss: 0.1589\n",
            "Epoch 879/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0215 - val_loss: 0.1561\n",
            "Epoch 880/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.1557\n",
            "Epoch 881/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0197 - val_loss: 0.1590\n",
            "Epoch 882/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0266 - val_loss: 0.1600\n",
            "Epoch 883/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0313 - val_loss: 0.1602\n",
            "Epoch 884/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.1558\n",
            "Epoch 885/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0263 - val_loss: 0.1589\n",
            "Epoch 886/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1547\n",
            "Epoch 887/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0205 - val_loss: 0.1555\n",
            "Epoch 888/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.1583\n",
            "Epoch 889/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.1558\n",
            "Epoch 890/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.1523\n",
            "Epoch 891/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0260 - val_loss: 0.1634\n",
            "Epoch 892/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0310 - val_loss: 0.1577\n",
            "Epoch 893/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.1634\n",
            "Epoch 894/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0421 - val_loss: 0.1633\n",
            "Epoch 895/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0445 - val_loss: 0.1603\n",
            "Epoch 896/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0362 - val_loss: 0.1552\n",
            "Epoch 897/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0400 - val_loss: 0.1578\n",
            "Epoch 898/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0323 - val_loss: 0.1638\n",
            "Epoch 899/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0294 - val_loss: 0.1517\n",
            "Epoch 900/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0279 - val_loss: 0.1606\n",
            "Epoch 901/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.1539\n",
            "Epoch 902/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0266 - val_loss: 0.1609\n",
            "Epoch 903/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.1581\n",
            "Epoch 904/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0228 - val_loss: 0.1591\n",
            "Epoch 905/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 0.1588\n",
            "Epoch 906/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0223 - val_loss: 0.1570\n",
            "Epoch 907/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0314 - val_loss: 0.1578\n",
            "Epoch 908/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0324 - val_loss: 0.1597\n",
            "Epoch 909/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0332 - val_loss: 0.1564\n",
            "Epoch 910/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0281 - val_loss: 0.1582\n",
            "Epoch 911/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0199 - val_loss: 0.1608\n",
            "Epoch 912/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0303 - val_loss: 0.1566\n",
            "Epoch 913/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1625\n",
            "Epoch 914/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 0.1678\n",
            "Epoch 915/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0336 - val_loss: 0.1579\n",
            "Epoch 916/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0291 - val_loss: 0.1635\n",
            "Epoch 917/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1535\n",
            "Epoch 918/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0269 - val_loss: 0.1625\n",
            "Epoch 919/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.1557\n",
            "Epoch 920/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0287 - val_loss: 0.1611\n",
            "Epoch 921/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0227 - val_loss: 0.1617\n",
            "Epoch 922/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1599\n",
            "Epoch 923/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0225 - val_loss: 0.1618\n",
            "Epoch 924/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0248 - val_loss: 0.1521\n",
            "Epoch 925/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0357 - val_loss: 0.1632\n",
            "Epoch 926/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0339 - val_loss: 0.1551\n",
            "Epoch 927/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0354 - val_loss: 0.1624\n",
            "Epoch 928/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0294 - val_loss: 0.1581\n",
            "Epoch 929/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0292 - val_loss: 0.1580\n",
            "Epoch 930/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0335 - val_loss: 0.1674\n",
            "Epoch 931/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0408 - val_loss: 0.1582\n",
            "Epoch 932/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0375 - val_loss: 0.1537\n",
            "Epoch 933/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0306 - val_loss: 0.1588\n",
            "Epoch 934/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0379 - val_loss: 0.1594\n",
            "Epoch 935/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.1645\n",
            "Epoch 936/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.1588\n",
            "Epoch 937/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0347 - val_loss: 0.1632\n",
            "Epoch 938/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.1615\n",
            "Epoch 939/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0280 - val_loss: 0.1610\n",
            "Epoch 940/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1579\n",
            "Epoch 941/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0211 - val_loss: 0.1554\n",
            "Epoch 942/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0291 - val_loss: 0.1581\n",
            "Epoch 943/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0215 - val_loss: 0.1587\n",
            "Epoch 944/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0259 - val_loss: 0.1549\n",
            "Epoch 945/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0225 - val_loss: 0.1598\n",
            "Epoch 946/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0252 - val_loss: 0.1581\n",
            "Epoch 947/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0292 - val_loss: 0.1627\n",
            "Epoch 948/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.1631\n",
            "Epoch 949/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0339 - val_loss: 0.1593\n",
            "Epoch 950/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0313 - val_loss: 0.1680\n",
            "Epoch 951/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0289 - val_loss: 0.1586\n",
            "Epoch 952/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1577\n",
            "Epoch 953/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.1569\n",
            "Epoch 954/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0213 - val_loss: 0.1594\n",
            "Epoch 955/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0268 - val_loss: 0.1607\n",
            "Epoch 956/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0237 - val_loss: 0.1593\n",
            "Epoch 957/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0201 - val_loss: 0.1590\n",
            "Epoch 958/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.1578\n",
            "Epoch 959/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0194 - val_loss: 0.1593\n",
            "Epoch 960/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0235 - val_loss: 0.1557\n",
            "Epoch 961/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0244 - val_loss: 0.1604\n",
            "Epoch 962/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.1561\n",
            "Epoch 963/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0247 - val_loss: 0.1614\n",
            "Epoch 964/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0241 - val_loss: 0.1604\n",
            "Epoch 965/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.1602\n",
            "Epoch 966/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0280 - val_loss: 0.1598\n",
            "Epoch 967/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0335 - val_loss: 0.1660\n",
            "Epoch 968/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0254 - val_loss: 0.1585\n",
            "Epoch 969/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0318 - val_loss: 0.1607\n",
            "Epoch 970/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0410 - val_loss: 0.1622\n",
            "Epoch 971/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 0.1583\n",
            "Epoch 972/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0223 - val_loss: 0.1606\n",
            "Epoch 973/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0228 - val_loss: 0.1554\n",
            "Epoch 974/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1626\n",
            "Epoch 975/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0412 - val_loss: 0.1595\n",
            "Epoch 976/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0327 - val_loss: 0.1673\n",
            "Epoch 977/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0496 - val_loss: 0.1646\n",
            "Epoch 978/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0600 - val_loss: 0.1733\n",
            "Epoch 979/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0436 - val_loss: 0.1608\n",
            "Epoch 980/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0390 - val_loss: 0.1610\n",
            "Epoch 981/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 0.1602\n",
            "Epoch 982/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.1605\n",
            "Epoch 983/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.1580\n",
            "Epoch 984/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0248 - val_loss: 0.1580\n",
            "Epoch 985/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.1651\n",
            "Epoch 986/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.1521\n",
            "Epoch 987/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0256 - val_loss: 0.1643\n",
            "Epoch 988/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0303 - val_loss: 0.1581\n",
            "Epoch 989/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0297 - val_loss: 0.1598\n",
            "Epoch 990/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.1535\n",
            "Epoch 991/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0376 - val_loss: 0.1635\n",
            "Epoch 992/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.1572\n",
            "Epoch 993/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0304 - val_loss: 0.1538\n",
            "Epoch 994/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0274 - val_loss: 0.1536\n",
            "Epoch 995/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0209 - val_loss: 0.1596\n",
            "Epoch 996/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.1580\n",
            "Epoch 997/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0269 - val_loss: 0.1581\n",
            "Epoch 998/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.1629\n",
            "Epoch 999/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0313 - val_loss: 0.1587\n",
            "Epoch 1000/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0311 - val_loss: 0.1699\n",
            "Epoch 1001/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0368 - val_loss: 0.1611\n",
            "Epoch 1002/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0392 - val_loss: 0.1664\n",
            "Epoch 1003/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0350 - val_loss: 0.1580\n",
            "Epoch 1004/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0354 - val_loss: 0.1610\n",
            "Epoch 1005/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0338 - val_loss: 0.1578\n",
            "Epoch 1006/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0432 - val_loss: 0.1676\n",
            "Epoch 1007/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0487 - val_loss: 0.1538\n",
            "Epoch 1008/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0360 - val_loss: 0.1610\n",
            "Epoch 1009/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0378 - val_loss: 0.1566\n",
            "Epoch 1010/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0322 - val_loss: 0.1536\n",
            "Epoch 1011/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0290 - val_loss: 0.1584\n",
            "Epoch 1012/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1615\n",
            "Epoch 1013/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0349 - val_loss: 0.1645\n",
            "Epoch 1014/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0373 - val_loss: 0.1570\n",
            "Epoch 1015/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0284 - val_loss: 0.1636\n",
            "Epoch 1016/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0301 - val_loss: 0.1604\n",
            "Epoch 1017/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0245 - val_loss: 0.1553\n",
            "Epoch 1018/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1586\n",
            "Epoch 1019/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0218 - val_loss: 0.1564\n",
            "Epoch 1020/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0259 - val_loss: 0.1605\n",
            "Epoch 1021/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.1558\n",
            "Epoch 1022/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.1632\n",
            "Epoch 1023/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0245 - val_loss: 0.1555\n",
            "Epoch 1024/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.1610\n",
            "Epoch 1025/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0247 - val_loss: 0.1529\n",
            "Epoch 1026/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0275 - val_loss: 0.1622\n",
            "Epoch 1027/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.1572\n",
            "Epoch 1028/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0385 - val_loss: 0.1627\n",
            "Epoch 1029/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0284 - val_loss: 0.1551\n",
            "Epoch 1030/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0271 - val_loss: 0.1648\n",
            "Epoch 1031/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.1548\n",
            "Epoch 1032/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0252 - val_loss: 0.1563\n",
            "Epoch 1033/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 0.1609\n",
            "Epoch 1034/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.1552\n",
            "Epoch 1035/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0230 - val_loss: 0.1612\n",
            "Epoch 1036/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.1557\n",
            "Epoch 1037/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0176 - val_loss: 0.1594\n",
            "Epoch 1038/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0199 - val_loss: 0.1529\n",
            "Epoch 1039/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.1584\n",
            "Epoch 1040/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.1589\n",
            "Epoch 1041/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0267 - val_loss: 0.1557\n",
            "Epoch 1042/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1592\n",
            "Epoch 1043/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0206 - val_loss: 0.1574\n",
            "Epoch 1044/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0221 - val_loss: 0.1580\n",
            "Epoch 1045/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 0.1530\n",
            "Epoch 1046/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0260 - val_loss: 0.1590\n",
            "Epoch 1047/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.1621\n",
            "Epoch 1048/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 0.1593\n",
            "Epoch 1049/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0254 - val_loss: 0.1596\n",
            "Epoch 1050/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0243 - val_loss: 0.1555\n",
            "Epoch 1051/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0280 - val_loss: 0.1635\n",
            "Epoch 1052/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0237 - val_loss: 0.1572\n",
            "Epoch 1053/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 0.1687\n",
            "Epoch 1054/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0335 - val_loss: 0.1590\n",
            "Epoch 1055/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0302 - val_loss: 0.1599\n",
            "Epoch 1056/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0275 - val_loss: 0.1537\n",
            "Epoch 1057/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0244 - val_loss: 0.1587\n",
            "Epoch 1058/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0238 - val_loss: 0.1561\n",
            "Epoch 1059/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0301 - val_loss: 0.1590\n",
            "Epoch 1060/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0265 - val_loss: 0.1541\n",
            "Epoch 1061/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0225 - val_loss: 0.1552\n",
            "Epoch 1062/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0180 - val_loss: 0.1529\n",
            "Epoch 1063/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.1592\n",
            "Epoch 1064/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0259 - val_loss: 0.1546\n",
            "Epoch 1065/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.1566\n",
            "Epoch 1066/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.1560\n",
            "Epoch 1067/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0199 - val_loss: 0.1547\n",
            "Epoch 1068/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0235 - val_loss: 0.1572\n",
            "Epoch 1069/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0204 - val_loss: 0.1581\n",
            "Epoch 1070/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0154 - val_loss: 0.1584\n",
            "Epoch 1071/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0187 - val_loss: 0.1553\n",
            "Epoch 1072/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 0.1562\n",
            "Epoch 1073/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0248 - val_loss: 0.1551\n",
            "Epoch 1074/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0206 - val_loss: 0.1548\n",
            "Epoch 1075/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0232 - val_loss: 0.1586\n",
            "Epoch 1076/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0314 - val_loss: 0.1599\n",
            "Epoch 1077/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0508 - val_loss: 0.1617\n",
            "Epoch 1078/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0411 - val_loss: 0.1537\n",
            "Epoch 1079/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0348 - val_loss: 0.1562\n",
            "Epoch 1080/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.1643\n",
            "Epoch 1081/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1524\n",
            "Epoch 1082/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.1577\n",
            "Epoch 1083/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0191 - val_loss: 0.1541\n",
            "Epoch 1084/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0252 - val_loss: 0.1524\n",
            "Epoch 1085/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0227 - val_loss: 0.1569\n",
            "Epoch 1086/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0197 - val_loss: 0.1514\n",
            "Epoch 1087/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0232 - val_loss: 0.1546\n",
            "Epoch 1088/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0266 - val_loss: 0.1563\n",
            "Epoch 1089/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0209 - val_loss: 0.1564\n",
            "Epoch 1090/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0200 - val_loss: 0.1553\n",
            "Epoch 1091/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0182 - val_loss: 0.1564\n",
            "Epoch 1092/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1564\n",
            "Epoch 1093/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1563\n",
            "Epoch 1094/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0231 - val_loss: 0.1537\n",
            "Epoch 1095/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.1530\n",
            "Epoch 1096/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0192 - val_loss: 0.1575\n",
            "Epoch 1097/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0194 - val_loss: 0.1549\n",
            "Epoch 1098/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.1561\n",
            "Epoch 1099/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0262 - val_loss: 0.1527\n",
            "Epoch 1100/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.1558\n",
            "Epoch 1101/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1589\n",
            "Epoch 1102/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0260 - val_loss: 0.1551\n",
            "Epoch 1103/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0304 - val_loss: 0.1640\n",
            "Epoch 1104/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0319 - val_loss: 0.1564\n",
            "Epoch 1105/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.1533\n",
            "Epoch 1106/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0257 - val_loss: 0.1527\n",
            "Epoch 1107/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0276 - val_loss: 0.1554\n",
            "Epoch 1108/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1502\n",
            "Epoch 1109/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0196 - val_loss: 0.1537\n",
            "Epoch 1110/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0161 - val_loss: 0.1553\n",
            "Epoch 1111/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0179 - val_loss: 0.1590\n",
            "Epoch 1112/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0247 - val_loss: 0.1502\n",
            "Epoch 1113/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 0.1606\n",
            "Epoch 1114/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0334 - val_loss: 0.1668\n",
            "Epoch 1115/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0352 - val_loss: 0.1489\n",
            "Epoch 1116/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0427 - val_loss: 0.1595\n",
            "Epoch 1117/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0291 - val_loss: 0.1598\n",
            "Epoch 1118/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0259 - val_loss: 0.1554\n",
            "Epoch 1119/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.1533\n",
            "Epoch 1120/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0193 - val_loss: 0.1549\n",
            "Epoch 1121/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0219 - val_loss: 0.1535\n",
            "Epoch 1122/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1540\n",
            "Epoch 1123/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0249 - val_loss: 0.1569\n",
            "Epoch 1124/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.1542\n",
            "Epoch 1125/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0380 - val_loss: 0.1525\n",
            "Epoch 1126/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0234 - val_loss: 0.1541\n",
            "Epoch 1127/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.1549\n",
            "Epoch 1128/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 0.1568\n",
            "Epoch 1129/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0216 - val_loss: 0.1548\n",
            "Epoch 1130/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.1536\n",
            "Epoch 1131/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0242 - val_loss: 0.1546\n",
            "Epoch 1132/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0237 - val_loss: 0.1588\n",
            "Epoch 1133/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0236 - val_loss: 0.1497\n",
            "Epoch 1134/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.1567\n",
            "Epoch 1135/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0281 - val_loss: 0.1521\n",
            "Epoch 1136/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0210 - val_loss: 0.1588\n",
            "Epoch 1137/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.1529\n",
            "Epoch 1138/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0215 - val_loss: 0.1574\n",
            "Epoch 1139/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0208 - val_loss: 0.1569\n",
            "Epoch 1140/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0218 - val_loss: 0.1573\n",
            "Epoch 1141/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0238 - val_loss: 0.1562\n",
            "Epoch 1142/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0306 - val_loss: 0.1588\n",
            "Epoch 1143/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0241 - val_loss: 0.1617\n",
            "Epoch 1144/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0359 - val_loss: 0.1548\n",
            "Epoch 1145/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0229 - val_loss: 0.1546\n",
            "Epoch 1146/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.1663\n",
            "Epoch 1147/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0300 - val_loss: 0.1513\n",
            "Epoch 1148/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0305 - val_loss: 0.1588\n",
            "Epoch 1149/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0294 - val_loss: 0.1612\n",
            "Epoch 1150/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0321 - val_loss: 0.1687\n",
            "Epoch 1151/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0330 - val_loss: 0.1528\n",
            "Epoch 1152/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1569\n",
            "Epoch 1153/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.1533\n",
            "Epoch 1154/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0202 - val_loss: 0.1595\n",
            "Epoch 1155/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0211 - val_loss: 0.1549\n",
            "Epoch 1156/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0178 - val_loss: 0.1551\n",
            "Epoch 1157/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1514\n",
            "Epoch 1158/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0295 - val_loss: 0.1592\n",
            "Epoch 1159/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0248 - val_loss: 0.1513\n",
            "Epoch 1160/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0254 - val_loss: 0.1573\n",
            "Epoch 1161/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0266 - val_loss: 0.1563\n",
            "Epoch 1162/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0294 - val_loss: 0.1608\n",
            "Epoch 1163/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0202 - val_loss: 0.1495\n",
            "Epoch 1164/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0194 - val_loss: 0.1554\n",
            "Epoch 1165/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0234 - val_loss: 0.1534\n",
            "Epoch 1166/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1566\n",
            "Epoch 1167/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0267 - val_loss: 0.1532\n",
            "Epoch 1168/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0182 - val_loss: 0.1542\n",
            "Epoch 1169/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0179 - val_loss: 0.1580\n",
            "Epoch 1170/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1551\n",
            "Epoch 1171/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0307 - val_loss: 0.1624\n",
            "Epoch 1172/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0252 - val_loss: 0.1530\n",
            "Epoch 1173/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 0.1553\n",
            "Epoch 1174/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0339 - val_loss: 0.1677\n",
            "Epoch 1175/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0313 - val_loss: 0.1565\n",
            "Epoch 1176/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0301 - val_loss: 0.1538\n",
            "Epoch 1177/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.1603\n",
            "Epoch 1178/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.1517\n",
            "Epoch 1179/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0196 - val_loss: 0.1577\n",
            "Epoch 1180/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.1539\n",
            "Epoch 1181/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0196 - val_loss: 0.1548\n",
            "Epoch 1182/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0294 - val_loss: 0.1572\n",
            "Epoch 1183/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.1571\n",
            "Epoch 1184/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0227 - val_loss: 0.1581\n",
            "Epoch 1185/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0286 - val_loss: 0.1518\n",
            "Epoch 1186/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.1561\n",
            "Epoch 1187/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0247 - val_loss: 0.1537\n",
            "Epoch 1188/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0227 - val_loss: 0.1526\n",
            "Epoch 1189/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.1573\n",
            "Epoch 1190/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0289 - val_loss: 0.1594\n",
            "Epoch 1191/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0266 - val_loss: 0.1547\n",
            "Epoch 1192/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0306 - val_loss: 0.1649\n",
            "Epoch 1193/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.1531\n",
            "Epoch 1194/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.1569\n",
            "Epoch 1195/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0262 - val_loss: 0.1519\n",
            "Epoch 1196/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.1564\n",
            "Epoch 1197/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1533\n",
            "Epoch 1198/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.1547\n",
            "Epoch 1199/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.1527\n",
            "Epoch 1200/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0225 - val_loss: 0.1551\n",
            "Epoch 1201/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0164 - val_loss: 0.1551\n",
            "Epoch 1202/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1514\n",
            "Epoch 1203/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0217 - val_loss: 0.1561\n",
            "Epoch 1204/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1518\n",
            "Epoch 1205/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0238 - val_loss: 0.1550\n",
            "Epoch 1206/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.1557\n",
            "Epoch 1207/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1514\n",
            "Epoch 1208/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.1544\n",
            "Epoch 1209/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0278 - val_loss: 0.1491\n",
            "Epoch 1210/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0242 - val_loss: 0.1541\n",
            "Epoch 1211/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0219 - val_loss: 0.1554\n",
            "Epoch 1212/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0288 - val_loss: 0.1607\n",
            "Epoch 1213/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0310 - val_loss: 0.1545\n",
            "Epoch 1214/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0245 - val_loss: 0.1548\n",
            "Epoch 1215/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0265 - val_loss: 0.1606\n",
            "Epoch 1216/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0221 - val_loss: 0.1514\n",
            "Epoch 1217/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0230 - val_loss: 0.1554\n",
            "Epoch 1218/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0213 - val_loss: 0.1559\n",
            "Epoch 1219/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.1548\n",
            "Epoch 1220/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0235 - val_loss: 0.1626\n",
            "Epoch 1221/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0258 - val_loss: 0.1473\n",
            "Epoch 1222/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.1538\n",
            "Epoch 1223/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0300 - val_loss: 0.1502\n",
            "Epoch 1224/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0306 - val_loss: 0.1564\n",
            "Epoch 1225/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0296 - val_loss: 0.1560\n",
            "Epoch 1226/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.1534\n",
            "Epoch 1227/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0315 - val_loss: 0.1563\n",
            "Epoch 1228/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0281 - val_loss: 0.1526\n",
            "Epoch 1229/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0382 - val_loss: 0.1571\n",
            "Epoch 1230/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0294 - val_loss: 0.1543\n",
            "Epoch 1231/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0238 - val_loss: 0.1557\n",
            "Epoch 1232/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1571\n",
            "Epoch 1233/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1536\n",
            "Epoch 1234/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.1545\n",
            "Epoch 1235/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0155 - val_loss: 0.1494\n",
            "Epoch 1236/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0201 - val_loss: 0.1564\n",
            "Epoch 1237/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0233 - val_loss: 0.1504\n",
            "Epoch 1238/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0201 - val_loss: 0.1573\n",
            "Epoch 1239/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0238 - val_loss: 0.1518\n",
            "Epoch 1240/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0230 - val_loss: 0.1577\n",
            "Epoch 1241/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.1584\n",
            "Epoch 1242/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0369 - val_loss: 0.1549\n",
            "Epoch 1243/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0267 - val_loss: 0.1543\n",
            "Epoch 1244/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0301 - val_loss: 0.1528\n",
            "Epoch 1245/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0242 - val_loss: 0.1571\n",
            "Epoch 1246/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0313 - val_loss: 0.1570\n",
            "Epoch 1247/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0293 - val_loss: 0.1577\n",
            "Epoch 1248/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.1578\n",
            "Epoch 1249/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0248 - val_loss: 0.1572\n",
            "Epoch 1250/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0207 - val_loss: 0.1515\n",
            "Epoch 1251/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 0.1555\n",
            "Epoch 1252/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0246 - val_loss: 0.1501\n",
            "Epoch 1253/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0233 - val_loss: 0.1536\n",
            "Epoch 1254/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1528\n",
            "Epoch 1255/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0268 - val_loss: 0.1550\n",
            "Epoch 1256/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0187 - val_loss: 0.1533\n",
            "Epoch 1257/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 0.1507\n",
            "Epoch 1258/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0198 - val_loss: 0.1566\n",
            "Epoch 1259/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0260 - val_loss: 0.1533\n",
            "Epoch 1260/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0353 - val_loss: 0.1551\n",
            "Epoch 1261/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0348 - val_loss: 0.1572\n",
            "Epoch 1262/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0270 - val_loss: 0.1526\n",
            "Epoch 1263/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0274 - val_loss: 0.1546\n",
            "Epoch 1264/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0249 - val_loss: 0.1507\n",
            "Epoch 1265/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.1565\n",
            "Epoch 1266/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.1528\n",
            "Epoch 1267/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.1557\n",
            "Epoch 1268/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.1563\n",
            "Epoch 1269/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0223 - val_loss: 0.1544\n",
            "Epoch 1270/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0279 - val_loss: 0.1551\n",
            "Epoch 1271/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0258 - val_loss: 0.1580\n",
            "Epoch 1272/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0221 - val_loss: 0.1504\n",
            "Epoch 1273/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0258 - val_loss: 0.1523\n",
            "Epoch 1274/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0217 - val_loss: 0.1575\n",
            "Epoch 1275/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0175 - val_loss: 0.1510\n",
            "Epoch 1276/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.1543\n",
            "Epoch 1277/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.1551\n",
            "Epoch 1278/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0210 - val_loss: 0.1552\n",
            "Epoch 1279/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1511\n",
            "Epoch 1280/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0169 - val_loss: 0.1559\n",
            "Epoch 1281/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.1529\n",
            "Epoch 1282/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0188 - val_loss: 0.1572\n",
            "Epoch 1283/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0210 - val_loss: 0.1584\n",
            "Epoch 1284/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.1515\n",
            "Epoch 1285/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1510\n",
            "Epoch 1286/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0204 - val_loss: 0.1588\n",
            "Epoch 1287/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0242 - val_loss: 0.1562\n",
            "Epoch 1288/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0243 - val_loss: 0.1564\n",
            "Epoch 1289/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0287 - val_loss: 0.1507\n",
            "Epoch 1290/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0251 - val_loss: 0.1543\n",
            "Epoch 1291/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0204 - val_loss: 0.1527\n",
            "Epoch 1292/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.1548\n",
            "Epoch 1293/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.1597\n",
            "Epoch 1294/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0338 - val_loss: 0.1578\n",
            "Epoch 1295/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0298 - val_loss: 0.1570\n",
            "Epoch 1296/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.1523\n",
            "Epoch 1297/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0224 - val_loss: 0.1558\n",
            "Epoch 1298/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0193 - val_loss: 0.1541\n",
            "Epoch 1299/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.1508\n",
            "Epoch 1300/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0191 - val_loss: 0.1506\n",
            "Epoch 1301/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0275 - val_loss: 0.1510\n",
            "Epoch 1302/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0249 - val_loss: 0.1609\n",
            "Epoch 1303/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0203 - val_loss: 0.1494\n",
            "Epoch 1304/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0227 - val_loss: 0.1554\n",
            "Epoch 1305/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0232 - val_loss: 0.1541\n",
            "Epoch 1306/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1515\n",
            "Epoch 1307/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.1541\n",
            "Epoch 1308/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0231 - val_loss: 0.1508\n",
            "Epoch 1309/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0222 - val_loss: 0.1557\n",
            "Epoch 1310/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0246 - val_loss: 0.1523\n",
            "Epoch 1311/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.1529\n",
            "Epoch 1312/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 0.1527\n",
            "Epoch 1313/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0199 - val_loss: 0.1554\n",
            "Epoch 1314/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0228 - val_loss: 0.1533\n",
            "Epoch 1315/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 0.1545\n",
            "Epoch 1316/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0195 - val_loss: 0.1540\n",
            "Epoch 1317/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0160 - val_loss: 0.1517\n",
            "Epoch 1318/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0217 - val_loss: 0.1527\n",
            "Epoch 1319/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0223 - val_loss: 0.1506\n",
            "Epoch 1320/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0190 - val_loss: 0.1526\n",
            "Epoch 1321/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.1524\n",
            "Epoch 1322/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0202 - val_loss: 0.1572\n",
            "Epoch 1323/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0254 - val_loss: 0.1515\n",
            "Epoch 1324/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.1554\n",
            "Epoch 1325/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0255 - val_loss: 0.1584\n",
            "Epoch 1326/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.1559\n",
            "Epoch 1327/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0252 - val_loss: 0.1568\n",
            "Epoch 1328/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.1499\n",
            "Epoch 1329/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.1561\n",
            "Epoch 1330/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0204 - val_loss: 0.1537\n",
            "Epoch 1331/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0247 - val_loss: 0.1558\n",
            "Epoch 1332/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0256 - val_loss: 0.1532\n",
            "Epoch 1333/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0266 - val_loss: 0.1524\n",
            "Epoch 1334/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.1567\n",
            "Epoch 1335/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0332 - val_loss: 0.1523\n",
            "Epoch 1336/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0287 - val_loss: 0.1515\n",
            "Epoch 1337/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.1628\n",
            "Epoch 1338/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0274 - val_loss: 0.1506\n",
            "Epoch 1339/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0269 - val_loss: 0.1518\n",
            "Epoch 1340/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0215 - val_loss: 0.1544\n",
            "Epoch 1341/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0202 - val_loss: 0.1493\n",
            "Epoch 1342/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0231 - val_loss: 0.1529\n",
            "Epoch 1343/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0259 - val_loss: 0.1541\n",
            "Epoch 1344/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0236 - val_loss: 0.1511\n",
            "Epoch 1345/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0165 - val_loss: 0.1560\n",
            "Epoch 1346/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0164 - val_loss: 0.1472\n",
            "Epoch 1347/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0193 - val_loss: 0.1594\n",
            "Epoch 1348/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.1527\n",
            "Epoch 1349/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0233 - val_loss: 0.1568\n",
            "Epoch 1350/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0244 - val_loss: 0.1523\n",
            "Epoch 1351/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0196 - val_loss: 0.1555\n",
            "Epoch 1352/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0276 - val_loss: 0.1597\n",
            "Epoch 1353/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0246 - val_loss: 0.1570\n",
            "Epoch 1354/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0284 - val_loss: 0.1587\n",
            "Epoch 1355/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0340 - val_loss: 0.1617\n",
            "Epoch 1356/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0392 - val_loss: 0.1594\n",
            "Epoch 1357/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0303 - val_loss: 0.1540\n",
            "Epoch 1358/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0233 - val_loss: 0.1562\n",
            "Epoch 1359/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0206 - val_loss: 0.1547\n",
            "Epoch 1360/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0240 - val_loss: 0.1588\n",
            "Epoch 1361/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0231 - val_loss: 0.1511\n",
            "Epoch 1362/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0275 - val_loss: 0.1525\n",
            "Epoch 1363/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0245 - val_loss: 0.1547\n",
            "Epoch 1364/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0276 - val_loss: 0.1604\n",
            "Epoch 1365/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.1502\n",
            "Epoch 1366/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0154 - val_loss: 0.1536\n",
            "Epoch 1367/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1543\n",
            "Epoch 1368/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0207 - val_loss: 0.1566\n",
            "Epoch 1369/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0222 - val_loss: 0.1552\n",
            "Epoch 1370/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.1522\n",
            "Epoch 1371/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1529\n",
            "Epoch 1372/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0247 - val_loss: 0.1536\n",
            "Epoch 1373/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0227 - val_loss: 0.1506\n",
            "Epoch 1374/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.1506\n",
            "Epoch 1375/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0208 - val_loss: 0.1549\n",
            "Epoch 1376/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0197 - val_loss: 0.1496\n",
            "Epoch 1377/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0258 - val_loss: 0.1538\n",
            "Epoch 1378/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0199 - val_loss: 0.1532\n",
            "Epoch 1379/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0193 - val_loss: 0.1524\n",
            "Epoch 1380/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0331 - val_loss: 0.1573\n",
            "Epoch 1381/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0354 - val_loss: 0.1530\n",
            "Epoch 1382/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0322 - val_loss: 0.1656\n",
            "Epoch 1383/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0303 - val_loss: 0.1531\n",
            "Epoch 1384/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0289 - val_loss: 0.1516\n",
            "Epoch 1385/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0183 - val_loss: 0.1556\n",
            "Epoch 1386/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0229 - val_loss: 0.1535\n",
            "Epoch 1387/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0193 - val_loss: 0.1506\n",
            "Epoch 1388/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0226 - val_loss: 0.1544\n",
            "Epoch 1389/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0209 - val_loss: 0.1525\n",
            "Epoch 1390/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0219 - val_loss: 0.1602\n",
            "Epoch 1391/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1527\n",
            "Epoch 1392/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0249 - val_loss: 0.1589\n",
            "Epoch 1393/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0249 - val_loss: 0.1566\n",
            "Epoch 1394/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0286 - val_loss: 0.1522\n",
            "Epoch 1395/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0253 - val_loss: 0.1607\n",
            "Epoch 1396/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0281 - val_loss: 0.1483\n",
            "Epoch 1397/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0335 - val_loss: 0.1615\n",
            "Epoch 1398/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0316 - val_loss: 0.1527\n",
            "Epoch 1399/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0268 - val_loss: 0.1505\n",
            "Epoch 1400/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0251 - val_loss: 0.1560\n",
            "Epoch 1401/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0261 - val_loss: 0.1510\n",
            "Epoch 1402/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0260 - val_loss: 0.1563\n",
            "Epoch 1403/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.1514\n",
            "Epoch 1404/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0255 - val_loss: 0.1537\n",
            "Epoch 1405/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0321 - val_loss: 0.1526\n",
            "Epoch 1406/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0288 - val_loss: 0.1575\n",
            "Epoch 1407/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0313 - val_loss: 0.1540\n",
            "Epoch 1408/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0259 - val_loss: 0.1574\n",
            "Epoch 1409/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0263 - val_loss: 0.1529\n",
            "Epoch 1410/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0235 - val_loss: 0.1562\n",
            "Epoch 1411/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.1538\n",
            "Epoch 1412/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0263 - val_loss: 0.1550\n",
            "Epoch 1413/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0310 - val_loss: 0.1582\n",
            "Epoch 1414/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0321 - val_loss: 0.1616\n",
            "Epoch 1415/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0381 - val_loss: 0.1547\n",
            "Epoch 1416/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0323 - val_loss: 0.1551\n",
            "Epoch 1417/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0207 - val_loss: 0.1552\n",
            "Epoch 1418/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.1541\n",
            "Epoch 1419/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0222 - val_loss: 0.1549\n",
            "Epoch 1420/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0239 - val_loss: 0.1517\n",
            "Epoch 1421/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.1574\n",
            "Epoch 1422/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0236 - val_loss: 0.1573\n",
            "Epoch 1423/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0253 - val_loss: 0.1564\n",
            "Epoch 1424/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0329 - val_loss: 0.1650\n",
            "Epoch 1425/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0232 - val_loss: 0.1481\n",
            "Epoch 1426/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0282 - val_loss: 0.1590\n",
            "Epoch 1427/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0244 - val_loss: 0.1547\n",
            "Epoch 1428/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0195 - val_loss: 0.1564\n",
            "Epoch 1429/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0202 - val_loss: 0.1517\n",
            "Epoch 1430/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0218 - val_loss: 0.1553\n",
            "Epoch 1431/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0225 - val_loss: 0.1552\n",
            "Epoch 1432/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0256 - val_loss: 0.1560\n",
            "Epoch 1433/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0210 - val_loss: 0.1500\n",
            "Epoch 1434/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0242 - val_loss: 0.1515\n",
            "Epoch 1435/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.1547\n",
            "Epoch 1436/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0178 - val_loss: 0.1557\n",
            "Epoch 1437/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0195 - val_loss: 0.1559\n",
            "Epoch 1438/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0342 - val_loss: 0.1512\n",
            "Epoch 1439/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0247 - val_loss: 0.1524\n",
            "Epoch 1440/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.1544\n",
            "Epoch 1441/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.1559\n",
            "Epoch 1442/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.1542\n",
            "Epoch 1443/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0209 - val_loss: 0.1509\n",
            "Epoch 1444/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0193 - val_loss: 0.1521\n",
            "Epoch 1445/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0273 - val_loss: 0.1554\n",
            "Epoch 1446/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0355 - val_loss: 0.1529\n",
            "Epoch 1447/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0251 - val_loss: 0.1545\n",
            "Epoch 1448/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0246 - val_loss: 0.1560\n",
            "Epoch 1449/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0279 - val_loss: 0.1528\n",
            "Epoch 1450/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0300 - val_loss: 0.1572\n",
            "Epoch 1451/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0236 - val_loss: 0.1496\n",
            "Epoch 1452/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.1569\n",
            "Epoch 1453/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0170 - val_loss: 0.1507\n",
            "Epoch 1454/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0169 - val_loss: 0.1585\n",
            "Epoch 1455/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.1540\n",
            "Epoch 1456/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0210 - val_loss: 0.1544\n",
            "Epoch 1457/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0184 - val_loss: 0.1530\n",
            "Epoch 1458/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0181 - val_loss: 0.1565\n",
            "Epoch 1459/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0165 - val_loss: 0.1555\n",
            "Epoch 1460/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.1558\n",
            "Epoch 1461/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0230 - val_loss: 0.1530\n",
            "Epoch 1462/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.1553\n",
            "Epoch 1463/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.1524\n",
            "Epoch 1464/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0243 - val_loss: 0.1566\n",
            "Epoch 1465/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1497\n",
            "Epoch 1466/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0240 - val_loss: 0.1548\n",
            "Epoch 1467/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 0.1484\n",
            "Epoch 1468/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0190 - val_loss: 0.1531\n",
            "Epoch 1469/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0212 - val_loss: 0.1528\n",
            "Epoch 1470/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1539\n",
            "Epoch 1471/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.1572\n",
            "Epoch 1472/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0260 - val_loss: 0.1546\n",
            "Epoch 1473/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0236 - val_loss: 0.1555\n",
            "Epoch 1474/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0261 - val_loss: 0.1582\n",
            "Epoch 1475/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0208 - val_loss: 0.1487\n",
            "Epoch 1476/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0227 - val_loss: 0.1558\n",
            "Epoch 1477/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0237 - val_loss: 0.1517\n",
            "Epoch 1478/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0263 - val_loss: 0.1506\n",
            "Epoch 1479/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.1546\n",
            "Epoch 1480/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.1539\n",
            "Epoch 1481/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0212 - val_loss: 0.1525\n",
            "Epoch 1482/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0188 - val_loss: 0.1571\n",
            "Epoch 1483/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0200 - val_loss: 0.1530\n",
            "Epoch 1484/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0192 - val_loss: 0.1554\n",
            "Epoch 1485/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0183 - val_loss: 0.1500\n",
            "Epoch 1486/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0198 - val_loss: 0.1554\n",
            "Epoch 1487/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.1524\n",
            "Epoch 1488/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0169 - val_loss: 0.1514\n",
            "Epoch 1489/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0210 - val_loss: 0.1561\n",
            "Epoch 1490/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0257 - val_loss: 0.1511\n",
            "Epoch 1491/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0229 - val_loss: 0.1537\n",
            "Epoch 1492/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0194 - val_loss: 0.1488\n",
            "Epoch 1493/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0252 - val_loss: 0.1599\n",
            "Epoch 1494/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0258 - val_loss: 0.1558\n",
            "Epoch 1495/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0234 - val_loss: 0.1534\n",
            "Epoch 1496/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1567\n",
            "Epoch 1497/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0201 - val_loss: 0.1592\n",
            "Epoch 1498/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.1584\n",
            "Epoch 1499/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0229 - val_loss: 0.1549\n",
            "Epoch 1500/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0210 - val_loss: 0.1476\n",
            "Epoch 1501/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0212 - val_loss: 0.1517\n",
            "Epoch 1502/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0193 - val_loss: 0.1561\n",
            "Epoch 1503/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0177 - val_loss: 0.1545\n",
            "Epoch 1504/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0176 - val_loss: 0.1591\n",
            "Epoch 1505/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0205 - val_loss: 0.1526\n",
            "Epoch 1506/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0196 - val_loss: 0.1556\n",
            "Epoch 1507/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1539\n",
            "Epoch 1508/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.1517\n",
            "Epoch 1509/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0184 - val_loss: 0.1538\n",
            "Epoch 1510/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0255 - val_loss: 0.1505\n",
            "Epoch 1511/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0255 - val_loss: 0.1519\n",
            "Epoch 1512/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0201 - val_loss: 0.1565\n",
            "Epoch 1513/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.1566\n",
            "Epoch 1514/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0252 - val_loss: 0.1616\n",
            "Epoch 1515/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0331 - val_loss: 0.1544\n",
            "Epoch 1516/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1569\n",
            "Epoch 1517/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0207 - val_loss: 0.1531\n",
            "Epoch 1518/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0200 - val_loss: 0.1586\n",
            "Epoch 1519/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0247 - val_loss: 0.1540\n",
            "Epoch 1520/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0198 - val_loss: 0.1548\n",
            "Epoch 1521/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0176 - val_loss: 0.1516\n",
            "Epoch 1522/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0188 - val_loss: 0.1528\n",
            "Epoch 1523/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0234 - val_loss: 0.1538\n",
            "Epoch 1524/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.1604\n",
            "Epoch 1525/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0393 - val_loss: 0.1525\n",
            "Epoch 1526/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0271 - val_loss: 0.1530\n",
            "Epoch 1527/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.1538\n",
            "Epoch 1528/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0341 - val_loss: 0.1581\n",
            "Epoch 1529/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0298 - val_loss: 0.1500\n",
            "Epoch 1530/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0314 - val_loss: 0.1544\n",
            "Epoch 1531/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.1593\n",
            "Epoch 1532/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0262 - val_loss: 0.1551\n",
            "Epoch 1533/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0241 - val_loss: 0.1576\n",
            "Epoch 1534/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0214 - val_loss: 0.1534\n",
            "Epoch 1535/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0176 - val_loss: 0.1550\n",
            "Epoch 1536/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0161 - val_loss: 0.1538\n",
            "Epoch 1537/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1567\n",
            "Epoch 1538/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0183 - val_loss: 0.1538\n",
            "Epoch 1539/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.1532\n",
            "Epoch 1540/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0241 - val_loss: 0.1572\n",
            "Epoch 1541/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0277 - val_loss: 0.1552\n",
            "Epoch 1542/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0253 - val_loss: 0.1595\n",
            "Epoch 1543/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0259 - val_loss: 0.1537\n",
            "Epoch 1544/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0173 - val_loss: 0.1532\n",
            "Epoch 1545/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0186 - val_loss: 0.1495\n",
            "Epoch 1546/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0225 - val_loss: 0.1572\n",
            "Epoch 1547/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0212 - val_loss: 0.1556\n",
            "Epoch 1548/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1508\n",
            "Epoch 1549/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0257 - val_loss: 0.1530\n",
            "Epoch 1550/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0210 - val_loss: 0.1529\n",
            "Epoch 1551/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0200 - val_loss: 0.1520\n",
            "Epoch 1552/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0243 - val_loss: 0.1552\n",
            "Epoch 1553/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0231 - val_loss: 0.1547\n",
            "Epoch 1554/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0228 - val_loss: 0.1562\n",
            "Epoch 1555/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 0.1601\n",
            "Epoch 1556/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0284 - val_loss: 0.1548\n",
            "Epoch 1557/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0234 - val_loss: 0.1515\n",
            "Epoch 1558/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0200 - val_loss: 0.1571\n",
            "Epoch 1559/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1525\n",
            "Epoch 1560/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.1511\n",
            "Epoch 1561/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0218 - val_loss: 0.1528\n",
            "Epoch 1562/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.1532\n",
            "Epoch 1563/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0214 - val_loss: 0.1553\n",
            "Epoch 1564/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0212 - val_loss: 0.1522\n",
            "Epoch 1565/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1547\n",
            "Epoch 1566/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0195 - val_loss: 0.1541\n",
            "Epoch 1567/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0188 - val_loss: 0.1541\n",
            "Epoch 1568/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0203 - val_loss: 0.1489\n",
            "Epoch 1569/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0214 - val_loss: 0.1577\n",
            "Epoch 1570/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.1523\n",
            "Epoch 1571/2500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0206 - val_loss: 0.1576\n",
            "Epoch 1572/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0192 - val_loss: 0.1500\n",
            "Epoch 1573/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0237 - val_loss: 0.1534\n",
            "Epoch 1574/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.1540\n",
            "Epoch 1575/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.1507\n",
            "Epoch 1576/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.1502\n",
            "Epoch 1577/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0177 - val_loss: 0.1534\n",
            "Epoch 1578/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.1503\n",
            "Epoch 1579/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0204 - val_loss: 0.1500\n",
            "Epoch 1580/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0214 - val_loss: 0.1515\n",
            "Epoch 1581/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1535\n",
            "Epoch 1582/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0206 - val_loss: 0.1527\n",
            "Epoch 1583/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0206 - val_loss: 0.1498\n",
            "Epoch 1584/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0193 - val_loss: 0.1551\n",
            "Epoch 1585/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0173 - val_loss: 0.1487\n",
            "Epoch 1586/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0201 - val_loss: 0.1548\n",
            "Epoch 1587/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1479\n",
            "Epoch 1588/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0235 - val_loss: 0.1510\n",
            "Epoch 1589/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0202 - val_loss: 0.1498\n",
            "Epoch 1590/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.1551\n",
            "Epoch 1591/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.1538\n",
            "Epoch 1592/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0292 - val_loss: 0.1626\n",
            "Epoch 1593/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0354 - val_loss: 0.1541\n",
            "Epoch 1594/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0245 - val_loss: 0.1521\n",
            "Epoch 1595/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.1501\n",
            "Epoch 1596/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1630\n",
            "Epoch 1597/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0244 - val_loss: 0.1555\n",
            "Epoch 1598/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0176 - val_loss: 0.1546\n",
            "Epoch 1599/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.1555\n",
            "Epoch 1600/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.1563\n",
            "Epoch 1601/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0170 - val_loss: 0.1554\n",
            "Epoch 1602/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0139 - val_loss: 0.1511\n",
            "Epoch 1603/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.1521\n",
            "Epoch 1604/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0177 - val_loss: 0.1524\n",
            "Epoch 1605/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0196 - val_loss: 0.1563\n",
            "Epoch 1606/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0211 - val_loss: 0.1525\n",
            "Epoch 1607/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0162 - val_loss: 0.1539\n",
            "Epoch 1608/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.1556\n",
            "Epoch 1609/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.1517\n",
            "Epoch 1610/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0170 - val_loss: 0.1516\n",
            "Epoch 1611/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1487\n",
            "Epoch 1612/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0221 - val_loss: 0.1560\n",
            "Epoch 1613/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.1602\n",
            "Epoch 1614/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0345 - val_loss: 0.1558\n",
            "Epoch 1615/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1564\n",
            "Epoch 1616/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1542\n",
            "Epoch 1617/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0212 - val_loss: 0.1548\n",
            "Epoch 1618/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0163 - val_loss: 0.1526\n",
            "Epoch 1619/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 0.1518\n",
            "Epoch 1620/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0135 - val_loss: 0.1540\n",
            "Epoch 1621/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0212 - val_loss: 0.1462\n",
            "Epoch 1622/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0240 - val_loss: 0.1545\n",
            "Epoch 1623/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0188 - val_loss: 0.1537\n",
            "Epoch 1624/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0178 - val_loss: 0.1561\n",
            "Epoch 1625/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.1528\n",
            "Epoch 1626/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0214 - val_loss: 0.1566\n",
            "Epoch 1627/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0249 - val_loss: 0.1572\n",
            "Epoch 1628/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0284 - val_loss: 0.1563\n",
            "Epoch 1629/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0250 - val_loss: 0.1532\n",
            "Epoch 1630/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0286 - val_loss: 0.1553\n",
            "Epoch 1631/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0297 - val_loss: 0.1557\n",
            "Epoch 1632/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0262 - val_loss: 0.1593\n",
            "Epoch 1633/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0226 - val_loss: 0.1568\n",
            "Epoch 1634/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.1527\n",
            "Epoch 1635/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0221 - val_loss: 0.1493\n",
            "Epoch 1636/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0183 - val_loss: 0.1525\n",
            "Epoch 1637/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0178 - val_loss: 0.1532\n",
            "Epoch 1638/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0200 - val_loss: 0.1554\n",
            "Epoch 1639/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0272 - val_loss: 0.1473\n",
            "Epoch 1640/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0335 - val_loss: 0.1563\n",
            "Epoch 1641/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0342 - val_loss: 0.1595\n",
            "Epoch 1642/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0235 - val_loss: 0.1487\n",
            "Epoch 1643/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.1550\n",
            "Epoch 1644/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0226 - val_loss: 0.1580\n",
            "Epoch 1645/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0240 - val_loss: 0.1573\n",
            "Epoch 1646/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0283 - val_loss: 0.1561\n",
            "Epoch 1647/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0234 - val_loss: 0.1547\n",
            "Epoch 1648/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0225 - val_loss: 0.1515\n",
            "Epoch 1649/2500\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0178 - val_loss: 0.1550\n",
            "Epoch 1650/2500\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0160 - val_loss: 0.1551\n",
            "Epoch 1651/2500\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.0183 - val_loss: 0.1522\n",
            "Epoch 1652/2500\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0160 - val_loss: 0.1535\n",
            "Epoch 1653/2500\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.0152 - val_loss: 0.1544\n",
            "Epoch 1654/2500\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0152 - val_loss: 0.1563\n",
            "Epoch 1655/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0136 - val_loss: 0.1522\n",
            "Epoch 1656/2500\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0187 - val_loss: 0.1568\n",
            "Epoch 1657/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0252 - val_loss: 0.1529\n",
            "Epoch 1658/2500\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0203 - val_loss: 0.1518\n",
            "Epoch 1659/2500\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0226 - val_loss: 0.1564\n",
            "Epoch 1660/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0212 - val_loss: 0.1504\n",
            "Epoch 1661/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0223 - val_loss: 0.1559\n",
            "Epoch 1662/2500\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0165 - val_loss: 0.1517\n",
            "Epoch 1663/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0206 - val_loss: 0.1561\n",
            "Epoch 1664/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0253 - val_loss: 0.1576\n",
            "Epoch 1665/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0211 - val_loss: 0.1527\n",
            "Epoch 1666/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0199 - val_loss: 0.1571\n",
            "Epoch 1667/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1502\n",
            "Epoch 1668/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0285 - val_loss: 0.1511\n",
            "Epoch 1669/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0261 - val_loss: 0.1509\n",
            "Epoch 1670/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0206 - val_loss: 0.1551\n",
            "Epoch 1671/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0193 - val_loss: 0.1561\n",
            "Epoch 1672/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0171 - val_loss: 0.1579\n",
            "Epoch 1673/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0220 - val_loss: 0.1541\n",
            "Epoch 1674/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0280 - val_loss: 0.1591\n",
            "Epoch 1675/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0255 - val_loss: 0.1602\n",
            "Epoch 1676/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.1549\n",
            "Epoch 1677/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0229 - val_loss: 0.1571\n",
            "Epoch 1678/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0179 - val_loss: 0.1515\n",
            "Epoch 1679/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0176 - val_loss: 0.1549\n",
            "Epoch 1680/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0225 - val_loss: 0.1552\n",
            "Epoch 1681/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0210 - val_loss: 0.1498\n",
            "Epoch 1682/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.1615\n",
            "Epoch 1683/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0219 - val_loss: 0.1529\n",
            "Epoch 1684/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0238 - val_loss: 0.1538\n",
            "Epoch 1685/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0249 - val_loss: 0.1533\n",
            "Epoch 1686/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0273 - val_loss: 0.1587\n",
            "Epoch 1687/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0229 - val_loss: 0.1564\n",
            "Epoch 1688/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0279 - val_loss: 0.1556\n",
            "Epoch 1689/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0256 - val_loss: 0.1535\n",
            "Epoch 1690/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0271 - val_loss: 0.1582\n",
            "Epoch 1691/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0295 - val_loss: 0.1558\n",
            "Epoch 1692/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0310 - val_loss: 0.1581\n",
            "Epoch 1693/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0301 - val_loss: 0.1547\n",
            "Epoch 1694/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0195 - val_loss: 0.1573\n",
            "Epoch 1695/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0185 - val_loss: 0.1564\n",
            "Epoch 1696/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 0.1545\n",
            "Epoch 1697/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0162 - val_loss: 0.1534\n",
            "Epoch 1698/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0150 - val_loss: 0.1525\n",
            "Epoch 1699/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0194 - val_loss: 0.1533\n",
            "Epoch 1700/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.1514\n",
            "Epoch 1701/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0166 - val_loss: 0.1538\n",
            "Epoch 1702/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0176 - val_loss: 0.1587\n",
            "Epoch 1703/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0194 - val_loss: 0.1537\n",
            "Epoch 1704/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.1609\n",
            "Epoch 1705/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0184 - val_loss: 0.1544\n",
            "Epoch 1706/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.1563\n",
            "Epoch 1707/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0183 - val_loss: 0.1548\n",
            "Epoch 1708/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0182 - val_loss: 0.1534\n",
            "Epoch 1709/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0182 - val_loss: 0.1523\n",
            "Epoch 1710/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1483\n",
            "Epoch 1711/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0232 - val_loss: 0.1527\n",
            "Epoch 1712/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.1547\n",
            "Epoch 1713/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0202 - val_loss: 0.1531\n",
            "Epoch 1714/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0179 - val_loss: 0.1546\n",
            "Epoch 1715/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0213 - val_loss: 0.1549\n",
            "Epoch 1716/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1503\n",
            "Epoch 1717/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0208 - val_loss: 0.1533\n",
            "Epoch 1718/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0182 - val_loss: 0.1546\n",
            "Epoch 1719/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0179 - val_loss: 0.1562\n",
            "Epoch 1720/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0256 - val_loss: 0.1583\n",
            "Epoch 1721/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0314 - val_loss: 0.1630\n",
            "Epoch 1722/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0368 - val_loss: 0.1565\n",
            "Epoch 1723/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0302 - val_loss: 0.1557\n",
            "Epoch 1724/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0311 - val_loss: 0.1514\n",
            "Epoch 1725/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0289 - val_loss: 0.1573\n",
            "Epoch 1726/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0248 - val_loss: 0.1534\n",
            "Epoch 1727/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0183 - val_loss: 0.1527\n",
            "Epoch 1728/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0228 - val_loss: 0.1545\n",
            "Epoch 1729/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0268 - val_loss: 0.1652\n",
            "Epoch 1730/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0294 - val_loss: 0.1529\n",
            "Epoch 1731/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.1537\n",
            "Epoch 1732/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0252 - val_loss: 0.1560\n",
            "Epoch 1733/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0280 - val_loss: 0.1569\n",
            "Epoch 1734/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0246 - val_loss: 0.1553\n",
            "Epoch 1735/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.1571\n",
            "Epoch 1736/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0206 - val_loss: 0.1595\n",
            "Epoch 1737/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0206 - val_loss: 0.1526\n",
            "Epoch 1738/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0194 - val_loss: 0.1569\n",
            "Epoch 1739/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.1516\n",
            "Epoch 1740/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0231 - val_loss: 0.1552\n",
            "Epoch 1741/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0290 - val_loss: 0.1605\n",
            "Epoch 1742/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0288 - val_loss: 0.1557\n",
            "Epoch 1743/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.1547\n",
            "Epoch 1744/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0197 - val_loss: 0.1537\n",
            "Epoch 1745/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0202 - val_loss: 0.1550\n",
            "Epoch 1746/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0147 - val_loss: 0.1537\n",
            "Epoch 1747/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0183 - val_loss: 0.1556\n",
            "Epoch 1748/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0183 - val_loss: 0.1561\n",
            "Epoch 1749/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0140 - val_loss: 0.1600\n",
            "Epoch 1750/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0181 - val_loss: 0.1534\n",
            "Epoch 1751/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0176 - val_loss: 0.1592\n",
            "Epoch 1752/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0182 - val_loss: 0.1538\n",
            "Epoch 1753/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0194 - val_loss: 0.1603\n",
            "Epoch 1754/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0243 - val_loss: 0.1520\n",
            "Epoch 1755/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0323 - val_loss: 0.1530\n",
            "Epoch 1756/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0282 - val_loss: 0.1589\n",
            "Epoch 1757/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0207 - val_loss: 0.1549\n",
            "Epoch 1758/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0167 - val_loss: 0.1562\n",
            "Epoch 1759/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.1591\n",
            "Epoch 1760/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0171 - val_loss: 0.1513\n",
            "Epoch 1761/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0199 - val_loss: 0.1598\n",
            "Epoch 1762/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.1570\n",
            "Epoch 1763/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0267 - val_loss: 0.1589\n",
            "Epoch 1764/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0285 - val_loss: 0.1610\n",
            "Epoch 1765/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.1541\n",
            "Epoch 1766/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0219 - val_loss: 0.1573\n",
            "Epoch 1767/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.1566\n",
            "Epoch 1768/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0171 - val_loss: 0.1514\n",
            "Epoch 1769/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0274 - val_loss: 0.1554\n",
            "Epoch 1770/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.1502\n",
            "Epoch 1771/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.1567\n",
            "Epoch 1772/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0187 - val_loss: 0.1589\n",
            "Epoch 1773/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1584\n",
            "Epoch 1774/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0231 - val_loss: 0.1569\n",
            "Epoch 1775/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0206 - val_loss: 0.1569\n",
            "Epoch 1776/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0212 - val_loss: 0.1558\n",
            "Epoch 1777/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1564\n",
            "Epoch 1778/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0264 - val_loss: 0.1554\n",
            "Epoch 1779/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0253 - val_loss: 0.1538\n",
            "Epoch 1780/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0226 - val_loss: 0.1495\n",
            "Epoch 1781/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0225 - val_loss: 0.1571\n",
            "Epoch 1782/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.1516\n",
            "Epoch 1783/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0188 - val_loss: 0.1563\n",
            "Epoch 1784/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.1555\n",
            "Epoch 1785/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0159 - val_loss: 0.1568\n",
            "Epoch 1786/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0253 - val_loss: 0.1586\n",
            "Epoch 1787/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0256 - val_loss: 0.1528\n",
            "Epoch 1788/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.1594\n",
            "Epoch 1789/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0224 - val_loss: 0.1561\n",
            "Epoch 1790/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0229 - val_loss: 0.1547\n",
            "Epoch 1791/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0226 - val_loss: 0.1554\n",
            "Epoch 1792/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1608\n",
            "Epoch 1793/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0213 - val_loss: 0.1530\n",
            "Epoch 1794/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0208 - val_loss: 0.1556\n",
            "Epoch 1795/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0194 - val_loss: 0.1525\n",
            "Epoch 1796/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0146 - val_loss: 0.1562\n",
            "Epoch 1797/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0167 - val_loss: 0.1530\n",
            "Epoch 1798/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.1567\n",
            "Epoch 1799/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0154 - val_loss: 0.1518\n",
            "Epoch 1800/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0224 - val_loss: 0.1557\n",
            "Epoch 1801/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0199 - val_loss: 0.1546\n",
            "Epoch 1802/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0151 - val_loss: 0.1578\n",
            "Epoch 1803/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0167 - val_loss: 0.1558\n",
            "Epoch 1804/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0162 - val_loss: 0.1541\n",
            "Epoch 1805/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0170 - val_loss: 0.1514\n",
            "Epoch 1806/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0215 - val_loss: 0.1567\n",
            "Epoch 1807/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0184 - val_loss: 0.1577\n",
            "Epoch 1808/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0239 - val_loss: 0.1524\n",
            "Epoch 1809/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0211 - val_loss: 0.1541\n",
            "Epoch 1810/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0189 - val_loss: 0.1577\n",
            "Epoch 1811/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0246 - val_loss: 0.1515\n",
            "Epoch 1812/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0213 - val_loss: 0.1552\n",
            "Epoch 1813/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1507\n",
            "Epoch 1814/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0239 - val_loss: 0.1605\n",
            "Epoch 1815/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0208 - val_loss: 0.1543\n",
            "Epoch 1816/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.1581\n",
            "Epoch 1817/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0196 - val_loss: 0.1545\n",
            "Epoch 1818/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.1543\n",
            "Epoch 1819/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0136 - val_loss: 0.1572\n",
            "Epoch 1820/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0130 - val_loss: 0.1522\n",
            "Epoch 1821/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0148 - val_loss: 0.1556\n",
            "Epoch 1822/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0145 - val_loss: 0.1560\n",
            "Epoch 1823/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.1560\n",
            "Epoch 1824/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0226 - val_loss: 0.1547\n",
            "Epoch 1825/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0206 - val_loss: 0.1590\n",
            "Epoch 1826/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0194 - val_loss: 0.1527\n",
            "Epoch 1827/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0226 - val_loss: 0.1579\n",
            "Epoch 1828/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0210 - val_loss: 0.1523\n",
            "Epoch 1829/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0178 - val_loss: 0.1558\n",
            "Epoch 1830/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0162 - val_loss: 0.1519\n",
            "Epoch 1831/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.1577\n",
            "Epoch 1832/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.1522\n",
            "Epoch 1833/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.1534\n",
            "Epoch 1834/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0153 - val_loss: 0.1517\n",
            "Epoch 1835/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0232 - val_loss: 0.1521\n",
            "Epoch 1836/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.1551\n",
            "Epoch 1837/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.1540\n",
            "Epoch 1838/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.1570\n",
            "Epoch 1839/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0243 - val_loss: 0.1538\n",
            "Epoch 1840/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0142 - val_loss: 0.1536\n",
            "Epoch 1841/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0144 - val_loss: 0.1551\n",
            "Epoch 1842/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0171 - val_loss: 0.1535\n",
            "Epoch 1843/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0229 - val_loss: 0.1512\n",
            "Epoch 1844/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0251 - val_loss: 0.1523\n",
            "Epoch 1845/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0172 - val_loss: 0.1534\n",
            "Epoch 1846/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0200 - val_loss: 0.1545\n",
            "Epoch 1847/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0286 - val_loss: 0.1566\n",
            "Epoch 1848/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0231 - val_loss: 0.1563\n",
            "Epoch 1849/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0208 - val_loss: 0.1535\n",
            "Epoch 1850/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0217 - val_loss: 0.1561\n",
            "Epoch 1851/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0184 - val_loss: 0.1531\n",
            "Epoch 1852/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.1529\n",
            "Epoch 1853/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0144 - val_loss: 0.1554\n",
            "Epoch 1854/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0203 - val_loss: 0.1559\n",
            "Epoch 1855/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.1550\n",
            "Epoch 1856/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0164 - val_loss: 0.1561\n",
            "Epoch 1857/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0234 - val_loss: 0.1549\n",
            "Epoch 1858/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0141 - val_loss: 0.1527\n",
            "Epoch 1859/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0157 - val_loss: 0.1540\n",
            "Epoch 1860/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0167 - val_loss: 0.1553\n",
            "Epoch 1861/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0270 - val_loss: 0.1577\n",
            "Epoch 1862/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0269 - val_loss: 0.1572\n",
            "Epoch 1863/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0220 - val_loss: 0.1532\n",
            "Epoch 1864/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.1573\n",
            "Epoch 1865/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0165 - val_loss: 0.1526\n",
            "Epoch 1866/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0192 - val_loss: 0.1527\n",
            "Epoch 1867/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0237 - val_loss: 0.1513\n",
            "Epoch 1868/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0205 - val_loss: 0.1521\n",
            "Epoch 1869/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0171 - val_loss: 0.1529\n",
            "Epoch 1870/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.1559\n",
            "Epoch 1871/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0164 - val_loss: 0.1559\n",
            "Epoch 1872/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0176 - val_loss: 0.1579\n",
            "Epoch 1873/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0163 - val_loss: 0.1556\n",
            "Epoch 1874/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.1554\n",
            "Epoch 1875/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0154 - val_loss: 0.1563\n",
            "Epoch 1876/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0165 - val_loss: 0.1569\n",
            "Epoch 1877/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0212 - val_loss: 0.1650\n",
            "Epoch 1878/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0215 - val_loss: 0.1501\n",
            "Epoch 1879/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0183 - val_loss: 0.1548\n",
            "Epoch 1880/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1515\n",
            "Epoch 1881/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0232 - val_loss: 0.1543\n",
            "Epoch 1882/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0145 - val_loss: 0.1521\n",
            "Epoch 1883/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0135 - val_loss: 0.1569\n",
            "Epoch 1884/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0128 - val_loss: 0.1520\n",
            "Epoch 1885/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.1521\n",
            "Epoch 1886/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0302 - val_loss: 0.1547\n",
            "Epoch 1887/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0291 - val_loss: 0.1538\n",
            "Epoch 1888/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0204 - val_loss: 0.1517\n",
            "Epoch 1889/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0215 - val_loss: 0.1544\n",
            "Epoch 1890/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0193 - val_loss: 0.1560\n",
            "Epoch 1891/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1545\n",
            "Epoch 1892/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0319 - val_loss: 0.1532\n",
            "Epoch 1893/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0254 - val_loss: 0.1548\n",
            "Epoch 1894/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0238 - val_loss: 0.1543\n",
            "Epoch 1895/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.1562\n",
            "Epoch 1896/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0190 - val_loss: 0.1558\n",
            "Epoch 1897/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.1570\n",
            "Epoch 1898/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0178 - val_loss: 0.1529\n",
            "Epoch 1899/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0177 - val_loss: 0.1536\n",
            "Epoch 1900/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0169 - val_loss: 0.1532\n",
            "Epoch 1901/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.1533\n",
            "Epoch 1902/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.1575\n",
            "Epoch 1903/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0176 - val_loss: 0.1582\n",
            "Epoch 1904/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0211 - val_loss: 0.1655\n",
            "Epoch 1905/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0302 - val_loss: 0.1634\n",
            "Epoch 1906/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0314 - val_loss: 0.1556\n",
            "Epoch 1907/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0256 - val_loss: 0.1599\n",
            "Epoch 1908/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0256 - val_loss: 0.1555\n",
            "Epoch 1909/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0220 - val_loss: 0.1552\n",
            "Epoch 1910/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0292 - val_loss: 0.1592\n",
            "Epoch 1911/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0282 - val_loss: 0.1636\n",
            "Epoch 1912/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0239 - val_loss: 0.1595\n",
            "Epoch 1913/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.1573\n",
            "Epoch 1914/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0226 - val_loss: 0.1555\n",
            "Epoch 1915/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0305 - val_loss: 0.1589\n",
            "Epoch 1916/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0365 - val_loss: 0.1575\n",
            "Epoch 1917/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0283 - val_loss: 0.1540\n",
            "Epoch 1918/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.1600\n",
            "Epoch 1919/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.1560\n",
            "Epoch 1920/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.1632\n",
            "Epoch 1921/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0175 - val_loss: 0.1537\n",
            "Epoch 1922/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0208 - val_loss: 0.1567\n",
            "Epoch 1923/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0202 - val_loss: 0.1556\n",
            "Epoch 1924/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.1556\n",
            "Epoch 1925/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0232 - val_loss: 0.1590\n",
            "Epoch 1926/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0324 - val_loss: 0.1615\n",
            "Epoch 1927/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.1578\n",
            "Epoch 1928/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0253 - val_loss: 0.1591\n",
            "Epoch 1929/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0225 - val_loss: 0.1545\n",
            "Epoch 1930/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.1556\n",
            "Epoch 1931/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0182 - val_loss: 0.1580\n",
            "Epoch 1932/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0194 - val_loss: 0.1590\n",
            "Epoch 1933/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0182 - val_loss: 0.1628\n",
            "Epoch 1934/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.1573\n",
            "Epoch 1935/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0195 - val_loss: 0.1599\n",
            "Epoch 1936/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.1565\n",
            "Epoch 1937/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.1568\n",
            "Epoch 1938/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0184 - val_loss: 0.1588\n",
            "Epoch 1939/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.1591\n",
            "Epoch 1940/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0235 - val_loss: 0.1579\n",
            "Epoch 1941/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0160 - val_loss: 0.1569\n",
            "Epoch 1942/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1562\n",
            "Epoch 1943/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0226 - val_loss: 0.1614\n",
            "Epoch 1944/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0216 - val_loss: 0.1565\n",
            "Epoch 1945/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0233 - val_loss: 0.1574\n",
            "Epoch 1946/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0209 - val_loss: 0.1560\n",
            "Epoch 1947/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0214 - val_loss: 0.1559\n",
            "Epoch 1948/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0236 - val_loss: 0.1604\n",
            "Epoch 1949/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0172 - val_loss: 0.1568\n",
            "Epoch 1950/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0143 - val_loss: 0.1565\n",
            "Epoch 1951/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0160 - val_loss: 0.1524\n",
            "Epoch 1952/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0195 - val_loss: 0.1568\n",
            "Epoch 1953/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.1568\n",
            "Epoch 1954/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0189 - val_loss: 0.1588\n",
            "Epoch 1955/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0199 - val_loss: 0.1583\n",
            "Epoch 1956/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0243 - val_loss: 0.1588\n",
            "Epoch 1957/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0206 - val_loss: 0.1570\n",
            "Epoch 1958/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.1552\n",
            "Epoch 1959/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0168 - val_loss: 0.1541\n",
            "Epoch 1960/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0186 - val_loss: 0.1574\n",
            "Epoch 1961/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0199 - val_loss: 0.1558\n",
            "Epoch 1962/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0240 - val_loss: 0.1569\n",
            "Epoch 1963/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0191 - val_loss: 0.1617\n",
            "Epoch 1964/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.1568\n",
            "Epoch 1965/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0247 - val_loss: 0.1611\n",
            "Epoch 1966/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0308 - val_loss: 0.1578\n",
            "Epoch 1967/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0311 - val_loss: 0.1553\n",
            "Epoch 1968/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0277 - val_loss: 0.1579\n",
            "Epoch 1969/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0282 - val_loss: 0.1607\n",
            "Epoch 1970/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0247 - val_loss: 0.1633\n",
            "Epoch 1971/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0257 - val_loss: 0.1609\n",
            "Epoch 1972/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0307 - val_loss: 0.1563\n",
            "Epoch 1973/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0165 - val_loss: 0.1603\n",
            "Epoch 1974/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0174 - val_loss: 0.1586\n",
            "Epoch 1975/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.1618\n",
            "Epoch 1976/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0165 - val_loss: 0.1618\n",
            "Epoch 1977/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0147 - val_loss: 0.1609\n",
            "Epoch 1978/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.1617\n",
            "Epoch 1979/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.1587\n",
            "Epoch 1980/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0184 - val_loss: 0.1616\n",
            "Epoch 1981/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0188 - val_loss: 0.1576\n",
            "Epoch 1982/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0199 - val_loss: 0.1575\n",
            "Epoch 1983/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0150 - val_loss: 0.1561\n",
            "Epoch 1984/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0166 - val_loss: 0.1591\n",
            "Epoch 1985/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0220 - val_loss: 0.1563\n",
            "Epoch 1986/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0207 - val_loss: 0.1589\n",
            "Epoch 1987/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0242 - val_loss: 0.1602\n",
            "Epoch 1988/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0158 - val_loss: 0.1587\n",
            "Epoch 1989/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0142 - val_loss: 0.1586\n",
            "Epoch 1990/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0182 - val_loss: 0.1592\n",
            "Epoch 1991/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1653\n",
            "Epoch 1992/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0214 - val_loss: 0.1547\n",
            "Epoch 1993/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0233 - val_loss: 0.1616\n",
            "Epoch 1994/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0264 - val_loss: 0.1591\n",
            "Epoch 1995/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0244 - val_loss: 0.1623\n",
            "Epoch 1996/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.1588\n",
            "Epoch 1997/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.1556\n",
            "Epoch 1998/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0191 - val_loss: 0.1596\n",
            "Epoch 1999/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.1612\n",
            "Epoch 2000/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0147 - val_loss: 0.1569\n",
            "Epoch 2001/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0138 - val_loss: 0.1576\n",
            "Epoch 2002/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0174 - val_loss: 0.1578\n",
            "Epoch 2003/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0168 - val_loss: 0.1591\n",
            "Epoch 2004/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.1545\n",
            "Epoch 2005/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.1545\n",
            "Epoch 2006/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0217 - val_loss: 0.1582\n",
            "Epoch 2007/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0148 - val_loss: 0.1565\n",
            "Epoch 2008/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0161 - val_loss: 0.1590\n",
            "Epoch 2009/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0139 - val_loss: 0.1570\n",
            "Epoch 2010/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0167 - val_loss: 0.1574\n",
            "Epoch 2011/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0141 - val_loss: 0.1597\n",
            "Epoch 2012/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0188 - val_loss: 0.1585\n",
            "Epoch 2013/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0166 - val_loss: 0.1566\n",
            "Epoch 2014/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0157 - val_loss: 0.1557\n",
            "Epoch 2015/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.1558\n",
            "Epoch 2016/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0185 - val_loss: 0.1543\n",
            "Epoch 2017/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0209 - val_loss: 0.1582\n",
            "Epoch 2018/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0236 - val_loss: 0.1606\n",
            "Epoch 2019/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0297 - val_loss: 0.1586\n",
            "Epoch 2020/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0307 - val_loss: 0.1590\n",
            "Epoch 2021/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1585\n",
            "Epoch 2022/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.1597\n",
            "Epoch 2023/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0178 - val_loss: 0.1562\n",
            "Epoch 2024/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0204 - val_loss: 0.1568\n",
            "Epoch 2025/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0266 - val_loss: 0.1565\n",
            "Epoch 2026/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1541\n",
            "Epoch 2027/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0182 - val_loss: 0.1601\n",
            "Epoch 2028/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0178 - val_loss: 0.1575\n",
            "Epoch 2029/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0156 - val_loss: 0.1554\n",
            "Epoch 2030/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0186 - val_loss: 0.1585\n",
            "Epoch 2031/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.1574\n",
            "Epoch 2032/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0165 - val_loss: 0.1601\n",
            "Epoch 2033/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0179 - val_loss: 0.1543\n",
            "Epoch 2034/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0186 - val_loss: 0.1607\n",
            "Epoch 2035/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0143 - val_loss: 0.1546\n",
            "Epoch 2036/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0133 - val_loss: 0.1557\n",
            "Epoch 2037/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0153 - val_loss: 0.1542\n",
            "Epoch 2038/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0161 - val_loss: 0.1544\n",
            "Epoch 2039/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0198 - val_loss: 0.1595\n",
            "Epoch 2040/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0159 - val_loss: 0.1573\n",
            "Epoch 2041/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0172 - val_loss: 0.1590\n",
            "Epoch 2042/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0237 - val_loss: 0.1597\n",
            "Epoch 2043/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0176 - val_loss: 0.1580\n",
            "Epoch 2044/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0175 - val_loss: 0.1597\n",
            "Epoch 2045/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0184 - val_loss: 0.1545\n",
            "Epoch 2046/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0216 - val_loss: 0.1619\n",
            "Epoch 2047/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0266 - val_loss: 0.1611\n",
            "Epoch 2048/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0328 - val_loss: 0.1604\n",
            "Epoch 2049/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0318 - val_loss: 0.1546\n",
            "Epoch 2050/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0271 - val_loss: 0.1529\n",
            "Epoch 2051/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0206 - val_loss: 0.1585\n",
            "Epoch 2052/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0224 - val_loss: 0.1542\n",
            "Epoch 2053/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.1582\n",
            "Epoch 2054/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0244 - val_loss: 0.1578\n",
            "Epoch 2055/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0265 - val_loss: 0.1604\n",
            "Epoch 2056/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0279 - val_loss: 0.1643\n",
            "Epoch 2057/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0236 - val_loss: 0.1626\n",
            "Epoch 2058/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0218 - val_loss: 0.1632\n",
            "Epoch 2059/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0222 - val_loss: 0.1538\n",
            "Epoch 2060/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0198 - val_loss: 0.1598\n",
            "Epoch 2061/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0207 - val_loss: 0.1581\n",
            "Epoch 2062/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0230 - val_loss: 0.1574\n",
            "Epoch 2063/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0190 - val_loss: 0.1583\n",
            "Epoch 2064/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0151 - val_loss: 0.1596\n",
            "Epoch 2065/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1571\n",
            "Epoch 2066/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.1613\n",
            "Epoch 2067/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.1588\n",
            "Epoch 2068/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0157 - val_loss: 0.1600\n",
            "Epoch 2069/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0141 - val_loss: 0.1592\n",
            "Epoch 2070/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0173 - val_loss: 0.1596\n",
            "Epoch 2071/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.1558\n",
            "Epoch 2072/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0156 - val_loss: 0.1600\n",
            "Epoch 2073/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.1594\n",
            "Epoch 2074/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.1575\n",
            "Epoch 2075/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.1616\n",
            "Epoch 2076/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0181 - val_loss: 0.1599\n",
            "Epoch 2077/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.1615\n",
            "Epoch 2078/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0158 - val_loss: 0.1574\n",
            "Epoch 2079/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0171 - val_loss: 0.1582\n",
            "Epoch 2080/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0146 - val_loss: 0.1601\n",
            "Epoch 2081/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0172 - val_loss: 0.1551\n",
            "Epoch 2082/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.1614\n",
            "Epoch 2083/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0218 - val_loss: 0.1628\n",
            "Epoch 2084/2500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0308 - val_loss: 0.1611\n",
            "Epoch 2085/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0216 - val_loss: 0.1604\n",
            "Epoch 2086/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0200 - val_loss: 0.1616\n",
            "Epoch 2087/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.1590\n",
            "Epoch 2088/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0152 - val_loss: 0.1590\n",
            "Epoch 2089/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0165 - val_loss: 0.1584\n",
            "Epoch 2090/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0188 - val_loss: 0.1607\n",
            "Epoch 2091/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.1595\n",
            "Epoch 2092/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.1573\n",
            "Epoch 2093/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0228 - val_loss: 0.1578\n",
            "Epoch 2094/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.1579\n",
            "Epoch 2095/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0179 - val_loss: 0.1584\n",
            "Epoch 2096/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0173 - val_loss: 0.1608\n",
            "Epoch 2097/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0221 - val_loss: 0.1594\n",
            "Epoch 2098/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0164 - val_loss: 0.1534\n",
            "Epoch 2099/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0161 - val_loss: 0.1626\n",
            "Epoch 2100/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0166 - val_loss: 0.1583\n",
            "Epoch 2101/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0198 - val_loss: 0.1617\n",
            "Epoch 2102/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0155 - val_loss: 0.1591\n",
            "Epoch 2103/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.1603\n",
            "Epoch 2104/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0153 - val_loss: 0.1589\n",
            "Epoch 2105/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.1603\n",
            "Epoch 2106/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0174 - val_loss: 0.1606\n",
            "Epoch 2107/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0169 - val_loss: 0.1587\n",
            "Epoch 2108/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0172 - val_loss: 0.1601\n",
            "Epoch 2109/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0217 - val_loss: 0.1610\n",
            "Epoch 2110/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0156 - val_loss: 0.1591\n",
            "Epoch 2111/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0167 - val_loss: 0.1590\n",
            "Epoch 2112/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.1600\n",
            "Epoch 2113/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0189 - val_loss: 0.1593\n",
            "Epoch 2114/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0184 - val_loss: 0.1630\n",
            "Epoch 2115/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0185 - val_loss: 0.1604\n",
            "Epoch 2116/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.1558\n",
            "Epoch 2117/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.1568\n",
            "Epoch 2118/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.1591\n",
            "Epoch 2119/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0179 - val_loss: 0.1599\n",
            "Epoch 2120/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0208 - val_loss: 0.1594\n",
            "Epoch 2121/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0198 - val_loss: 0.1554\n",
            "Epoch 2122/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0206 - val_loss: 0.1660\n",
            "Epoch 2123/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0245 - val_loss: 0.1610\n",
            "Epoch 2124/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0181 - val_loss: 0.1619\n",
            "Epoch 2125/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0148 - val_loss: 0.1584\n",
            "Epoch 2126/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.1572\n",
            "Epoch 2127/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.1590\n",
            "Epoch 2128/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0224 - val_loss: 0.1598\n",
            "Epoch 2129/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0239 - val_loss: 0.1605\n",
            "Epoch 2130/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1586\n",
            "Epoch 2131/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0212 - val_loss: 0.1572\n",
            "Epoch 2132/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0228 - val_loss: 0.1599\n",
            "Epoch 2133/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.1607\n",
            "Epoch 2134/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0212 - val_loss: 0.1606\n",
            "Epoch 2135/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0180 - val_loss: 0.1603\n",
            "Epoch 2136/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0151 - val_loss: 0.1592\n",
            "Epoch 2137/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0130 - val_loss: 0.1589\n",
            "Epoch 2138/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0143 - val_loss: 0.1637\n",
            "Epoch 2139/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0175 - val_loss: 0.1573\n",
            "Epoch 2140/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0173 - val_loss: 0.1592\n",
            "Epoch 2141/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0198 - val_loss: 0.1612\n",
            "Epoch 2142/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1595\n",
            "Epoch 2143/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0176 - val_loss: 0.1572\n",
            "Epoch 2144/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0217 - val_loss: 0.1566\n",
            "Epoch 2145/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0241 - val_loss: 0.1567\n",
            "Epoch 2146/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0231 - val_loss: 0.1604\n",
            "Epoch 2147/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0273 - val_loss: 0.1663\n",
            "Epoch 2148/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0233 - val_loss: 0.1636\n",
            "Epoch 2149/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0246 - val_loss: 0.1638\n",
            "Epoch 2150/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0240 - val_loss: 0.1595\n",
            "Epoch 2151/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0236 - val_loss: 0.1644\n",
            "Epoch 2152/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0176 - val_loss: 0.1620\n",
            "Epoch 2153/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0178 - val_loss: 0.1685\n",
            "Epoch 2154/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0184 - val_loss: 0.1623\n",
            "Epoch 2155/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0178 - val_loss: 0.1598\n",
            "Epoch 2156/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0203 - val_loss: 0.1583\n",
            "Epoch 2157/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0180 - val_loss: 0.1616\n",
            "Epoch 2158/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0171 - val_loss: 0.1645\n",
            "Epoch 2159/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0206 - val_loss: 0.1650\n",
            "Epoch 2160/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.1623\n",
            "Epoch 2161/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0176 - val_loss: 0.1590\n",
            "Epoch 2162/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0152 - val_loss: 0.1599\n",
            "Epoch 2163/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0166 - val_loss: 0.1618\n",
            "Epoch 2164/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0195 - val_loss: 0.1634\n",
            "Epoch 2165/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0177 - val_loss: 0.1612\n",
            "Epoch 2166/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0187 - val_loss: 0.1594\n",
            "Epoch 2167/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0176 - val_loss: 0.1641\n",
            "Epoch 2168/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0175 - val_loss: 0.1626\n",
            "Epoch 2169/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0154 - val_loss: 0.1570\n",
            "Epoch 2170/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0165 - val_loss: 0.1597\n",
            "Epoch 2171/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0158 - val_loss: 0.1588\n",
            "Epoch 2172/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0150 - val_loss: 0.1619\n",
            "Epoch 2173/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0131 - val_loss: 0.1601\n",
            "Epoch 2174/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0154 - val_loss: 0.1604\n",
            "Epoch 2175/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0135 - val_loss: 0.1591\n",
            "Epoch 2176/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0136 - val_loss: 0.1592\n",
            "Epoch 2177/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.1617\n",
            "Epoch 2178/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0158 - val_loss: 0.1611\n",
            "Epoch 2179/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0188 - val_loss: 0.1594\n",
            "Epoch 2180/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 0.1593\n",
            "Epoch 2181/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0188 - val_loss: 0.1589\n",
            "Epoch 2182/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.1583\n",
            "Epoch 2183/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0136 - val_loss: 0.1587\n",
            "Epoch 2184/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0132 - val_loss: 0.1596\n",
            "Epoch 2185/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0193 - val_loss: 0.1579\n",
            "Epoch 2186/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0275 - val_loss: 0.1618\n",
            "Epoch 2187/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0270 - val_loss: 0.1612\n",
            "Epoch 2188/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0220 - val_loss: 0.1595\n",
            "Epoch 2189/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.1589\n",
            "Epoch 2190/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0266 - val_loss: 0.1673\n",
            "Epoch 2191/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0206 - val_loss: 0.1642\n",
            "Epoch 2192/2500\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0225 - val_loss: 0.1627\n",
            "Epoch 2193/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0149 - val_loss: 0.1604\n",
            "Epoch 2194/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0150 - val_loss: 0.1617\n",
            "Epoch 2195/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0162 - val_loss: 0.1591\n",
            "Epoch 2196/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0169 - val_loss: 0.1653\n",
            "Epoch 2197/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0208 - val_loss: 0.1622\n",
            "Epoch 2198/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.1681\n",
            "Epoch 2199/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0230 - val_loss: 0.1664\n",
            "Epoch 2200/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.1636\n",
            "Epoch 2201/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0139 - val_loss: 0.1635\n",
            "Epoch 2202/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0163 - val_loss: 0.1607\n",
            "Epoch 2203/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0116 - val_loss: 0.1662\n",
            "Epoch 2204/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0172 - val_loss: 0.1617\n",
            "Epoch 2205/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0160 - val_loss: 0.1606\n",
            "Epoch 2206/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0149 - val_loss: 0.1612\n",
            "Epoch 2207/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0193 - val_loss: 0.1627\n",
            "Epoch 2208/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0157 - val_loss: 0.1585\n",
            "Epoch 2209/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0147 - val_loss: 0.1622\n",
            "Epoch 2210/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0151 - val_loss: 0.1591\n",
            "Epoch 2211/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0171 - val_loss: 0.1621\n",
            "Epoch 2212/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0164 - val_loss: 0.1561\n",
            "Epoch 2213/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0222 - val_loss: 0.1639\n",
            "Epoch 2214/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0211 - val_loss: 0.1605\n",
            "Epoch 2215/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0171 - val_loss: 0.1595\n",
            "Epoch 2216/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0157 - val_loss: 0.1592\n",
            "Epoch 2217/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0161 - val_loss: 0.1659\n",
            "Epoch 2218/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0183 - val_loss: 0.1624\n",
            "Epoch 2219/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0208 - val_loss: 0.1663\n",
            "Epoch 2220/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.1607\n",
            "Epoch 2221/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0168 - val_loss: 0.1575\n",
            "Epoch 2222/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0162 - val_loss: 0.1597\n",
            "Epoch 2223/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0164 - val_loss: 0.1585\n",
            "Epoch 2224/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0208 - val_loss: 0.1599\n",
            "Epoch 2225/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0122 - val_loss: 0.1593\n",
            "Epoch 2226/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0131 - val_loss: 0.1636\n",
            "Epoch 2227/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0150 - val_loss: 0.1585\n",
            "Epoch 2228/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.1624\n",
            "Epoch 2229/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0190 - val_loss: 0.1615\n",
            "Epoch 2230/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0165 - val_loss: 0.1627\n",
            "Epoch 2231/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0167 - val_loss: 0.1609\n",
            "Epoch 2232/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0144 - val_loss: 0.1639\n",
            "Epoch 2233/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0184 - val_loss: 0.1615\n",
            "Epoch 2234/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0149 - val_loss: 0.1639\n",
            "Epoch 2235/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0206 - val_loss: 0.1586\n",
            "Epoch 2236/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0288 - val_loss: 0.1649\n",
            "Epoch 2237/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0206 - val_loss: 0.1654\n",
            "Epoch 2238/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.1606\n",
            "Epoch 2239/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0225 - val_loss: 0.1650\n",
            "Epoch 2240/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0175 - val_loss: 0.1619\n",
            "Epoch 2241/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0160 - val_loss: 0.1652\n",
            "Epoch 2242/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0169 - val_loss: 0.1629\n",
            "Epoch 2243/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0164 - val_loss: 0.1644\n",
            "Epoch 2244/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0167 - val_loss: 0.1636\n",
            "Epoch 2245/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1651\n",
            "Epoch 2246/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0200 - val_loss: 0.1601\n",
            "Epoch 2247/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.1658\n",
            "Epoch 2248/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0172 - val_loss: 0.1617\n",
            "Epoch 2249/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0171 - val_loss: 0.1622\n",
            "Epoch 2250/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0241 - val_loss: 0.1635\n",
            "Epoch 2251/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0344 - val_loss: 0.1568\n",
            "Epoch 2252/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0257 - val_loss: 0.1640\n",
            "Epoch 2253/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.1665\n",
            "Epoch 2254/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0180 - val_loss: 0.1616\n",
            "Epoch 2255/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0177 - val_loss: 0.1640\n",
            "Epoch 2256/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0125 - val_loss: 0.1611\n",
            "Epoch 2257/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0146 - val_loss: 0.1570\n",
            "Epoch 2258/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0185 - val_loss: 0.1591\n",
            "Epoch 2259/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1599\n",
            "Epoch 2260/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0244 - val_loss: 0.1610\n",
            "Epoch 2261/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0230 - val_loss: 0.1667\n",
            "Epoch 2262/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0215 - val_loss: 0.1649\n",
            "Epoch 2263/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0178 - val_loss: 0.1631\n",
            "Epoch 2264/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.1667\n",
            "Epoch 2265/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0256 - val_loss: 0.1623\n",
            "Epoch 2266/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0152 - val_loss: 0.1621\n",
            "Epoch 2267/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0157 - val_loss: 0.1631\n",
            "Epoch 2268/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.1616\n",
            "Epoch 2269/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0228 - val_loss: 0.1589\n",
            "Epoch 2270/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0191 - val_loss: 0.1589\n",
            "Epoch 2271/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0124 - val_loss: 0.1667\n",
            "Epoch 2272/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.1653\n",
            "Epoch 2273/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0149 - val_loss: 0.1614\n",
            "Epoch 2274/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0172 - val_loss: 0.1607\n",
            "Epoch 2275/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0172 - val_loss: 0.1654\n",
            "Epoch 2276/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0140 - val_loss: 0.1639\n",
            "Epoch 2277/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0160 - val_loss: 0.1615\n",
            "Epoch 2278/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0138 - val_loss: 0.1615\n",
            "Epoch 2279/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0156 - val_loss: 0.1651\n",
            "Epoch 2280/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0131 - val_loss: 0.1638\n",
            "Epoch 2281/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0142 - val_loss: 0.1620\n",
            "Epoch 2282/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.1612\n",
            "Epoch 2283/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0108 - val_loss: 0.1606\n",
            "Epoch 2284/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0141 - val_loss: 0.1597\n",
            "Epoch 2285/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.1613\n",
            "Epoch 2286/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0156 - val_loss: 0.1622\n",
            "Epoch 2287/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0202 - val_loss: 0.1621\n",
            "Epoch 2288/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0195 - val_loss: 0.1632\n",
            "Epoch 2289/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0201 - val_loss: 0.1622\n",
            "Epoch 2290/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0154 - val_loss: 0.1653\n",
            "Epoch 2291/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 0.1671\n",
            "Epoch 2292/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0222 - val_loss: 0.1645\n",
            "Epoch 2293/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0213 - val_loss: 0.1674\n",
            "Epoch 2294/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0242 - val_loss: 0.1610\n",
            "Epoch 2295/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.1631\n",
            "Epoch 2296/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0241 - val_loss: 0.1608\n",
            "Epoch 2297/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0212 - val_loss: 0.1594\n",
            "Epoch 2298/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.1617\n",
            "Epoch 2299/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0188 - val_loss: 0.1656\n",
            "Epoch 2300/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0159 - val_loss: 0.1660\n",
            "Epoch 2301/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0166 - val_loss: 0.1597\n",
            "Epoch 2302/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0164 - val_loss: 0.1597\n",
            "Epoch 2303/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0173 - val_loss: 0.1652\n",
            "Epoch 2304/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0242 - val_loss: 0.1616\n",
            "Epoch 2305/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0260 - val_loss: 0.1633\n",
            "Epoch 2306/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0211 - val_loss: 0.1627\n",
            "Epoch 2307/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.1669\n",
            "Epoch 2308/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1616\n",
            "Epoch 2309/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0149 - val_loss: 0.1645\n",
            "Epoch 2310/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0141 - val_loss: 0.1616\n",
            "Epoch 2311/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0176 - val_loss: 0.1621\n",
            "Epoch 2312/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0172 - val_loss: 0.1619\n",
            "Epoch 2313/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0175 - val_loss: 0.1639\n",
            "Epoch 2314/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0173 - val_loss: 0.1631\n",
            "Epoch 2315/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0175 - val_loss: 0.1657\n",
            "Epoch 2316/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0186 - val_loss: 0.1616\n",
            "Epoch 2317/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0167 - val_loss: 0.1605\n",
            "Epoch 2318/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0136 - val_loss: 0.1605\n",
            "Epoch 2319/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0151 - val_loss: 0.1588\n",
            "Epoch 2320/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0143 - val_loss: 0.1619\n",
            "Epoch 2321/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0223 - val_loss: 0.1610\n",
            "Epoch 2322/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0173 - val_loss: 0.1635\n",
            "Epoch 2323/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0126 - val_loss: 0.1642\n",
            "Epoch 2324/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.1634\n",
            "Epoch 2325/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0174 - val_loss: 0.1632\n",
            "Epoch 2326/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0183 - val_loss: 0.1625\n",
            "Epoch 2327/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0259 - val_loss: 0.1631\n",
            "Epoch 2328/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0259 - val_loss: 0.1664\n",
            "Epoch 2329/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0268 - val_loss: 0.1630\n",
            "Epoch 2330/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0168 - val_loss: 0.1662\n",
            "Epoch 2331/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1577\n",
            "Epoch 2332/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0206 - val_loss: 0.1666\n",
            "Epoch 2333/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0186 - val_loss: 0.1637\n",
            "Epoch 2334/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.1669\n",
            "Epoch 2335/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0217 - val_loss: 0.1711\n",
            "Epoch 2336/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0217 - val_loss: 0.1580\n",
            "Epoch 2337/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.1656\n",
            "Epoch 2338/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0177 - val_loss: 0.1598\n",
            "Epoch 2339/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1621\n",
            "Epoch 2340/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.1649\n",
            "Epoch 2341/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0154 - val_loss: 0.1627\n",
            "Epoch 2342/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0147 - val_loss: 0.1631\n",
            "Epoch 2343/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0198 - val_loss: 0.1650\n",
            "Epoch 2344/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.1604\n",
            "Epoch 2345/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.1726\n",
            "Epoch 2346/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0249 - val_loss: 0.1636\n",
            "Epoch 2347/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.1604\n",
            "Epoch 2348/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0234 - val_loss: 0.1650\n",
            "Epoch 2349/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.1591\n",
            "Epoch 2350/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0241 - val_loss: 0.1678\n",
            "Epoch 2351/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0207 - val_loss: 0.1662\n",
            "Epoch 2352/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.1630\n",
            "Epoch 2353/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.1651\n",
            "Epoch 2354/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0195 - val_loss: 0.1633\n",
            "Epoch 2355/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0230 - val_loss: 0.1622\n",
            "Epoch 2356/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.1611\n",
            "Epoch 2357/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0225 - val_loss: 0.1601\n",
            "Epoch 2358/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1636\n",
            "Epoch 2359/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.1634\n",
            "Epoch 2360/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.1601\n",
            "Epoch 2361/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0170 - val_loss: 0.1622\n",
            "Epoch 2362/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.1649\n",
            "Epoch 2363/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0156 - val_loss: 0.1573\n",
            "Epoch 2364/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0146 - val_loss: 0.1617\n",
            "Epoch 2365/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0184 - val_loss: 0.1605\n",
            "Epoch 2366/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0220 - val_loss: 0.1579\n",
            "Epoch 2367/2500\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0226 - val_loss: 0.1643\n",
            "Epoch 2368/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0165 - val_loss: 0.1594\n",
            "Epoch 2369/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.1618\n",
            "Epoch 2370/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0156 - val_loss: 0.1631\n",
            "Epoch 2371/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0167 - val_loss: 0.1656\n",
            "Epoch 2372/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0183 - val_loss: 0.1631\n",
            "Epoch 2373/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0119 - val_loss: 0.1625\n",
            "Epoch 2374/2500\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0116 - val_loss: 0.1631\n",
            "Epoch 2375/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.1666\n",
            "Epoch 2376/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0133 - val_loss: 0.1665\n",
            "Epoch 2377/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0211 - val_loss: 0.1639\n",
            "Epoch 2378/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.1608\n",
            "Epoch 2379/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0149 - val_loss: 0.1593\n",
            "Epoch 2380/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0187 - val_loss: 0.1626\n",
            "Epoch 2381/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0189 - val_loss: 0.1597\n",
            "Epoch 2382/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0150 - val_loss: 0.1640\n",
            "Epoch 2383/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.1620\n",
            "Epoch 2384/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0166 - val_loss: 0.1591\n",
            "Epoch 2385/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0166 - val_loss: 0.1621\n",
            "Epoch 2386/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.1601\n",
            "Epoch 2387/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0139 - val_loss: 0.1598\n",
            "Epoch 2388/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.1625\n",
            "Epoch 2389/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0180 - val_loss: 0.1643\n",
            "Epoch 2390/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0153 - val_loss: 0.1619\n",
            "Epoch 2391/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0161 - val_loss: 0.1580\n",
            "Epoch 2392/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.1605\n",
            "Epoch 2393/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0180 - val_loss: 0.1644\n",
            "Epoch 2394/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0148 - val_loss: 0.1630\n",
            "Epoch 2395/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0154 - val_loss: 0.1634\n",
            "Epoch 2396/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0161 - val_loss: 0.1618\n",
            "Epoch 2397/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0113 - val_loss: 0.1618\n",
            "Epoch 2398/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0086 - val_loss: 0.1639\n",
            "Epoch 2399/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0125 - val_loss: 0.1633\n",
            "Epoch 2400/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0137 - val_loss: 0.1619\n",
            "Epoch 2401/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0140 - val_loss: 0.1695\n",
            "Epoch 2402/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.1620\n",
            "Epoch 2403/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0161 - val_loss: 0.1616\n",
            "Epoch 2404/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0138 - val_loss: 0.1611\n",
            "Epoch 2405/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.1610\n",
            "Epoch 2406/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0160 - val_loss: 0.1658\n",
            "Epoch 2407/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0163 - val_loss: 0.1614\n",
            "Epoch 2408/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.1612\n",
            "Epoch 2409/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0163 - val_loss: 0.1633\n",
            "Epoch 2410/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0140 - val_loss: 0.1614\n",
            "Epoch 2411/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0200 - val_loss: 0.1631\n",
            "Epoch 2412/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.1638\n",
            "Epoch 2413/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0220 - val_loss: 0.1645\n",
            "Epoch 2414/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.1604\n",
            "Epoch 2415/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0242 - val_loss: 0.1653\n",
            "Epoch 2416/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0179 - val_loss: 0.1661\n",
            "Epoch 2417/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0177 - val_loss: 0.1642\n",
            "Epoch 2418/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0163 - val_loss: 0.1641\n",
            "Epoch 2419/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0120 - val_loss: 0.1641\n",
            "Epoch 2420/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0120 - val_loss: 0.1636\n",
            "Epoch 2421/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.1602\n",
            "Epoch 2422/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0142 - val_loss: 0.1637\n",
            "Epoch 2423/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0167 - val_loss: 0.1631\n",
            "Epoch 2424/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0176 - val_loss: 0.1632\n",
            "Epoch 2425/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.1644\n",
            "Epoch 2426/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0172 - val_loss: 0.1651\n",
            "Epoch 2427/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0196 - val_loss: 0.1624\n",
            "Epoch 2428/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0253 - val_loss: 0.1677\n",
            "Epoch 2429/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0200 - val_loss: 0.1617\n",
            "Epoch 2430/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0166 - val_loss: 0.1632\n",
            "Epoch 2431/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0147 - val_loss: 0.1665\n",
            "Epoch 2432/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0153 - val_loss: 0.1608\n",
            "Epoch 2433/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0152 - val_loss: 0.1693\n",
            "Epoch 2434/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0207 - val_loss: 0.1621\n",
            "Epoch 2435/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0215 - val_loss: 0.1660\n",
            "Epoch 2436/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0159 - val_loss: 0.1631\n",
            "Epoch 2437/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0152 - val_loss: 0.1673\n",
            "Epoch 2438/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0139 - val_loss: 0.1657\n",
            "Epoch 2439/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0145 - val_loss: 0.1641\n",
            "Epoch 2440/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0172 - val_loss: 0.1658\n",
            "Epoch 2441/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.1624\n",
            "Epoch 2442/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0167 - val_loss: 0.1623\n",
            "Epoch 2443/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0144 - val_loss: 0.1624\n",
            "Epoch 2444/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0185 - val_loss: 0.1654\n",
            "Epoch 2445/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0181 - val_loss: 0.1633\n",
            "Epoch 2446/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0160 - val_loss: 0.1624\n",
            "Epoch 2447/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0171 - val_loss: 0.1612\n",
            "Epoch 2448/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0153 - val_loss: 0.1600\n",
            "Epoch 2449/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0157 - val_loss: 0.1589\n",
            "Epoch 2450/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0172 - val_loss: 0.1623\n",
            "Epoch 2451/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.1671\n",
            "Epoch 2452/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0176 - val_loss: 0.1636\n",
            "Epoch 2453/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0210 - val_loss: 0.1695\n",
            "Epoch 2454/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0292 - val_loss: 0.1690\n",
            "Epoch 2455/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0248 - val_loss: 0.1672\n",
            "Epoch 2456/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.1661\n",
            "Epoch 2457/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.1649\n",
            "Epoch 2458/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0171 - val_loss: 0.1615\n",
            "Epoch 2459/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0154 - val_loss: 0.1648\n",
            "Epoch 2460/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.1639\n",
            "Epoch 2461/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0167 - val_loss: 0.1624\n",
            "Epoch 2462/2500\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0233 - val_loss: 0.1637\n",
            "Epoch 2463/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0188 - val_loss: 0.1669\n",
            "Epoch 2464/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0196 - val_loss: 0.1655\n",
            "Epoch 2465/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1657\n",
            "Epoch 2466/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.1633\n",
            "Epoch 2467/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0193 - val_loss: 0.1606\n",
            "Epoch 2468/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0202 - val_loss: 0.1651\n",
            "Epoch 2469/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0205 - val_loss: 0.1620\n",
            "Epoch 2470/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0210 - val_loss: 0.1672\n",
            "Epoch 2471/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0273 - val_loss: 0.1716\n",
            "Epoch 2472/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0260 - val_loss: 0.1704\n",
            "Epoch 2473/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0240 - val_loss: 0.1673\n",
            "Epoch 2474/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0287 - val_loss: 0.1655\n",
            "Epoch 2475/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0229 - val_loss: 0.1696\n",
            "Epoch 2476/2500\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0160 - val_loss: 0.1697\n",
            "Epoch 2477/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.1629\n",
            "Epoch 2478/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0207 - val_loss: 0.1659\n",
            "Epoch 2479/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0154 - val_loss: 0.1627\n",
            "Epoch 2480/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0210 - val_loss: 0.1675\n",
            "Epoch 2481/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0218 - val_loss: 0.1692\n",
            "Epoch 2482/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0184 - val_loss: 0.1632\n",
            "Epoch 2483/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0137 - val_loss: 0.1643\n",
            "Epoch 2484/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0124 - val_loss: 0.1629\n",
            "Epoch 2485/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0140 - val_loss: 0.1693\n",
            "Epoch 2486/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.1678\n",
            "Epoch 2487/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0141 - val_loss: 0.1664\n",
            "Epoch 2488/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0146 - val_loss: 0.1627\n",
            "Epoch 2489/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0111 - val_loss: 0.1642\n",
            "Epoch 2490/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 0.1629\n",
            "Epoch 2491/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0151 - val_loss: 0.1674\n",
            "Epoch 2492/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.1663\n",
            "Epoch 2493/2500\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0152 - val_loss: 0.1647\n",
            "Epoch 2494/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0117 - val_loss: 0.1596\n",
            "Epoch 2495/2500\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0145 - val_loss: 0.1634\n",
            "Epoch 2496/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.1612\n",
            "Epoch 2497/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0150 - val_loss: 0.1603\n",
            "Epoch 2498/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0178 - val_loss: 0.1611\n",
            "Epoch 2499/2500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0184 - val_loss: 0.1620\n",
            "Epoch 2500/2500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0224 - val_loss: 0.1683\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f65ae2f7dc0>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "patience = 50\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Simple_FC_Model.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y))#, callbacks=[reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtIovqi1coZG",
        "outputId": "1a77931b-243c-4eda-a03e-e2735e7183d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"simple_fc_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_74 (Dense)            multiple                  110       \n",
            "                                                                 \n",
            " dense_75 (Dense)            multiple                  979       \n",
            "                                                                 \n",
            " dense_76 (Dense)            multiple                  34560     \n",
            "                                                                 \n",
            " dense_77 (Dense)            multiple                  12320     \n",
            "                                                                 \n",
            " dense_78 (Dense)            multiple                  33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,002\n",
            "Trainable params: 48,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Simple_FC_Model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prdGeqM9nsGq"
      },
      "source": [
        "## Group Avg + $M$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BixC_uBDcrh0"
      },
      "outputs": [],
      "source": [
        "lambda_val = 0.01\n",
        "l2_reg = tf.keras.regularizers.l2(1e-5)\n",
        "def groupAvereaging(inputs, operation):\n",
        "    x = inputs\n",
        "    a, b, c, d, e, f, g, h, i, j = tf.unstack(x, axis=1)\n",
        "\n",
        "    # Z5 in S5\n",
        "    x1 = x\n",
        "    x2 = tf.stack([b, c, d, e, f, g, h, i, j, a], axis=1)\n",
        "    x3 = tf.stack([c, d, e, f, g, h, i, j, a, b], axis=1)\n",
        "    x4 = tf.stack([d, e, f, g, h, i, j, a, b, c], axis=1)\n",
        "    x5 = tf.stack([e, f, g, h, i, j, a, b, c, d], axis=1)\n",
        "    x6 = tf.stack([f, g, h, i, j, a, b, c, d, e], axis=1)\n",
        "    x7 = tf.stack([g, h, i, j, a, b, c, d, e, f], axis=1)\n",
        "    x8 = tf.stack([h, i, j, a, b, c, d, e, f, g], axis=1)\n",
        "    x9 = tf.stack([i, j, a, b, c, d, e, f, g, h], axis=1)\n",
        "    x10 = tf.stack([j, a, b, c, d, e, f, g, h, i], axis=1)\n",
        "\n",
        "    x1 = operation(x1)\n",
        "    x2 = operation(x2)\n",
        "    x3 = operation(x3)\n",
        "    x4 = operation(x4)\n",
        "    x5 = operation(x5)    \n",
        "    x6 = operation(x6)\n",
        "    x7 = operation(x7)\n",
        "    x8 = operation(x8)\n",
        "    x9 = operation(x9)\n",
        "    x10 = operation(x10)    \n",
        "\n",
        "    x = tf.reduce_mean(tf.stack([x1, x2, x3, x4, x5, x6, x7, x8, x9, x10], -1), -1)\n",
        "    return x\n",
        "\n",
        "class SimpleNet_M_Z10(tf.keras.Model):\n",
        "    def __init__(self, d=10):\n",
        "        self.d = d\n",
        "        super(SimpleNet_M_Z10, self).__init__()\n",
        "        activation = tf.keras.activations.tanh\n",
        "        self.fc1 = tf.keras.layers.Dense(self.d, activation=None, \n",
        "                                         kernel_regularizer=tf.keras.regularizers.l1(lambda_val),\n",
        "                                         kernel_constraint = nonneg())        \n",
        "        self.features = [\n",
        "            tf.keras.layers.Dense(89, activation, kernel_regularizer=l2_reg),\n",
        "            tf.keras.layers.Dense(6 * 32, activation, kernel_regularizer=l2_reg),\n",
        "            tf.keras.layers.Dense(32, activation, kernel_regularizer=l2_reg),\n",
        "            tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
        "        ]\n",
        "\n",
        "    def process(self, x):\n",
        "        x = apply_layers(x, self.features)\n",
        "        return x\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.fc1(inputs)\n",
        "        x = groupAvereaging(x, self.process)\n",
        "        return x    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC4DrQ4fn27c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Model_discover\n",
        "    #gc.collect()\n",
        "except:\n",
        "    print(\"Do nothing\")\n",
        "\n",
        "np.random.seed(2048)\n",
        "Model_discover = SimpleNet_M_Z10()\n",
        "adam = Adam(lr=1e-3)\n",
        "Model_discover.compile(optimizer=adam, loss='mae')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V21gXJMYqaQx",
        "outputId": "4c560158-afc9-49a1-85df-0c54877d4dfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((64, 10), (64,), (480, 10), (480,))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.shape, train_y.shape, val_ds.shape, val_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKW2m7OjprNI"
      },
      "outputs": [],
      "source": [
        "patience = 50\n",
        "batch_size = 16\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
        "max_epochs = 2500\n",
        "Model_discover.fit(train_ds, train_y, \n",
        "          epochs=max_epochs, batch_size=batch_size,\n",
        "          shuffle=True, validation_data=(val_ds, val_y), callbacks=[reduce_lr])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "H6Zz8bF7T31o",
        "X__6-vvNWXLH",
        "-MFKadp0ONjX",
        "WbSP8l2J3hP9",
        "fK4AQ9Q4VRti",
        "B4ewogqNphvJ",
        "LRHchnEw6Xz5",
        "oI9J_fygub4X",
        "kF9DGRt45IOf",
        "-ya_uzcXQUE0",
        "a7hHcwi1RCDy",
        "prdGeqM9nsGq"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}